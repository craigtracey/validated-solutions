<!doctype html><html><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=0"><meta name="last modified" content="27/09/2021 12:47:24"><meta name=abstract content><meta name=author content="dpavel@vmware.com"><meta name=primary-product-name content="MD2Docs-TestBed"><meta name=primary-product-version content="1"><meta name=description content><meta name=guid content="GUID-1Intro"><meta name=language content="en"><meta name=title content="Basic Markdown"><meta name=publication-author content="dpavel@vmware.com"><meta property="og:title" content="Basic Markdown"><meta property="og:image" content="https://docs-uat.vmware.com/uicontent/images/vmware-docs-default.png"><meta property="og:description" content><meta property="og:type" content="article"><meta property="og:locale" content="en"><meta property="og:url" content="https://docs-uat-staging.vmware.com/en/MD2Docs-TestBed/1/md-2-docs-test-bed/GUID-1Intro.html"><meta name=cdf-utag content="https://tags.tiqcdn.com/utag/vmware/cdf-privacy/qa/utag.js"><link rel=stylesheet type=text/css href=/css/commonltr.css><link rel=stylesheet type=text/css href=/css/non-draft.vmware.productdocs.css><link rel=canonical href=https://docs-uat-staging.vmware.com/en/MD2Docs-TestBed/1/md-2-docs-test-bed/GUID-1Intro.html><link class=user href=/css/responsive.css rel=stylesheet type=text/css><link rel=icon href=https://www.vmware.com/favicon.ico type=image/x-icon><link rel=stylesheet href=/css/v2-global.20200911172508.css><title>Docs Preview</title></head><body><header class=tech-pub-header><div id=header class="global-header col-12"><div class="row desktop-header h-100"><div class="col col-md-3 align-self-center header-logo-wrapper"><div class="d-inline-flex align-items-center justify-content-start w-100"><div class="d-inline-flex align-items-center w-100"><span class="my-auto d-md-none header-menu-icon"><i class="fa fa-bars"></i></span><h1><a href=https://docs-uat-staging.vmware.com/ class="d-inline-flex align-items-center my-auto nav-link header-logo-url pl-md-1 pl-xl-3"><span class=mr-2><img src=/img/vm-logo.png alt="VMware Logo"></span>
<span class=vm-logo-title>Docs Preview</span></a></h1></div><div class=align-items-center><span id=toggleTOC class="d-md-none header-toc-icon"><i class="fa fa-ellipsis-v px-2"></i></span></div></div></div></div></div><div class="col-12 vmware-gradient w-100 px-0 mx-0"></div></header><div class="tech-pub-container main-container d-flex flex-column pubView"><section class="tech-pub-section d-flex flex-md-row"><div class="lhs lhs-container col-md-4 col-lg-3" style=display:block><div class=backdrop></div><div class=left-panel><div class="panel-header position-relative hidden-xs"><div class="panel-header-left d-flex justify-content-between align-items-center"><span class=container-collapse-expand><span id=expand-all-id class=expand-tree onclick=expandAll()><span class="lhs-expand-shape align-middle"><i class="fa fa-chevron-down"></i></span>
<span class="expand-text align-middle" data-i18n data-i18n-expand-all>Expand All</span></span>
<span id=collapse-all-id class="collapse-tree hide" onclick=collapseAll()><span class="lhs-collapse-shape align-middle"><i class="fa fa-chevron-up"></i></span>
<span class="collapse-text align-middle" data-i18n data-i18n-collapse-all>Collapse All</span></span></span></div></div><div class="panel-content p-2" id=left_toc><div class="dropdown collection-dropdown-container w-100"><button class="btn w-100 text-left dropdown-toggle collection-name" type=button id=collectionDropdwnBtn data-toggle=dropdown aria-haspopup=true aria-label="Collection Dropdown" aria-expanded=false>
<span class=label></span>
<span class="float-right icon-down pl-2 fa fa-angle-down"></span></button><div class="dropdown-menu w-100" id=collectionMenu aria-labelledby=collectionDropdwnBtn></div></div><div id=tree class=mt-2></div><div class="w-100 px-2 toc-product-container"><a class="mr-3 my-2 position-relative toc-product-link"><span class=toc-product-name></span>
<span class=localized-page-name>Product Documentation</span></a></div><ul class=rm-default-ul-styles></ul></div></div></div><div class="rhs rhs-container col-md-8 col-lg-7" style=display:block><div class=rhs-top><div class="rhs-top-container-top d-flex flex-row justify-content-between"><div class=primary-header id=page-heading-id></div></div><div class="rhs-top-container-middle d-flex flex-row justify-content-between"></div><div class=rhs-top-container-bottom><div class=last-updated-container><div class=calendar-icon><svg width="36" height="36" viewBox="0 0 36 36" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path class="clr-i-outline clr-i-outline-path-1" d="M32.25 6H29V8h3V30H4V8H7V6H3.75A1.78 1.78.0 002 7.81V30.19A1.78 1.78.0 003.75 32h28.5A1.78 1.78.0 0034 30.19V7.81A1.78 1.78.0 0032.25 6z"/><rect class="clr-i-outline clr-i-outline-path-2" x="8" y="14" width="2" height="2"/><rect class="clr-i-outline clr-i-outline-path-3" x="14" y="14" width="2" height="2"/><rect class="clr-i-outline clr-i-outline-path-4" x="20" y="14" width="2" height="2"/><rect class="clr-i-outline clr-i-outline-path-5" x="26" y="14" width="2" height="2"/><rect class="clr-i-outline clr-i-outline-path-6" x="8" y="19" width="2" height="2"/><rect class="clr-i-outline clr-i-outline-path-7" x="14" y="19" width="2" height="2"/><rect class="clr-i-outline clr-i-outline-path-8" x="20" y="19" width="2" height="2"/><rect class="clr-i-outline clr-i-outline-path-9" x="26" y="19" width="2" height="2"/><rect class="clr-i-outline clr-i-outline-path-10" x="8" y="24" width="2" height="2"/><rect class="clr-i-outline clr-i-outline-path-11" x="14" y="24" width="2" height="2"/><rect class="clr-i-outline clr-i-outline-path-12" x="20" y="24" width="2" height="2"/><rect class="clr-i-outline clr-i-outline-path-13" x="26" y="24" width="2" height="2"/><path class="clr-i-outline clr-i-outline-path-14" d="M10 10a1 1 0 001-1V3A1 1 0 009 3V9a1 1 0 001 1z"/><path class="clr-i-outline clr-i-outline-path-15" d="M26 10a1 1 0 001-1V3a1 1 0 00-2 0V9a1 1 0 001 1z"/><rect class="clr-i-outline clr-i-outline-path-16" x="13" y="6" width="10" height="2"/><rect x="0" y="0" width="36" height="36" fill-opacity="0"/></svg></div><span class=last-updated-label data-i18n-updated-on>Updated on</span>
&nbsp;
<span class=last-updated-date-ph>9/22/22</span></div></div></div><div class="rhs-center article-wrapper" id=content-div-id><h1 id=deploy-tanzu-for-kubernetes-operations-on-vsphere-with-vmware-vds>Deploy Tanzu for Kubernetes Operations on vSphere with VMware VDS</h1><p>This document provides step-by-step instructions for installing and configuring Tanzu for Kubernetes Operations on a vSphere environment backed by a Virtual Distributed Switch (VDS). The deployment is based on the reference design provided in <a href=../reference-designs/tko-on-vsphere.md>VMware Tanzu for Kubernetes Operations on vSphere Reference Design</a>. This document does not provide instructions for deploying the underlying SDDC components.</p><h2 id=deploying-with-vmware-service-installer-for-tanzu>Deploying with VMware Service Installer for Tanzu</h2><p>You can use VMware Service Installer for VMware Tanzu to automate this deployment.</p><p>VMware Service Installer for Tanzu automates the deployment of the reference designs for Tanzu for Kubernetes Operations. It uses best practices for deploying and configuring the required Tanzu for Kubernetes Operations components.</p><p>To use Service Installer to automate this deployment, see <a href=https://docs.vmware.com/en/Service-Installer-for-VMware-Tanzu/1.3/service-installer/GUID-vSphere%20-%20Backed%20by%20VDS-TKGm-TKOonVsphereVDStkg.html>Deploying VMware Tanzu for Kubernetes Operations on vSphere with vSphere Distributed Switch Using Service Installer for VMware Tanzu</a>.</p><p>Alternatively, if you decide to manually deploy each component, follow the steps provided in this document.</p><h2 id=prepare-your-environment-for-deploying-tanzu-for-kubernetes-operations>Prepare Your Environment for Deploying Tanzu for Kubernetes Operations</h2><p>Before you start the deployment, ensure that the required resource pools and folders are created.
Following are sample entries of the resource pools and folders.</p><table><thead><tr><th>Resource Type</th><th>Sample Resource Pool Name</th><th>Sample Folder Name</th></tr></thead><tbody><tr><td>NSX Advanced Load Balancer Components</td><td><code>nsx-alb-components</code></td><td><code>nsx-alb-components</code></td></tr><tr><td>TKG Management Components</td><td><code>tkg-management-components</code></td><td><code>tkg-management-components</code></td></tr><tr><td>TKG Shared Service Components</td><td><code>tkg-sharedsvc-components</code></td><td><code>tkg-sharedsvc-components</code></td></tr><tr><td>TKG Workload components</td><td><code>tkg-workload01-components</code></td><td><code>tkg-workload01-components</code></td></tr></tbody></table><p>The following picture is an example of resource pools in a vSphere environment:</p><p><img src=.//img/deployment-guides/tko-on-vsphere-vds/image37.png alt></p><p>The following picture is an example of VM folders in a vSphere environment:</p><p><img src=.//img/deployment-guides/tko-on-vsphere-vds/image43.png alt></p><h2 id=overview--of-the-deployment-steps>Overview of the Deployment Steps</h2><p>The following is an overview of the main steps for deploying Tanzu Kubernetes Operations on vSphere backed by VDS:</p><ol><li><p><a href=#deploy-and-configure-nsx-advanced-load-balancer>Deploy and Configure NSX Advanced Load Balancer</a></p></li><li><p><a href=#deploy-and-configure-bootstrap-machine>Deploy and Configure Bootstrap Machine</a></p></li><li><p><a href=#deploy-tanzu-kubernetes-grid-tkg-management-cluster>Deploy Tanzu Kubernetes Grid Management Cluster</a></p></li><li><p><a href=#deploy-tanzu-shared-service-cluster>Deploy Tanzu Kubernetes Grid Shared Service Cluster</a></p></li><li><p><a href=#deploy-tanzu-workload-clusters>Deploy Tanzu Kubernetes Grid Workload Cluster</a></p></li><li><p><a href=#deploy-user-managed-packages-on-tkg-clusters>Deploy User-Managed Packages on Tanzu Kubernetes Grid Clusters</a></p></li></ol><h2 id=a-iddeploy-and-configure-nsx-advanced-load-balancer-a-deploy-and-configure-nsx-advanced-load-balancer>Deploy and Configure NSX Advanced Load Balancer</h2><p>NSX Advanced Load Balancer is an enterprise-grade integrated load balancer that provides L4-L7 Load Balancer support. NSX Advanced Load Balancer is recommended for vSphere deployments without NSX-T, or when there are unique scaling requirements.</p><p>For a production-grade deployment, VMware recommends that you deploy 3 instances of the NSX Advanced Load Balancer Controller for high availability and resiliency.</p><p>The following is the sample IP address and FQDN set for the NSX Advanced Load Balancer controllers:</p><table><thead><tr><th>Controller Node</th><th>IP Address</th><th>FQDN</th></tr></thead><tbody><tr><td>Node 1 Primary</td><td>172.16.10.10</td><td>avi01.lab.vmw</td></tr><tr><td>Node 2 Secondary</td><td>172.16.10.28</td><td>avi02.lab.vmw</td></tr><tr><td>Node 3 Secondary</td><td>172.16.10.29</td><td>avi03.lab.vmw</td></tr><tr><td>HA Address</td><td>172.16.10.30</td><td>avi-ha.lab.vmw</td></tr></tbody></table><h3 id=a-iddeploy-nsx-advanced-load-balancer-a-deploy-nsx-advanced-load-balancer>Deploy NSX Advanced Load Balancer</h3><p>As one of the prerequisites, you must have downloaded the NSX Advanced Load Balancer 20.1.6 Open Virtual Appliance (OVA) and imported it to the content library. Deploy the NSX Advanced Load Balancer under the <strong>resource pool “nsx-alb-components”</strong> and place it in the <strong>“nsx-alb-components”</strong> <strong>folder</strong>.</p><p>To deploy NSX Advanced Load Balancer:</p><ol><li><p>Log in to <strong>vCenter</strong> <strong>Home</strong> <strong>Content</strong> <strong>Libraries</strong>.</p></li><li><p><strong>Select the Content Library</strong> under which the NSX-ALB OVA is placed.</p></li><li><p>Click <strong>OVA & OVF Templates</strong>.</p></li><li><p>Right-click <strong>NSX ALB Image</strong> and select <strong>New VM from this Template</strong>.</p></li><li><p>On the <strong>Select name and folder</strong> page, enter a <strong>name</strong> and select a <strong>folder</strong> for the NSX Advanced Load Balancer VM as <strong>nsx-alb-components</strong>.</p></li><li><p>On the <strong>Select a compute resource</strong> page, select the <strong>resource pool</strong> as <strong>nsx-alb-components</strong>.</p></li><li><p>On the <strong>Review details</strong> page, verify the template details and click <strong>Next</strong>.</p></li><li><p>On the <strong>Select storage</strong> page, select a storage policy from the VM Storage Policy drop-down menu and choose the datastore location where you want to store the virtual machine files.</p></li><li><p>On the <strong>Select networks</strong> page, select the <code>nsx_alb_management_pg</code> network and click <strong>Next</strong>.</p></li><li><p>On the <strong>Customize Template</strong> page, provide the NSX Advanced Load Balancer Management <strong>network details</strong>, such as IP Address, Subnet Mask, and Gateway, and click <strong>Next</strong>.
<strong>Note:</strong> If you choose to use DHCP, you can leave these entries blank.</p></li><li><p>On the <strong>Ready to complete</strong> page, review the page and click <strong>Finish</strong>.</p></li></ol><p><img src=.//img/deployment-guides/tko-on-vsphere-vds/image95.png alt></p><p>A new task for creating the virtual machine appears in the Recent Tasks pane. After the task is complete, the NSX Advanced Load Balancer virtual machine is created on the selected resource. Power on the virtual machine and give it few minutes for the system to boot.</p><p><strong>Note:</strong> While the system is booting up, a blank web page or a 503 status code may appear.</p><h3 id=nsx-advanced-load-balancer-initial-setup>NSX Advanced Load Balancer: Initial setup</h3><p>Once the NSX Advanced Load Balancer is successfully deployed and boots up, navigate to NSX Advanced Load Balancer on your browser using the URL “https://&lt;AVI_IP/FQDN>” and configure the basic system settings:</p><ol><li><p>Administrator account setup.
Set an admin password and click <strong>Create Account</strong>.
<img src=.//img/deployment-guides/tko-on-vsphere-vds/image59.png alt></p></li><li><p>On the <strong>Welcome admin</strong> page:</p></li><li><p>Under <strong>System Settings</strong>: Set backup <strong>Passphrase</strong> and provide <strong>DNS</strong> information and click <strong>Next</strong>.
<img src=.//img/deployment-guides/tko-on-vsphere-vds/image17.png alt></p></li><li><p>Under <strong>Email/SMTP</strong>: Provide <strong>Email</strong> or <strong>SMTP</strong> information.
<img src=.//img/deployment-guides/tko-on-vsphere-vds/image25.png alt></p></li><li><p>Under <strong>Multi-Tenant</strong>: Configure settings as shown and click <strong>Save
IP Route Domain</strong>: Share IP route domain across tenants
<strong>Service Engines are managed within the</strong>: Provider (Shared across tenants)
<strong>Tenant Access to Service Engine</strong>: Read Access
<img src=.//img/deployment-guides/tko-on-vsphere-vds/image23.png alt></p></li><li><p>Go to <strong>Administration</strong> <strong>Settings</strong> <strong>DNS/NTP Edit</strong> to add your NTP server details and <strong>Save</strong>.</p><p><strong>Note:</strong> You may also delete the default NTP servers.</p><p><img src=.//img/deployment-guides/tko-on-vsphere-vds/image82.png alt></p></li></ol><h3 id=nsx-advanced-load-balancer-licensing>NSX Advanced Load Balancer: Licensing</h3><p>By default, the evaluation license uses Enterprise licensing. If you plan to use the Enterprise Licensing for your deployment, you may add your license key in the licensing section. If not, change the license model to Essentials.</p><p>See <a href=https://avinetworks.com/docs/21.1/nsx-license-editions/>NSX Advanced Load Balancer Editions</a> for a comparison of available licensing editions.</p><p>To change the license edition to Essentials:</p><ol><li><p>Log in to NSX Advanced Load Balancer <strong>Administration</strong> <strong>Settings</strong> <strong>Licensing</strong>. On the <strong>Licensing</strong> page, click <strong>gear</strong> <strong>icon</strong> next to Licensing**.
**<img src=.//img/deployment-guides/tko-on-vsphere-vds/image92.png alt></p></li><li><p>Select <strong>Essentials License</strong> and click <strong>Save</strong>.
<img src=.//img/deployment-guides/tko-on-vsphere-vds/image41.png alt></p></li></ol><h3 id=nsx-advanced-load-balancer-controller-high-availability>NSX Advanced Load Balancer: Controller High Availability</h3><p>NSX Advanced Load Balancer can run with a single controller (single-node deployment) or with a 3-node controller cluster. In a deployment that uses a single controller, that controller performs all administrative functions as well as all analytics data gathering and processing.</p><p>Adding 2 additional nodes to create a 3-node cluster provides node-level redundancy for the controller and also maximizes performance for CPU-intensive analytics functions.</p><p>In a 3-node NSX Advanced Load Balancer Controller cluster, one node is the primary (leader) node and performs the administrative functions. The other two nodes are followers (secondaries) and perform data collection for analytics, in addition to standing by as backups for the leader.
Perform the following steps to configure AVI Advanced Load Balancer HA:</p><ol><li><p><strong>Set the Cluster IP for the NSX Advanced Load Balancer controller</strong>
Log in to the primary NSX Advanced Load Balancer controller. Navigate to <strong>Administrator</strong> <strong>Controller</strong> <strong>Nodes,</strong> and click <strong>Edit</strong>. The Edit Controller Configuration popup appears</p></li><li><p>In the Controller Cluster IP field, enter the <strong>Controller Cluster IP</strong> for the controller and click <strong>Save</strong>.
<img src=.//img/deployment-guides/tko-on-vsphere-vds/image53.png alt></p></li><li><p>Deploy the 2nd and 3rd NSX Advanced Load Balancer nodes, following the steps provided in <a href=#deploy-nsx-advanced-load-balancer>Deploy NSX Advanced Load Balancer</a>.</p></li><li><p>Log in to the Primary NSX Advanced Load Balancer controller using the Controller Cluster IP/FQDN, navigate to <strong>Administrator</strong> <strong>Controller</strong> <strong>Nodes,</strong> and click <strong>Edit</strong>. The <strong>Edit Controller</strong> Configuration popup appears.</p></li><li><p>In the <strong>Cluster Nodes</strong> field, enter the IP address for the 2nd and 3rd controllers and click <strong>Save</strong>.</p></li></ol><p><strong>Optional:</strong> Provide a friendly name for all 3 nodes.
<img src=.//img/deployment-guides/tko-on-vsphere-vds/image86.png alt></p><p>After these steps, the primary Avi Controller becomes the leader for the cluster and invites the other controllers to the cluster as members.</p><p>NSX Advanced Load Balancer then performs a warm reboot of the cluster. This process can take 2-3 minutes. The configuration of the primary (leader) controller is synchronized to the new member nodes when the cluster comes online following the reboot.</p><p>Once the cluster is successfully formed, you should see the following status:
<img src=.//img/deployment-guides/tko-on-vsphere-vds/image36.png alt></p><p><strong>Note:</strong> After the cluster is formed, all NSX Advanced Load Balancer configurations are done by connecting to the NSX Advanced Load Balancer Controller Cluster IP/FQDN.</p><h3 id=nsx-advanced-load-balancer-certificate-management>NSX Advanced Load Balancer: Certificate Management</h3><p>The default system-generated controller certificate generated for SSL/TSL connections will not have required SAN entries. Follow the following steps to create a Controller certificate:</p><ol><li><p>Log in to NSX Advanced Load Balancer Controller <strong>Templates</strong> <strong>Security</strong> <strong>SSL/TLS Certificates</strong></p></li><li><p>Click <strong>Create</strong> and select <strong>Controller Certificate</strong></p></li><li><p>You can either generate a Self-Signed certificate, generate a Certificate Signing Request (CSR), or import a certificate.
For the purpose of this document, a self-signed certificate will be generated.</p></li><li><p>Provide all required details as per your infrastructure requirements. Under <strong>the Subject Alternate Name</strong> (SAN) section, provide the IP address and Fully Qualified Domain Name (FQDN) of all NSX Advanced Load Balancer controllers, including the IP address and FQDN of the NSX Advanced Load Balancer cluster, and click <strong>Save</strong>.
<img src=.//img/deployment-guides/tko-on-vsphere-vds/image66.png alt></p></li><li><p>Once the certificate is created, capture the certificate contents. You will need this when you deploy the Tanzu Kubernetes Grid management cluster.</p></li></ol><p>To capture the certificate content, click the “Download” icon next to the certificate, and then click “Copy to clipboard” under the <strong>Certificate</strong> section.
<img src=.//img/deployment-guides/tko-on-vsphere-vds/image29.png alt></p><ol><li><p>To replace the certificate, navigate to <strong>Administration</strong> <strong>Settings</strong> <strong>Access</strong> <strong>Settings</strong>, and click the pencil icon at the top right to <strong>edit</strong> the System Access Settings, replace the SSL/TSL certificate, and click <strong>Save</strong>.
<img src=.//img/deployment-guides/tko-on-vsphere-vds/image72.png alt></p></li><li><p>To complete the certificate configuration, log out and log back in to the NSX Advanced Load Balancer.</p></li></ol><h3 id=nsx-advanced-load-balancer-create-vcenter-cloud-and-se-groups>NSX Advanced Load Balancer: Create vCenter Cloud and SE Groups</h3><p>Avi Vantage may be deployed in multiple environments for the same system. Each environment is called a cloud. The following procedure provides steps on how to create a VMware vCenter cloud. As shown in the architecture, two Service Engine (SE) Groups will be created:
<strong>Service Engine Group 1</strong>: Service engines in this group host:</p><ul><li><p>Virtual services for all load balancer functionalities requested by the Tanzu Kubernetes Grid management and workload clusters</p></li><li><p>Virtual services that load balance the control plane nodes of all Tanzu Kubernetes Grid Kubernetes clusters</p></li></ul><p><strong>Service Engine Group 2</strong>: Service engines part of this Service Engine group hosts virtual services for all load balancer functionalities requested by Tanzu Kubernetes Grid Workload clusters mapped to this SE group.</p><ul><li><p>Based on your requirements, you can create additional Service Engine groups for the workload clusters.</p></li><li><p>Multiple Workload clusters can be mapped to a single SE group.</p></li><li><p>A Tanzu Kubernetes Grid cluster can be mapped to only one SE group for Advanced Load Balancer services.
See <a href=#configure-nsx-advanced-load-balancer-in-tkg-workload-cluster>Configure NSX Advanced Load Balancer in Tanzu Kubernetes Grid Workload Cluster</a> for more details on mapping a specific SE group to a Tanzu Kubernetes Grid workload cluster.</p></li></ul><p>The following components will be created in NSX Advanced Load Balancer:</p><table><thead><tr><th>Object</th><th>Sample Name</th></tr></thead><tbody><tr><td>vCenter Cloud</td><td><code>tanzu-vcenter01</code></td></tr><tr><td>Service Engine Group 1</td><td><code>tanzu-mgmt-segroup-01</code></td></tr><tr><td>Service Engine Group 2</td><td><code>tanzu-wkld-segroup-01</code></td></tr></tbody></table><ol><li><p>Log in to <strong>NSX ALB > Infrastructure > Clouds > Create > VMware vCenter/vSphere ESX</strong>
<img src=.//img/deployment-guides/tko-on-vsphere-vds/image18.png alt></p></li><li><p>Provide the cloud name and click <strong>Next</strong>.
<img src=.//img/deployment-guides/tko-on-vsphere-vds/image38.png alt></p></li><li><p>Under the <strong>Infrastructure</strong> pane, provide <strong>vCenter Address</strong>, <strong>username</strong>, and <strong>password</strong>. Set <strong>Access</strong> <strong>Permission</strong> to &ldquo;<strong>Write</strong>&rdquo; and click <strong>Next</strong>.
<img src=.//img/deployment-guides/tko-on-vsphere-vds/image58.png alt></p></li><li><p>Under the <strong>Datacenter</strong> pane, choose the datacenter for NSX Advanced Load Balancer to discover infrastructure resources.
<img src=.//img/deployment-guides/tko-on-vsphere-vds/image74.png alt></p></li><li><p>Under the <strong>Network</strong> pane, choose the NSX Advanced Load Balancer <strong>Management</strong> <strong>Network</strong> for Service Engines and provide a <strong>Static</strong> <strong>IP</strong> <strong>pool</strong> for SEs and VIP and click <strong>Complete</strong>.
<img src=.//img/deployment-guides/tko-on-vsphere-vds/image45.png alt></p></li><li><p>Wait for the cloud to configure and the cloud status indicator to turn green.
<img src=.//img/deployment-guides/tko-on-vsphere-vds/image1.png alt></p></li><li><p>To create a Service Engine group for Tanzu Kubernetes Grid management clusters, click the <strong>Service Engine Group</strong> tab. Under <strong>Select Cloud</strong>, choose the cloud created in the previous step, and click <strong>Create</strong>.</p></li><li><p>Provide a name for the Tanzu Kubernetes Grid management Service Engine group and set the following parameters:</p></li></ol><table><thead><tr><th>Parameter</th><th>Value</th></tr></thead><tbody><tr><td>High availability mode</td><td>Active/Standby (Tanzu Essentials License supports only Active/Standby Mode</td></tr><tr><td>Memory per Service Engine</td><td>4</td></tr><tr><td>vCPU per Service Engine</td><td>2</td></tr></tbody></table><p>Leave the remaining parameters at their default settings.
<img src=.//img/deployment-guides/tko-on-vsphere-vds/image79.png alt></p><ol start=9><li><p>For advanced configuration, click the <strong>Advanced</strong> tab. Here you can set the AVI service engine folder and service engine name prefix and select a specific cluster and datastore for service engine placement. Click <strong>Save</strong> when you are done.
<img src=.//img/deployment-guides/tko-on-vsphere-vds/image94.png alt></p></li><li><p>Follow steps 7 and 8 to create another SE group for Tanzu Kubernetes Grid workload clusters. Once complete, there will be two SE groups created.
<img src=.//img/deployment-guides/tko-on-vsphere-vds/image20.png alt></p></li></ol><h3 id=nsx-advanced-load-balancer-configure-network-and-ipam-profile>NSX Advanced Load Balancer: Configure Network and IPAM Profile</h3><h4 id=configure-tanzu-kubernetes-grid-networks-in-nsx-advanced-load-balancer>Configure Tanzu Kubernetes Grid Networks in NSX Advanced Load Balancer</h4><p>As part of the cloud creation in NSX Advanced Load Balancer, only the Management Network has been configured in NSX Advanced Load Balancer. Follow this procedure to configure these networks:</p><ul><li>TKG Management Network</li><li>TKG Workload Network</li><li>TKG Cluster VIP/Data Network</li><li>TKG Management VIP/Data Network</li><li>TKG Workload VIP/Data Network</li></ul><ol><li><p>Log in to NSX Advanced Load Balancer <strong>Infrastructure</strong> <strong>Networks</strong></p></li><li><p>Select the appropriate Cloud</p></li></ol><p>All the networks available in vCenter will be listed.
<img src=.//img/deployment-guides/tko-on-vsphere-vds/image93.png alt></p><ol><li>Click the edit icon for the network and configure as shown. Change the details provided in the table according to your SDDC configuration.
<strong>Note:</strong> Not all networks will be auto-discovered. For those that are not, add the subnet manually.</li></ol><table><thead><tr><th>Network Name</th><th>DHCP</th><th>Subnet</th><th>Static IP Pool</th></tr></thead><tbody><tr><td><code>tkg_mgmt_pg</code></td><td>Yes</td><td>172.16.40.0/24</td><td>NA</td></tr><tr><td><code>tkg_workload_pg</code></td><td>Yes</td><td>172.16.60.0/24</td><td>NA</td></tr><tr><td><code>tkg_cluster_vip_pg</code></td><td>No</td><td>172.16.80.0/24</td><td>172.16.80.100 - 172.16.80.200</td></tr><tr><td><code>tkg_mgmt_vip_pg</code></td><td>No</td><td>172.16.50.0/24</td><td>172.16.50.100 - 172.16.50.200</td></tr><tr><td><code>tkg_workload_vip_pg</code></td><td>No</td><td>172.16.70.0/24</td><td>172.16.70.100 - 172.16.70.200</td></tr></tbody></table><p>The following snippet shows the configuration of one of the networks, for example: <code>tkg_cluster_vip_pg</code>:
<img src=.//img/deployment-guides/tko-on-vsphere-vds/image64.png alt></p><p>Once the networks are configured, the configuration must look as shown:
<img src=.//img/deployment-guides/tko-on-vsphere-vds/image22.png alt></p><ol><li>After the networks are configured, set the default routes for all VIP/Data networks. Click <strong>Routing</strong>. Create and add default routes for following networks. Change the gateway subnet to match your network configuration:</li></ol><table><thead><tr><th>Network Name</th><th>Gateway Subnet</th><th>Next Hop</th></tr></thead><tbody><tr><td>tkg_cluster_vip_pg</td><td>0.0.0.0/0</td><td>172.16.80.1</td></tr><tr><td>tkg_mgmt_vip_pg</td><td>0.0.0.0/0</td><td>172.16.50.1</td></tr><tr><td>tkg_workload_vip_pg</td><td>0.0.0.0/0</td><td>172.16.70.1</td></tr></tbody></table><p><img src=.//img/deployment-guides/tko-on-vsphere-vds/image80.png alt></p><h4 id=create-ipam-profile-in-nsx-advanced-load-balancer-and-attach-it-to-cloud>Create IPAM Profile in NSX Advanced Load Balancer and Attach it to Cloud</h4><p>At this point, all the networks required for Tanzu functionality are configured in NSX Advanced Load Balancer, except for the TKG Management and Workload Networks, which use DHCP. NSX Advanced Load Balancer provides IPAM service for the TKG Cluster VIP network, the TKG Management VIP network, and the TKG Workload VIP network.
The following procedure creates an IPAM profile and attaches it to the vCenter cloud created earlier.</p><ol><li>Log in to NSX Advanced Load Balancer <strong>Infrastructure</strong> <strong>Templates</strong> <strong>IPAM/DNS Profiles</strong> <strong>Create</strong> <strong>IPAM Profile</strong> and provide the following details and click <strong>Save</strong>.</li></ol><table><thead><tr><th>Parameter</th><th>Value</th></tr></thead><tbody><tr><td>Name</td><td>tanzu-vcenter-ipam-01</td></tr><tr><td>Type</td><td>AVI Vintage IPAM</td></tr><tr><td>Cloud for Usable Networks</td><td>Tanzu-vcenter-01, created here</td></tr><tr><td>Usable Networks</td><td>tkg_cluster_vip_pgtkg_mgmt_vip_pgtkg_workload_vip_pg</td></tr></tbody></table><p><img src=.//img/deployment-guides/tko-on-vsphere-vds/image21.png alt></p><ol start=2><li><p>Attach the IPAM profile to the “tanzu-vcenter-01” cloud:</p></li><li><p>Navigate to <strong>Infrastructure</strong> <strong>Clouds</strong>.</p></li><li><p>Edit the <strong>tanzu-vcenter-01</strong> cloud.</p></li><li><p>Under <strong>IPAM Profile</strong>, choose the profile created in the previous step and <strong>Save</strong> the configuration,</p></li></ol><p><img src=.//img/deployment-guides/tko-on-vsphere-vds/image50.png alt></p><p>This completes the NSX Advanced Load Balancer configuration. Next is to deploy and configure the bootstrap machine that will be used to deploy and manage Tanzu Kubernetes clusters.</p><h2 id=a-iddeploy-and-configure-bootstrap-machineadeploy-and-configure-bootstrap-machine>Deploy and Configure Bootstrap Machine</h2><p>The bootstrap machine can be a laptop, host, or server (running on a Linux/MAC/Windows platform) that you deploy management and workload clusters from, and that keeps the Tanzu and Kubernetes configuration files for your deployments. The bootstrap machine is typically local.</p><p>For the purpose of this document, the bootstrap server is a Photon-based virtual machine. See <a href=https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid/1.5/vmware-tanzu-kubernetes-grid-15/GUID-install-cli.html>Install the Tanzu CLI and Other Tools</a> to configure MAC/Windows machines.</p><ol><li><p>Ensure that the bootstrap VM is connected to TKG Management Network <code>tkg_mgmt_pg</code>.</p></li><li><p><a href=https://kb.vmware.com/s/article/76088>Configure NTP</a> on your bootstrap machine.</p></li><li><p>For this example using the Photon OS, download and unpack following Linux CLI packages from <a href="https://customerconnect.vmware.com/en/downloads/details?downloadGroup=TKG-140&productId=988&rPId=49705">myvmware</a></p><ul><li><p>VMware Tanzu CLI for Linux</p></li><li><p>kubectl cluster cli v1.21.2 for Linux</p></li></ul></li><li><p>Execute the following commands to install the Tanzu Kubernetes Grid CLI, Kubectl CLIs, and Carvel tools:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e>## Install required packages</span>
</span></span><span style=display:flex><span>tdnf install tar zip unzip wget -y
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>## Install TKG CLI</span>
</span></span><span style=display:flex><span>tar -xvf tanzu-cli-bundle-linux-amd64.tar
</span></span><span style=display:flex><span>cd ./cli/
</span></span><span style=display:flex><span>sudo install core/v1.5.0/tanzu-core-linux_amd64 /usr/local/bin/tanzu
</span></span><span style=display:flex><span>chmod +x /usr/local/bin/tanzu
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>## Install TKG CLI Plugins</span>
</span></span><span style=display:flex><span>tanzu plugin install --local ./cli all
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>## Install Kubectl CLI</span>
</span></span><span style=display:flex><span>gunzip kubectl-linux-v1.21.2+vmware.1.gz
</span></span><span style=display:flex><span>mv kubectl-linux-v1.21.2+vmware.1 /usr/local/bin/kubectl <span style=color:#f92672>&amp;&amp;</span> chmod +x /usr/local/bin/kubectl
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Instal Carvel tools</span>
</span></span><span style=display:flex><span>cd ./cli
</span></span><span style=display:flex><span>gunzip ytt-linux-amd64-v0.34.0+vmware.1.gz
</span></span><span style=display:flex><span>chmod ugo+x ytt-linux-amd64-v0.34.0+vmware.1 <span style=color:#f92672>&amp;&amp;</span> mv ./ytt-linux-amd64-v0.34.0+vmware.1 /usr/local/bin/ytt
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>cd ./cli
</span></span><span style=display:flex><span>gunzip kapp-linux-amd64-v0.37.0+vmware.1.gz
</span></span><span style=display:flex><span>chmod ugo+x kapp-linux-amd64-v0.37.0+vmware.1 <span style=color:#f92672>&amp;&amp;</span> mv ./kapp-linux-amd64-v0.37.0+vmware.1 /usr/local/bin/kapp
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>cd ./cli
</span></span><span style=display:flex><span>gunzip kbld-linux-amd64-v0.30.0+vmware.1.gz
</span></span><span style=display:flex><span>chmod ugo+x kbld-linux-amd64-v0.30.0+vmware.1 <span style=color:#f92672>&amp;&amp;</span> mv ./kbld-linux-amd64-v0.30.0+vmware.1 /usr/local/bin/kbld
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>cd ./cli
</span></span><span style=display:flex><span>gunzip imgpkg-linux-amd64-v0.10.0+vmware.1.gz
</span></span><span style=display:flex><span>chmod ugo+x imgpkg-linux-amd64-v0.10.0+vmware.1 <span style=color:#f92672>&amp;&amp;</span> mv ./imgpkg-linux-amd64-v0.10.0+vmware.1 /usr/local/bin/imgpkg
</span></span></code></pre></div></li><li><p>Validate the Carvel tools installation using the following commands:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>ytt version
</span></span><span style=display:flex><span>kapp version
</span></span><span style=display:flex><span>kbld version
</span></span><span style=display:flex><span>imgpkg version
</span></span></code></pre></div></li><li><p>Install <code>yq</code>. <code>yq</code> is a lightweight and portable command-line YAML processor. <code>yq</code> uses <code>jq</code>-like syntax but works with yaml files as well as json.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>wget https://github.com/mikefarah/yq/releases/download/v4.13.4/yq_linux_amd64.tar.gz
</span></span><span style=display:flex><span>tar -xvf yq_linux_amd64.tar <span style=color:#f92672>&amp;&amp;</span> mv yq_linux_amd64 /usr/local/bin/yq
</span></span></code></pre></div></li><li><p>By default, Photon OS has Docker installed. Use the following commands to start the Docker service and enable it to start at boot.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e>## Check Docker service status</span>
</span></span><span style=display:flex><span>systemctl status docker
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>## Start Docker Service</span>
</span></span><span style=display:flex><span>systemctl start docker
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>## To start Docker Service at boot</span>
</span></span><span style=display:flex><span>systemctl enable docker
</span></span></code></pre></div></li><li><p>Ensure that the bootstrap machine is using <a href=https://man7.org/linux/man-pages/man7/cgroups.7.html>cgroup v1</a> by running the following command:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>docker info | grep -i cgroup
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>## You should see the following:</span>
</span></span><span style=display:flex><span>Cgroup Driver: cgroupfs
</span></span></code></pre></div></li><li><p>Create an SSH Key Pair. This is required for Tanzu CLI to connect to vSphere from the bootstrap machine.
The public key part of the generated key will be passed during the Tanzu Kubernetes Grid management cluster deployment.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e>## Generate SSH key pair</span>
</span></span><span style=display:flex><span><span style=color:#75715e>## When prompted enter file in which to save the key (/root/.ssh/id_rsa): press Enter to accept the default and provide password</span>
</span></span><span style=display:flex><span>ssh-keygen -t rsa -b <span style=color:#ae81ff>4096</span> -C <span style=color:#e6db74>&#34;email@example.com&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>## Add the private key to the SSH agent running on your machine, and enter the password you created in the previous step.</span>
</span></span><span style=display:flex><span>ssh-add ~/.ssh/id_rsa
</span></span><span style=display:flex><span><span style=color:#75715e>## If this command fails, execute &#34;eval $(ssh-agent)&#34; and then rerun the command.</span>
</span></span></code></pre></div></li><li><p>If your bootstrap machine runs Linux or Windows Subsystem for Linux, and it has a Linux kernel built after the May 2021 Linux security patch, for example Linux 5.11 and 5.12 with Fedora, run the following:</p><p><code>sudo sysctl net/netfilter/nf_conntrack_max=131072</code></p></li></ol><p>Now all the required packages are installed and the required configurations are in place in the bootstrap virtual machines. Proceed to the next section to deploy the Tanzu Kubernetes Grid management cluster.</p><h2 id=a-iddeploy-tanzu-kubernetes-grid-tkg-management-cluster-a-deploy-tanzu-kubernetes-grid-management-cluster>Deploy Tanzu Kubernetes Grid Management Cluster</h2><p>After you have performed the steps described in <a href=#deploy-and-configure-bootstrap-machine>deploy and configure bootstrap machine</a>, you can deploy the Tanzu Kubernetes Grid management cluster.</p><p>The management cluster is a Kubernetes cluster that runs cluster API operations on a specific cloud provider to create and manage workload clusters on that provider.</p><p>The management cluster is also where you configure the shared and in-cluster services utilized by the workload clusters. You may deploy management clusters in two ways:</p><ol><li><p>Run the Tanzu Kubernetes Grid installer, a wizard interface that guides you through the process of deploying a management cluster. This is the recommended method.</p></li><li><p>Create and edit YAML configuration files, and use them with CLI commands to deploy a management cluster.</p></li></ol><p>You may deploy and manage Tanzu Kubernetes Grid management clusters on:</p><ul><li><p>vSphere 6.7u3</p></li><li><p>vSphere 7, if vSphere with Tanzu is not enabled.</p></li></ul><h3 id=a-idimport-base-image-template-for-tkg-cluster-deployment-a-import-base-image-template-for-tanzu-kubernetes-grid-cluster-deployment>Import Base Image template for Tanzu Kubernetes Grid Cluster Deployment</h3><p>Before create the management cluster, ensure that the base image template is imported into vSphere and is available as a template. To import a base image template into vSphere:</p><ol><li><p>Go to the <a href="https://customerconnect.vmware.com/en/downloads/details?downloadGroup=TKG-154&productId=988&rPId=49705">Tanzu Kubernetes Grid downloads</a> page, and download a Tanzu Kubernetes Grid OVA for the cluster nodes.</p><ol><li><p>For the management cluster, this must be either a Photon- or Ubuntu-based Kubernetes v1.21.2 OVA.</p><p><strong>Note:</strong> Custom OVA with a custom Tanzu Kubernetes release (TKr) is also supported, as described in <a href=https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid/1.5/vmware-tanzu-kubernetes-grid-15/GUID-build-images-index.html>Build Machine Images</a>.</p></li><li><p>For <strong>workload clusters,</strong> OVA can have any supported combination of OS and Kubernetes versions, as packaged in a Tanzu Kubernetes release</p></li></ol><p><strong>Important</strong>: Make sure you download the most recent OVA base image templates in the event of security patch releases. You can find updated base image templates that include security patches on the Tanzu Kubernetes Grid product download page.</p></li><li><p>In the vSphere Client, right-click an object in the vCenter Server inventory, select <strong>Deploy OVF template</strong>.</p></li><li><p>Select <strong>Local file</strong>, click the button to upload files, and navigate to the downloaded OVA file on your local machine.</p></li><li><p>Follow the installer prompts to deploy a VM from the OVA.</p></li><li><p>Click <strong>Finish</strong> to deploy the VM. When the OVA deployment finishes, right-click the VM and select <strong>Template</strong> <strong>Convert to Template</strong>.
<strong>NOTE:</strong> Do not power on the VM before you convert it to a template.</p></li><li><p><strong>If using non administrator SSO account</strong>: In the VMs and Templates view, right-click the new template, select <strong>Add Permission</strong>, and assign the <strong>tkg-user</strong> to the template with the <strong>TKG role</strong>.
For information about how to create the user and role for Tanzu Kubernetes Grid, see <a href=https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid/1.5/vmware-tanzu-kubernetes-grid-15/GUID-mgmt-clusters-vsphere.html#required-permissions-for-the-vsphere-account-5>Required Permissions for the vSphere Account</a>.</p></li></ol><h3 id=deploy-tanzu-kubernetes-grid-management-cluster-from-the-ui>Deploy Tanzu Kubernetes Grid Management Cluster from the UI</h3><p><strong>Important</strong>: If you are deploying Tanzu Kubernetes Grid clusters in an Internet- restricted environment, ensure that the local image repository is accessible from the boot strap machine and the TKG Management and Workload Networks.
For the boot strap machine to pull images from the private image repository, set the <code>TKG_CUSTOM_IMAGE_REPOSITORY</code> environment variable tpo point to the repository.</p><p>Once this is set, Tanzu Kubernetes Grid will pull images from your local private registry rather than from the external public registry. To make sure that Tanzu Kubernetes Grid always pulls images from the local private registry, add “TKG_CUSTOM_IMAGE_REPOSITORY” to the global cluster configuration file, <code>~/.config/tanzu/tkg/config.yaml</code>.</p><p>If your local image repository uses self-signed certificates, also add <code>TKG_CUSTOM_IMAGE_REPOSITORY_CA_CERTIFICATE</code> to the global cluster configuration file. Provide the CA certificate in <code>base64</code> encoded format by executing <code>base64 -w 0 your-ca.crt</code>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>TKG_CUSTOM_IMAGE_REPOSITORY: custom-image-repository.io/yourproject
</span></span><span style=display:flex><span>TKG_CUSTOM_IMAGE_REPOSITORY_SKIP_TLS_VERIFY: false
</span></span><span style=display:flex><span>TKG_CUSTOM_IMAGE_REPOSITORY_CA_CERTIFICATE: LS0t<span style=color:#f92672>[</span>...<span style=color:#f92672>]</span>tLS0tLQ<span style=color:#f92672>==</span>
</span></span></code></pre></div><p>Do the following to deploy the Tanzu Kubernetes Grid management cluster using the Installer Interface:</p><ol><li><p>To launch the installer wizard UI, run the following command on the bootstrap machine:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>tanzu management-cluster create --ui --bind &lt;bootstrapper-ip&gt;:&lt;port&gt; --browser none
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>## For example</span>
</span></span><span style=display:flex><span>tanzu management-cluster create --ui --bind 172.16.40.135:8000 --browser none
</span></span></code></pre></div></li><li><p>Access the Tanzu UI wizard by opening a browser and entering <code>http://&lt;bootstrapper-ip>:port/</code>.</p><p><img src=.//img/deployment-guides/tko-on-vsphere-vds/image91.png alt></p></li><li><p>Click <strong>Deploy</strong> on the <strong>VMware vSphere</strong> tile</p></li><li><p>On the &ldquo;<strong>IaaS Provider</strong>&rdquo; section, enter the IP/FQDN and credentials of the vCenter server where the Tanzu Kubernetes Grid management cluster will be deployed</p><p><img src=.//img/deployment-guides/tko-on-vsphere-vds/image49.png alt></p></li><li><p>Click <strong>Connect</strong> and accept the vCenter Server SSL thumbprint</p></li><li><p>If you are running on a vCenter 7.x environment, you would get the following popup. Select “DEPLOY TKG MANAGEMENT CLUSTER” to continue.
<img src=.//img/deployment-guides/tko-on-vsphere-vds/image87.png alt></p></li><li><p>Select the <strong>Datacenter</strong> and provide the <strong>SSH Public Key</strong> generated while configuring the Bootstrap VM .</p><p>If you have saved the SSH key in the default location, execute the following command in your bootstrap machine to get the SSH public key:
<code>cat /root/.ssh/id_rsa.pub</code></p></li><li><p>Click <strong>Next</strong></p><p><img src=.//img/deployment-guides/tko-on-vsphere-vds/image5.png alt></p></li><li><p>On the <strong>Management cluster settings</strong> section, provide the following details.</p><ol><li><p>Based on your environment requirements, select the appropriate <strong>deployment type</strong> for the Tanzu Kubernetes Grid management cluster</p><ul><li><p><strong>Development</strong>: Recommended for Dev or POC environments</p></li><li><p><strong>Production</strong>: Recommended for Production environments</p></li></ul></li><li><p>VMware recommends that you set the <strong>instance</strong> <strong>type</strong> to <strong>Large</strong> or greater.</p></li></ol><p>For the purpose of this document, the deployment type is <strong>Development</strong> and instance type is <strong>Large</strong>.</p></li><li><p><strong>Management Cluster Name</strong>: Name for your management cluster.</p></li><li><p><strong>Control Plane Endpoint Provider</strong>: Select NSX Advanced Load Balancer for the Control Plane HA.</p></li><li><p><strong>Control Plane Endpoint</strong>: This field is optional. If left blank, NSX Advanced Load Balancer will assign an IP address from the <code>tkg_cluster_vip_pg</code> pool created earlier.</p><p>If you need to provide an IP address, pick an unused IP address from the <code>tkg_cluster_vip_pg</code> static IP pools configured in AVI.</p></li><li><p><strong>Machine Health Checks</strong>: Enable</p></li><li><p><strong>Enable Audit Logging</strong>: Enables audit logging for Kubernetes API server and node VMs. Set according to your environmental needs. For more information, see <a href=https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid/1.5/vmware-tanzu-kubernetes-grid-15/GUID-troubleshooting-tkg-audit-logging.html>Audit Logging</a></p></li><li><p>Click <strong>Next</strong></p><p><img src=.//img/deployment-guides/tko-on-vsphere-vds/image27.png alt></p></li><li><p>In the <strong>NSX Advanced Load Balancer</strong> section, provide the following:</p><ul><li><p><strong>Controller Host</strong>: NSX Advanced Load Balancer Controller IP/FQDN (Advanced Load Balancer Controller cluster IP/FQDN of the controller cluster is configured)</p></li><li><p>Controller credentials: <strong>Username</strong> and <strong>Password</strong> of NSX Advanced Load Balancer</p></li><li><p><strong>Controller certificate</strong></p></li></ul></li><li><p>Click “<strong>Verify Credentials</strong>” and set the following parameters:</p><ul><li><p><strong>Cloud Name</strong>: Name of the cloud created while configuring NSX Advanced Load Balancer <code>tanzu-vcenter-01</code></p></li><li><p><strong>Service Engine Group Name</strong>: Name of the Service Engine Group created for Tanzu Kubernetes Grid management clusters created while configuring NSX Advanced Load Balancer <code>tanzu-mgmt-segroup-01</code></p></li><li><p><strong>Workload VIP Network Name</strong>: Select <code>tkg_mgmt_vip_pg</code> as the TKG Management VIP/Data Network network and select the discovered subnet.</p></li><li><p><strong>Workload VIP network CIDR</strong>: Select the discovered subnet. In our example, <code>172.16.50.0/24</code></p></li><li><p><strong>Management VIP network Name</strong>: Select <code>tkg_cluster_vip_pg</code> as the TKG Cluster VIP/Data Network network.</p></li><li><p><strong>Cluster Labels</strong>: To adhere to the architecture, defining a label is <strong>mandatory</strong>. Provide the required labels, for example, <strong>type</strong>:<strong>management</strong>
<strong>Note:</strong> Based on your requirements, you may specify multiple labels.</p></li></ul></li><li><p>Click <strong>Next</strong>.</p><p><img src=.//img/deployment-guides/tko-on-vsphere-vds/image8.png alt></p><p><strong>Important</strong>: With the above configurations, when Tanzu Kubernetes Grid clusters (shared service/workload) are tagged with the label <code>type=management</code>, an <code>ako</code> pod is deployed on the cluster. Any applications hosted on the cluster that require load balancing will be exposed via the <code>tkg_mgmt_vip_pg</code> network, and the virtual service will be placed in the <code>tanzu-mgmt-segroup-01</code> SE group.</p><p>As defined in the architecture, the <strong>Cluster Labels</strong> specified here will be applied <strong>only on shared service clusters</strong>.</p><p>If no labels are specified in the “Cluster Labels” section, an AKO pod is deployed on all clusters without any labeling requirement, which deviates from the defined architecture.</p></li><li><p>On the <strong>Metadata</strong> page, you can specify location and labels and click <strong>Next</strong>. These are <strong>optional</strong>.
<img src=.//img/deployment-guides/tko-on-vsphere-vds/image68.png alt></p></li><li><p>On the <strong>Resources</strong> section, specify the resources to be consumed by Tanzu Kubernetes Grid management cluster and click <strong>Next</strong>.
<img src=.//img/deployment-guides/tko-on-vsphere-vds/image71.png alt></p></li><li><p>On the Kubernetes Network section, select the <strong>TKG Management Network (“tkg_mgmt_pg”)</strong> where the control plane and worker nodes will be placed during management cluster deployment. Ensure that the network has <strong>DHCP</strong> service enabled.</p><p>Optionally, change the <strong>Pod</strong> and <strong>Service CIDR</strong> if the default provided network is already in use in your environment.</p></li><li><p>If the Tanzu environment is placed behind a proxy, enable the proxy and provide the proxy details.
If using a <strong>proxy</strong>, the following details are the <strong>key points</strong>:</p><ul><li>If you set http-proxy, you must also set https-proxy and vice-versa.</li></ul></li><li><p>For the no-proxy section:</p><ul><li><p>For Tanzu Kubernetes Grid management and workload clusters, localhost, 127.0.0.1, the values of CLUSTER_CIDR and SERVICE_CIDR, .svc, and .svc.cluster.local values are appended along with the user-specified values.</p></li><li><p><strong>Important</strong>: If the Kubernetes cluster needs to communicate with external services and infrastructure endpoints in your Tanzu Kubernetes Grid environment, ensure that those endpoints are reachable by your proxies or add them to TKG_NO_PROXY. Depending on your environment configuration, this may include, but is not limited to, your OIDC or LDAP server, Harbor, NSX-T, and NSX Advanced Load Balancer, and vCenter.</p></li><li><p>For vSphere, you must manually add the CIDR of the TKG Management Network and Cluster VIP networks which includes the IP address of your control plane endpoints, to TKG_NO_PROXY.</p></li></ul><p><img src=.//img/deployment-guides/tko-on-vsphere-vds/image84.png alt></p></li><li><p>Optionally, specify <strong>Identity Management with OIDC or LDAPs</strong> - This is not covered in this document and will be documented separately.
For the purpose of this document, identity management integration has been deactivated .</p><p><img src=.//img/deployment-guides/tko-on-vsphere-vds/image35.png alt></p></li><li><p>Select the <strong>OS image</strong> that will be used for the management cluster deployment.
<strong>Note</strong>: This list will appear empty if you don’t have a compatible template present in your environment. See the steps provided in <a href=#import-base-image-template-for-tkg-cluster-deployment>Import Base Image template for TKG Cluster deployment</a>.</p><p><img src=.//img/deployment-guides/tko-on-vsphere-vds/image44.png alt></p></li><li><p><strong>Register Tanzu Mission Control</strong>: Registering Tanzu Kubernetes Grid management clusters with Tanzu Mission Control allows you to provision and manage Tanzu Kubernetes clusters by using the Tanzu Mission Control dashboard interface..</p></li><li><p>Check the <strong>Participate in the Customer Experience Improvement Program</strong>, if you so desire and click <strong>Review Configuration</strong></p></li><li><p>Review the entire configuration. After reviewing it, you can either copy the command provided and execute it on the CLI or proceed with UI to <strong>Deploy Management Cluster</strong>.</p><p>When the deployment is triggered from the UI, the installer wizard displays the deployment logs on the screen.</p><p><img src=.//img/deployment-guides/tko-on-vsphere-vds/image55.png alt></p></li></ol><p>While the cluster is being deployed, you will find that a virtual service will be created in NSX Advanced Load Balancer and new service engines will be deployed in vCenter by NSX Advanced Load Balancer. The service engines will be mapped to the SE Group <code>tanzu-mgmt-segroup-01</code>.</p><p>Behind the scenes when the Tanzu Kubernetes Grid management cluster is being deployed:</p><ul><li><p>NSX Advanced Load Balancer service engines are deployed in vCenter. This task is orchestrated by the NSX Advanced Load Balancer controller.</p><p><img src=.//img/deployment-guides/tko-on-vsphere-vds/image81.png alt></p></li><li><p>Service engine status in NSX Advanced Load Balancer: The following snippet shows that the first service engine has been initialized successfully and the second service engine is initializing.
<img src=.//img/deployment-guides/tko-on-vsphere-vds/image46.png alt></p></li><li><p>Service Engine Group Status in NSX Advanced Load Balancer: In our configuration, the virtual service required for the Tanzu Kubernetes Grid clusters control plane High Availability (HA) will be hosted on the <code>tgk-mgmt-segroup-01</code> service engine group.
<img src=.//img/deployment-guides/tko-on-vsphere-vds/image4.png alt></p></li><li><p>Virtual Service status in NSX Advanced Load Balancer
<img src=.//img/deployment-guides/tko-on-vsphere-vds/image90.png alt></p><p><img src=.//img/deployment-guides/tko-on-vsphere-vds/image33.png alt></p><p>The virtual service health is impacted because the second service engine is still being initialized; you can ignore this.</p></li><li><p>After the Tanzu Kubernetes Grid management cluster is successfully deployed, you will see this in the Tanzu Bootstrap UI:
<img src=.//img/deployment-guides/tko-on-vsphere-vds/image31.png alt></p></li><li><p>The installer will automatically set the context to the Tanzu Kubernetes Grid management cluster in the bootstrap machine.</p><p>Now you can access the Tanzu Kubernetes Grid management cluster from the bootstrap machine and perform additional tasks, such as verifying the management cluster health and deploying the workload clusters.</p><p>To get the status of Tanzu Kubernetes Grid management cluster, execute the following command:</p><p><code>tanzu management-cluster get</code></p><p><img src=.//img/deployment-guides/tko-on-vsphere-vds/image54.png alt></p></li><li><p>Use <code>kubectl</code> to get the status of the Tanzu Kubernetes Grid management cluster nodes
<img src=.//img/deployment-guides/tko-on-vsphere-vds/image47.png alt></p></li></ul><p>The Tanzu Kubernetes Grid management cluster is successfully deployed. Now you can create shared service and workload clusters.</p><h2 id=a-iddeploy-tanzu-shared-service-cluster-a-deploy-tanzu-kubernetes-grid-shared-service-cluster>Deploy Tanzu Kubernetes Grid Shared Service Cluster</h2><p>Each Tanzu Kubernetes Grid instance can have only one shared services cluster. Create a shared services cluster if you intend to deploy Harbor.</p><p>Deploying a shared service cluster is exactly the same as deploying a workload cluster, except that you will add the <strong>tanzu-services</strong> label to the shared services cluster as its cluster role. This label identifies the shared services cluster to the management cluster and workload clusters.
Another <strong>major difference</strong> between shared service clusters and workload clusters is that the “<strong>Cluster Labels</strong>” that were defined while deploying the management cluster will be applied to shared service clusters. This is to ensure that only shared service clusters will make use of the <strong>TKG Cluster VIP/Data Network</strong> for application load balancing and that virtual services are deployed on “<strong>Service Engine Group 1</strong>”.</p><p>In order to deploy a shared service cluster, you need to create a cluster config file in which you specify options to connect the shared service cluster to the vCenter server and to identify the vSphere resources that the cluster will use.
You can also specify standard sizes for the control plane and worker node VMs, or configure the CPU, memory, and disk sizes for control plane and worker nodes explicitly. If you use custom image templates, you can identify which template to use to create node VMs.</p><p>The following sample file includes the minimum required configurations. For information about all configuration file variables, see the <a href=https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid/1.5/vmware-tanzu-kubernetes-grid-15/GUID-tanzu-config-reference.html>Tanzu CLI Configuration File Variable Reference</a>.
Modify the parameters according to your requirements.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>CLUSTER_CIDR: 100.96.0.0/11
</span></span><span style=display:flex><span>SERVICE_CIDR: 100.64.0.0/13
</span></span><span style=display:flex><span>CLUSTER_PLAN: &lt;prod/dev&gt;
</span></span><span style=display:flex><span>ENABLE_CEIP_PARTICIPATION: <span style=color:#e6db74>&#39;true&#39;</span>
</span></span><span style=display:flex><span>ENABLE_MHC: <span style=color:#e6db74>&#39;true&#39;</span>
</span></span><span style=display:flex><span>IDENTITY_MANAGEMENT_TYPE: none
</span></span><span style=display:flex><span>INFRASTRUCTURE_PROVIDER: vsphere
</span></span><span style=display:flex><span>TKG_HTTP_PROXY_ENABLED: <span style=color:#e6db74>&#39;false&#39;</span>
</span></span><span style=display:flex><span>AVI_CONTROL_PLANE_HA_PROVIDER: <span style=color:#e6db74>&#34;true&#34;</span>
</span></span><span style=display:flex><span>CLUSTER_NAME: &lt;Provide a Name For the TKG Cluster&gt;
</span></span><span style=display:flex><span>DEPLOY_TKG_ON_VSPHERE7: <span style=color:#e6db74>&#39;true&#39;</span>
</span></span><span style=display:flex><span>VSPHERE_DATACENTER: /&lt;DC-Name&gt;
</span></span><span style=display:flex><span>VSPHERE_DATASTORE: /&lt;DC-Name&gt;/datastore/&lt;Datastore-Name&gt;
</span></span><span style=display:flex><span>VSPHERE_FOLDER: /&lt;DC-Name&gt;/vm/&lt;Folder_Name&gt;
</span></span><span style=display:flex><span>VSPHERE_NETWORK: /&lt;DC-Name&gt;/network/&lt;Network-Name&gt;
</span></span><span style=display:flex><span>VSPHERE_RESOURCE_POOL: /&lt;DC-Name&gt;/host/&lt;Cluster-Name&gt;/Resources/&lt;Resource-Pool-Name&gt;
</span></span><span style=display:flex><span>VSPHERE_SERVER: &lt;vCenter-Address&gt;
</span></span><span style=display:flex><span>VSPHERE_SSH_AUTHORIZED_KEY: <span style=color:#e6db74>&#34;ssh-rsa Nc2EA [...] h2X8uPYqw== email@example.com&#34;</span>
</span></span><span style=display:flex><span>VSPHERE_USERNAME: &lt;vCenter-SSO-Username&gt;
</span></span><span style=display:flex><span>VSPHERE_PASSWORD: &lt;SSO-User-Password&gt;
</span></span><span style=display:flex><span>VSPHERE_TLS_THUMBPRINT: &lt;vCenter Server Thumbprint&gt;
</span></span><span style=display:flex><span>ENABLE_AUDIT_LOGGING: true/false
</span></span><span style=display:flex><span>ENABLE_DEFAULT_STORAGE_CLASS: true/false
</span></span><span style=display:flex><span>ENABLE_AUTOSCALER: true/false
</span></span><span style=display:flex><span>CONTROLPLANE_SIZE: small/medium/large/extra-large
</span></span><span style=display:flex><span>WORKER_SIZE: small/medium/large/extra-large
</span></span><span style=display:flex><span>WORKER_MACHINE_COUNT: &lt;number of worker nodes to be deployed&gt;
</span></span></code></pre></div><p>Key considerations when creating your Shared Service cluster config file:</p><table><thead><tr><th>Variables</th><th>Value</th></tr></thead><tbody><tr><td><code>CLUSTER_PLAN</code></td><td>prod : For all production deploymentsdev: for POC/Dev environments</td></tr><tr><td><code>IDENTITY_MANAGEMENT_TYPE</code></td><td>Match the value set for the management cluster, oidc, ldap, or none.Note: You do not need to configure additional OIDC or LDAP settings in the configuration file for workload clusters</td></tr><tr><td><code>TKG_HTTP_PROXY_ENABLED</code></td><td>true/falseIf true, the following additional variables needs to be provided<code>TKG_HTTP_PROXY</code><code>TKG_HTTPS_PROXY</code><code>TKG_NO_PROXY</code></td></tr><tr><td><code>VSPHERE_NETWORK</code></td><td>As per the architecture, TKG Shared service cluster has dedicated overlay segment (tkg-ss-segment)</td></tr><tr><td><code>CONTROLPLANE_SIZE</code> & <code>WORKER_SIZE</code></td><td>Consider extra-large, as Harbor will be deployed on this cluster and this cluster may be attached to TMC and TO.To define a custom size, remove the <code>CONTROLPLANE_SIZE</code> and <code>WORKER_SIZE</code> variables from the config file and add the following variables with the required resource allocation.For Control Plane Nodes:​​<code>VSPHERE_CONTROL_PLANE_NUM_CPUS</code><code>VSPHERE_CONTROL_PLANE_MEM_MIB</code><code>VSPHERE_CONTROL_PLANE_DISK_GIB</code>For Worker Nodes:<code>VSPHERE_WORKER_NUM_CPUS</code><code>VSPHERE_WORKER_MEM_MIB</code><code>VSPHERE_WORKER_DISK_GIB</code></td></tr><tr><td><code>VSPHERE_CONTROL_PLANE_ENDPOINT</code></td><td>This is optional. If left blank, NSX ALB will assign an IP address from the <code>tkg-cluster-vip-segment</code> pool created earlier.If you need to provide an IP address, pick an unused address from the “TKG Cluster VIP/Data Network” static IP pools configured in NSX ALB.</td></tr></tbody></table><p>The following is the modified Tanzu Kubernetes Grid shared service config file:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>CLUSTER_CIDR: 100.96.0.0/11
</span></span><span style=display:flex><span>SERVICE_CIDR: 100.64.0.0/13
</span></span><span style=display:flex><span>CLUSTER_PLAN: dev
</span></span><span style=display:flex><span>ENABLE_CEIP_PARTICIPATION: <span style=color:#e6db74>&#39;true&#39;</span>
</span></span><span style=display:flex><span>ENABLE_MHC: <span style=color:#e6db74>&#39;true&#39;</span>
</span></span><span style=display:flex><span>IDENTITY_MANAGEMENT_TYPE: none
</span></span><span style=display:flex><span>INFRASTRUCTURE_PROVIDER: vsphere
</span></span><span style=display:flex><span>TKG_HTTP_PROXY_ENABLED: <span style=color:#e6db74>&#39;false&#39;</span>
</span></span><span style=display:flex><span>AVI_CONTROL_PLANE_HA_PROVIDER: <span style=color:#e6db74>&#34;true&#34;</span>
</span></span><span style=display:flex><span>CLUSTER_NAME: tkg-shared-svc
</span></span><span style=display:flex><span>DEPLOY_TKG_ON_VSPHERE7: <span style=color:#e6db74>&#39;true&#39;</span>
</span></span><span style=display:flex><span>VSPHERE_DATACENTER: /arcas-dvs-internet-dc1
</span></span><span style=display:flex><span>VSPHERE_DATASTORE: /arcas-dvs-internet-dc1/datastore/vsanDatastore
</span></span><span style=display:flex><span>VSPHERE_FOLDER: /arcas-dvs-internet-dc1/vm/tkg-sharedsvc-components
</span></span><span style=display:flex><span>VSPHERE_NETWORK: /arcas-dvs-internet-dc1/network/tkg-ss-segment
</span></span><span style=display:flex><span>VSPHERE_RESOURCE_POOL: /arcas-dvs-internet-dc1/host/arcas-dvs-internet-c1/Resources/tkg-sharedsvc-components
</span></span><span style=display:flex><span>VSPHERE_SERVER: vcenter.lab.vmw
</span></span><span style=display:flex><span>VSPHERE_SSH_AUTHORIZED_KEY: ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQC6l1Tnp3EQ24cqskvTi9EXA/1pL/NYSJoT0q+qwTp8jUA1LBo9pV8cu/HmnnA/5gsO/OEefMCfz+CGPOo1mH596EdA/rUQo5K2rqhuNwlA+i+hU87dxQ8KJYhjPOT/lGHQm8VpzNQrF3b0Cq5WEV8b81X/J+H3i57ply2BhC3BE7B0lKbuegnb5aaqvZC+Ig97j1gt5riV/aZg400c3YGJl9pmYpMbyEeJ8xd86wXXyx8X1xp6XIdwLyWGu6zAYYqN4+1pqjV5IBovu6M6rITS0DlgFEFhihZwXxCGyCpshSM2TsIJ1uqdX8zUlhlaQKyAt+2V29nnHDHG1WfMYQG2ypajmE1r4vOkS+C7yUbOTZn9sP7b2m7iDnCG0GvCUT+lNQy8WdFC/Gm0V6+5DeBY790y1NEsl+9RSNNL+MzT/18Yqiq8XIvwT2qs7d5GpSablsITBUNB5YqXNEaf76ro0fZcQNAPfZ67lCTlZFP8v/S5NExqn6P4EHht0m1hZm1FhGdY7pQe8dLz/74MLTEQlP7toOp2ywoArYno8cFVl3PT8YR3TVQARvkS2pfNOquc5iU0r1FXOCrEc3d+LvJYmalmquvghZjblvxQKwguLFIodzdO/3CcpJvwGg0PiANvYZRqVNfTDCjtrN+lFXurlm2pSsA+YI5cbRtZ1ADaPw<span style=color:#f92672>==</span> administrator@lab.vmw
</span></span><span style=display:flex><span>VSPHERE_USERNAME: administrator@lab.vmw
</span></span><span style=display:flex><span>VSPHERE_PASSWORD: VMware@123
</span></span><span style=display:flex><span>VSPHERE_TLS_THUMBPRINT: 40:1E:6D:30:4C:72:A6:8E:9D:AE:A8:67:DE:DA:C9:CA:B3:A6:C6:C2
</span></span><span style=display:flex><span>ENABLE_AUDIT_LOGGING: true
</span></span><span style=display:flex><span>ENABLE_DEFAULT_STORAGE_CLASS: true
</span></span><span style=display:flex><span>ENABLE_AUTOSCALER: false
</span></span><span style=display:flex><span>CONTROLPLANE_SIZE: large
</span></span><span style=display:flex><span>WORKER_SIZE: extra-large
</span></span><span style=display:flex><span>WORKER_MACHINE_COUNT: <span style=color:#ae81ff>1</span>
</span></span></code></pre></div><p>After creating the cluster configuration file, execute the following command to initiate the cluster deployment:</p><p><code>tanzu cluster create -f &lt;path-to-config.yaml> -v 6</code></p><p>When the cluster is successfully deployed, you will see the following results:
<img src=.//img/deployment-guides/tko-on-vsphere-vds/image69.png alt></p><p>Now, connect to the Tanzu management cluster context and apply the following labels:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e>## Add the tanzu-services label to the shared services cluster as its cluster role. In the following command, “tkg-shared-svc” is the name of the shared service cluster.</span>
</span></span><span style=display:flex><span>kubectl label cluster.cluster.x-k8s.io/tkg-shared-svc cluster-role.tkg.tanzu.vmware.com/tanzu-services<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;&#34;</span> --overwrite<span style=color:#f92672>=</span>true
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>## Tag shared service cluster with all “Cluster Labels” defined while deploying the management cluster. Once the “Cluster Labels” are applied, the AKO pod will be deployed on the Shared Service Cluster.</span>
</span></span><span style=display:flex><span>kubectl label cluster tkg-shared-svc type<span style=color:#f92672>=</span>management
</span></span></code></pre></div><p>Get the admin context of the shared service cluster using the following commands and switch the context to the Shared Service cluster:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e>## Use the following command to get the admin context of Shared Service Cluster. In this command, “tkg-shared-svc” is the name of the shared service cluster.</span>
</span></span><span style=display:flex><span>&lt;_Added line break above. Is this correct?_&gt;
</span></span><span style=display:flex><span>tanzu cluster kubeconfig get tkg-shared-svc --admin
</span></span><span style=display:flex><span><span style=color:#75715e>## Use the following to use the context of Shared Service Cluster</span>
</span></span><span style=display:flex><span>kubectl config use-context tkg-shared-svc-admin@tkg-shared-svc
</span></span></code></pre></div><p><img src=.//img/deployment-guides/tko-on-vsphere-vds/image14.png alt></p><p>After the shared service cluster is successfully created, you are ready to deploy the Harbor package. See <a href=#deploy-user-managed-packages-on-tkg-clusters>Deploy User-Managed Packages on Tanzu Kubernetes Grid Clusters</a>.</p><p>You must ensure that the “cert-manager” and “contour” user packages are installed before deploying Harbor. Deploy in the following order:</p><ol><li><p><a href=#install-cert-manager-user-package>Install Cert-Manager User Package</a></p></li><li><p><a href=#install-contour-user-package>Install Contour User Package</a></p></li><li><p><a href=#install-harbor-user-package>Install Harbor User Package</a></p></li></ol><h2 id=a-iddeploy-tanzu-workload-clusters-a-deploy-tanzu-kubernetes-grid-workload-clusters>Deploy Tanzu Kubernetes Grid Workload Clusters</h2><p>In order to deploy a workload cluster, you need to create a cluster config file in which you specify options for connecting to vCenter Server and for identifying the vSphere resources that the cluster will use.</p><p>You can also specify standard sizes for the control plane and worker node VMs, or you can configure the CPU, memory, and disk sizes for control plane and worker nodes explicitly. If you use custom image templates, you can identify which template to use to create node VMs.</p><p>As per the architecture, workload clusters make use of a separate SE group (<strong>Service Engine Group 2)</strong> and VIP Network (<strong>TKG Workload VIP/Data Network</strong>) for application load balancing. You can control the SE group by creating a new AKODeploymentConfig. For more details, see <a href=#configure-nsx-advanced-load-balancer-in-tkg-workload-cluster>Create and deploy AKO Deployment Config for Tanzu Kubernetes Grid Workload Cluster</a></p><p>The following sample cluster config file includes the minimum required configuration parameters to create a Tanzu Kubernetes Grid workload cluster. To learn about all configuration file parameters, see the <a href=https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid/1.5/vmware-tanzu-kubernetes-grid-15/GUID-tanzu-config-reference.html>Tanzu CLI Configuration File Variable Reference</a>.
Modify the config file according to your requirements.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>CLUSTER_CIDR: 100.96.0.0/11
</span></span><span style=display:flex><span>SERVICE_CIDR: 100.64.0.0/13
</span></span><span style=display:flex><span>CLUSTER_PLAN: &lt;prod/dev&gt;
</span></span><span style=display:flex><span>ENABLE_CEIP_PARTICIPATION: <span style=color:#e6db74>&#39;true&#39;</span>
</span></span><span style=display:flex><span>ENABLE_MHC: <span style=color:#e6db74>&#39;true&#39;</span>
</span></span><span style=display:flex><span>IDENTITY_MANAGEMENT_TYPE: none
</span></span><span style=display:flex><span>INFRASTRUCTURE_PROVIDER: vsphere
</span></span><span style=display:flex><span>TKG_HTTP_PROXY_ENABLED: <span style=color:#e6db74>&#39;false&#39;</span>
</span></span><span style=display:flex><span>AVI_CONTROL_PLANE_HA_PROVIDER: <span style=color:#e6db74>&#34;true&#34;</span>
</span></span><span style=display:flex><span>CLUSTER_NAME: &lt;Provide a Name For the TKG Cluster&gt;
</span></span><span style=display:flex><span>DEPLOY_TKG_ON_VSPHERE7: <span style=color:#e6db74>&#39;true&#39;</span>
</span></span><span style=display:flex><span>VSPHERE_DATACENTER: /&lt;DC-Name&gt;
</span></span><span style=display:flex><span>VSPHERE_DATASTORE: /&lt;DC-Name&gt;/datastore/&lt;Datastore-Name&gt;
</span></span><span style=display:flex><span>VSPHERE_FOLDER: /&lt;DC-Name&gt;/vm/&lt;Folder_Name&gt;
</span></span><span style=display:flex><span>VSPHERE_NETWORK: /&lt;DC-Name&gt;/network/&lt;Network-Name&gt;
</span></span><span style=display:flex><span>VSPHERE_RESOURCE_POOL: /&lt;DC-Name&gt;/host/&lt;Cluster-Name&gt;/Resources/&lt;Resource-Pool-Name&gt;
</span></span><span style=display:flex><span>VSPHERE_SERVER: &lt;vCenter-Address&gt;
</span></span><span style=display:flex><span>VSPHERE_SSH_AUTHORIZED_KEY: <span style=color:#e6db74>&#34;ssh-rsa Nc2EA [...] h2X8uPYqw== email@example.com&#34;</span>
</span></span><span style=display:flex><span>VSPHERE_USERNAME: &lt;vCenter-SSO-Username&gt;
</span></span><span style=display:flex><span>VSPHERE_PASSWORD: &lt;SSO-User-Password&gt;
</span></span><span style=display:flex><span>VSPHERE_TLS_THUMBPRINT: &lt;vCenter Server Thumbprint&gt;
</span></span><span style=display:flex><span>ENABLE_AUDIT_LOGGING: true/false
</span></span><span style=display:flex><span>ENABLE_DEFAULT_STORAGE_CLASS: true/false
</span></span><span style=display:flex><span>ENABLE_AUTOSCALER: true/false
</span></span><span style=display:flex><span>CONTROLPLANE_SIZE: small/medium/large/extra-large
</span></span><span style=display:flex><span>WORKER_SIZE: small/medium/large/extra-large
</span></span><span style=display:flex><span>WORKER_MACHINE_COUNT: &lt;<span style=color:#75715e># of worker nodes to be deployed&gt;</span>
</span></span></code></pre></div><p>Key considerations when creating workload cluster config files:</p><table><thead><tr><th>Variables</th><th>Value</th></tr></thead><tbody><tr><td><code>CLUSTER_PLAN</code></td><td>prod : For all production deploymentsdev: for POC/Dev environments</td></tr><tr><td><code>IDENTITY_MANAGEMENT_TYPE</code></td><td>Match the value set for the management cluster, oidc, ldap, or none.Note: You do not need to configure additional OIDC or LDAP settings in the workload cluster configuration file.</td></tr><tr><td><code>TKG_HTTP_PROXY_ENABLED</code></td><td>true/falseIf true, the following additional variables need to be provided/<code>TKG_HTTP_PROXY</code><code>TKG_HTTPS_PROXY</code><code>TKG_NO_PROXY</code></td></tr><tr><td><code>VSPHERE_NETWORK</code></td><td>As per the architecture, the TKG workload cluster will be attached to “TKG Workload Network”.<strong>Note:</strong> The architecture supports multiple TKG workload clusters on the same network and/or separate networks for each workload cluster.</td></tr><tr><td><code>CONTROLPLANE_SIZE</code> & <code>WORKER_SIZE</code></td><td>Consider setting this to extra-large, as Harbor will be deployed on this cluster and this cluster may be attached to TMC and TO.To define custom sizes, remove the <code>CONTROLPLANE_SIZE</code> and <code>WORKER_SIZE</code> variables from the config file and add the following variables with required resource allocationFor Control Plane Nodes:​​<code>VSPHERE_CONTROL_PLANE_NUM_CPUS</code><code>VSPHERE_CONTROL_PLANE_MEM_MIB</code><code>VSPHERE_CONTROL_PLANE_DISK_GIB</code>For Worker Nodes:<code>VSPHERE_WORKER_NUM_CPUS</code><code>VSPHERE_WORKER_MEM_MIB</code><code>VSPHERE_WORKER_DISK_GIB</code></td></tr><tr><td><code>VSPHERE_CONTROL_PLANE_ENDPOINT</code></td><td>This is optional; if left blank, NSX ALB will assign an IP address from the <code>tkg-cluster-vip-segment</code> pool created earlier.If you need to provide an IP address, pick an unused address from the “TKG Cluster VIP/Data Network” static IP pools configured in NSX ALB.</td></tr><tr><td><code>ENABLE_AUTOSCALER</code></td><td>This is an optional parameter. Set if you want to override the default value. The default value is false; if set to true, you must include the following additional variables:<code>AUTOSCALER_MAX_NODES_TOTAL</code><code>AUTOSCALER_SCALE_DOWN_DELAY_AFTER_ADD</code><code>AUTOSCALER_SCALE_DOWN_DELAY_AFTER_DELETE</code><code>AUTOSCALER_SCALE_DOWN_DELAY_AFTER_FAILURE</code><code>AUTOSCALER_SCALE_DOWN_UNNEEDED_TIME</code><code>AUTOSCALER_MAX_NODE_PROVISION_TIME</code><code>AUTOSCALER_MIN_SIZE_0</code><code>AUTOSCALER_MAX_SIZE_0</code>For more details see Cluster Autoscaler</td></tr><tr><td>WORKER_MACHINE_COUNT</td><td>Consider setting the value to 3 or above if the cluster needs to be part of Tanzu Service Mesh (TSM)</td></tr></tbody></table><p>The modified Tanzu Kubernetes Grid shared service config file is as follows:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>CLUSTER_CIDR: 100.96.0.0/11
</span></span><span style=display:flex><span>SERVICE_CIDR: 100.64.0.0/13
</span></span><span style=display:flex><span>CLUSTER_PLAN: dev
</span></span><span style=display:flex><span>ENABLE_CEIP_PARTICIPATION: <span style=color:#e6db74>&#39;true&#39;</span>
</span></span><span style=display:flex><span>ENABLE_MHC: <span style=color:#e6db74>&#39;true&#39;</span>
</span></span><span style=display:flex><span>IDENTITY_MANAGEMENT_TYPE: none
</span></span><span style=display:flex><span>INFRASTRUCTURE_PROVIDER: vsphere
</span></span><span style=display:flex><span>TKG_HTTP_PROXY_ENABLED: <span style=color:#e6db74>&#39;false&#39;</span>
</span></span><span style=display:flex><span>AVI_CONTROL_PLANE_HA_PROVIDER: <span style=color:#e6db74>&#34;true&#34;</span>
</span></span><span style=display:flex><span>CLUSTER_NAME: tkg-workload-dev
</span></span><span style=display:flex><span>DEPLOY_TKG_ON_VSPHERE7: <span style=color:#e6db74>&#39;true&#39;</span>
</span></span><span style=display:flex><span>VSPHERE_DATACENTER: /arcas-dvs-internet-dc1
</span></span><span style=display:flex><span>VSPHERE_DATASTORE: /arcas-dvs-internet-dc1/datastore/vsanDatastore
</span></span><span style=display:flex><span>VSPHERE_FOLDER: /arcas-dvs-internet-dc1/vm/tkg-workload01-components
</span></span><span style=display:flex><span>VSPHERE_NETWORK: /arcas-dvs-internet-dc1/network/tkg-workload-segment
</span></span><span style=display:flex><span>VSPHERE_RESOURCE_POOL: /arcas-dvs-internet-dc1/host/arcas-dvs-internet-c1/Resources/tkg-workload01-components
</span></span><span style=display:flex><span>VSPHERE_SERVER: vcenter.lab.vmw
</span></span><span style=display:flex><span>VSPHERE_SSH_AUTHORIZED_KEY: ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQC6l1Tnp3EQ24cqskvTi9EXA/1pL/NYSJoT0q+qwTp8jUA1LBo9pV8cu/HmnnA/5gsO/OEefMCfz+CGPOo1mH596EdA/rUQo5K2rqhuNwlA+i+hU87dxQ8KJYhjPOT/lGHQm8VpzNQrF3b0Cq5WEV8b81X/J+H3i57ply2BhC3BE7B0lKbuegnb5aaqvZC+Ig97j1gt5riV/aZg400c3YGJl9pmYpMbyEeJ8xd86wXXyx8X1xp6XIdwLyWGu6zAYYqN4+1pqjV5IBovu6M6rITS0DlgFEFhihZwXxCGyCpshSM2TsIJ1uqdX8zUlhlaQKyAt+2V29nnHDHG1WfMYQG2ypajmE1r4vOkS+C7yUbOTZn9sP7b2m7iDnCG0GvCUT+lNQy8WdFC/Gm0V6+5DeBY790y1NEsl+9RSNNL+MzT/18Yqiq8XIvwT2qs7d5GpSablsITBUNB5YqXNEaf76ro0fZcQNAPfZ67lCTlZFP8v/S5NExqn6P4EHht0m1hZm1FhGdY7pQe8dLz/74MLTEQlP7toOp2ywoArYno8cFVl3PT8YR3TVQARvkS2pfNOquc5iU0r1FXOCrEc3d+LvJYmalmquvghZjblvxQKwguLFIodzdO/3CcpJvwGg0PiANvYZRqVNfTDCjtrN+lFXurlm2pSsA+YI5cbRtZ1ADaPw<span style=color:#f92672>==</span> administrator@lab.vmw
</span></span><span style=display:flex><span>VSPHERE_USERNAME: administrator@lab.vmw
</span></span><span style=display:flex><span>VSPHERE_PASSWORD: VMware@123
</span></span><span style=display:flex><span>VSPHERE_TLS_THUMBPRINT: 40:1E:6D:30:4C:72:A6:8E:9D:AE:A8:67:DE:DA:C9:CA:B3:A6:C6:C2
</span></span><span style=display:flex><span>ENABLE_AUDIT_LOGGING: true
</span></span><span style=display:flex><span>ENABLE_DEFAULT_STORAGE_CLASS: true
</span></span><span style=display:flex><span>ENABLE_AUTOSCALER: false
</span></span><span style=display:flex><span>CONTROLPLANE_SIZE: large
</span></span><span style=display:flex><span>WORKER_SIZE: extra-large
</span></span><span style=display:flex><span>WORKER_MACHINE_COUNT: <span style=color:#ae81ff>3</span>
</span></span></code></pre></div><p>After preparing the cluster configuration file, execute the following command to initiate the cluster deployment:</p><p><code>tanzu cluster create -f &lt;path-to-config.yaml> -v 6</code></p><p>Once the cluster is successfully deployed, you will see the following results:
<img src=.//img/deployment-guides/tko-on-vsphere-vds/image16.png alt></p><h3 id=a-idconfigure-nsx-advanced-load-balancer-in-tkg-workload-cluster-a-configure-nsx-advanced-load-balancer-in-tanzu-kubernetes-grid-workload-cluster>Configure NSX Advanced Load Balancer in Tanzu Kubernetes Grid Workload Cluster</h3><p>Tanzu Kubernetes Grid management clusters with NSX Advanced Load Balancer have a default AKODeploymentConfig that is deployed during installation. It is called install-ako-for-all.</p><p>By default, any clusters that match the cluster labels defined in install-ako-for-all will reference this file for their virtual IP networks, service engine (SE) groups, and L7 ingress. As part of our architecture, only shared service cluster makes use of the configuration defined in the default AKODeploymentConfig install-ako-for-all.</p><p>As per the defined <strong>architecture</strong>, workload clusters must <strong>not</strong> make <strong>use</strong> of <strong>Service Engine Group 1</strong> and VIP Network <strong>TKG Cluster VIP/Data Network</strong> for application load balancer services**.
**A separate SE group (<strong>Service Engine Group 2)</strong> and VIP Network (<strong>TKG Workload VIP/Data Network</strong>) will be used by the workload clusters, These configurations can be enforced on workload clusters by:</p><ul><li><p>Creating a new AKODeploymentConfig in the Tanzu Kubernetes Grid management cluster. This AKODeploymentConfig file dictates which specific SE group and VIP network that the workload clusters can use for load balancing.</p></li><li><p>Apply the new AKODeploymentConfig: Label the workload cluster to match the AKODeploymentConfig.spec.clusterSelector.matchLabels element in the AKODeploymentConfig file.
Once the labels are applied on the workload cluster, Tanzu Kubernetes Grid management cluster will deploy the AKO pod on the target workload cluster that has the configuration defined in the new AKODeploymentConfig.</p></li></ul><p>The following is the format of the <code>AKODeploymentConfig yaml</code> file.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>networking.tkg.tanzu.vmware.com/v1alpha1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>AKODeploymentConfig</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>finalizers</span>:
</span></span><span style=display:flex><span>    - <span style=color:#ae81ff>ako-operator.networking.tkg.tanzu.vmware.com</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>generation</span>: <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>&lt;Unique name of AKODeploymentConfig&gt;</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>adminCredentialRef</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>name</span>: <span style=color:#ae81ff>avi-controller-credentials</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>namespace</span>: <span style=color:#ae81ff>tkg-system-networking</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>certificateAuthorityRef</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>name</span>: <span style=color:#ae81ff>avi-controller-ca</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>namespace</span>: <span style=color:#ae81ff>tkg-system-networking</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>cloudName</span>: <span style=color:#ae81ff>&lt;NAME OF THE CLOUD&gt;</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>clusterSelector</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>matchLabels</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>&lt;KEY&gt;</span>: <span style=color:#ae81ff>&lt;VALUE&gt;</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>controller</span>: <span style=color:#ae81ff>&lt;NSX ALB CONTROLLER IP/FQDN&gt;</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>dataNetwork</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>cidr</span>: <span style=color:#ae81ff>&lt;VIP NETWORK CIDR&gt;</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>name</span>: <span style=color:#ae81ff>&lt;VIP NETWORK NAME&gt;</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>extraConfigs</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>image</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>pullPolicy</span>: <span style=color:#ae81ff>IfNotPresent</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>repository</span>: <span style=color:#ae81ff>projects.registry.vmware.com/tkg/ako</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>version</span>: <span style=color:#ae81ff>v1.3.2_vmware.1</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>ingress</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>defaultIngressController</span>: <span style=color:#66d9ef>false</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>disableIngressClass</span>: <span style=color:#66d9ef>true</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>serviceEngineGroup</span>: <span style=color:#ae81ff>&lt;SERVICE ENGINE NAME&gt;</span>
</span></span></code></pre></div><p>The following is a sample AKODeploymentConfig file with sample values in place. In this example, the Tanzu Kubernetes Grid management cluster will deploy AKO pod on any workload cluster that matches the <strong>label</strong> <code>type=workloadset01</code>. The AKO configuration will be as follows:</p><ul><li><p>cloud: ​<code>tanzu-vcenter-01​</code></p></li><li><p>service engine Group: <code>tanzu-wkld-segroup-01</code></p></li><li><p>VIP/data network: <code>tkg-cluster-vip-segment</code></p></li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>networking.tkg.tanzu.vmware.com/v1alpha1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>AKODeploymentConfig</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>finalizers</span>:
</span></span><span style=display:flex><span>    - <span style=color:#ae81ff>ako-operator.networking.tkg.tanzu.vmware.com</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>generation</span>: <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>tanzu-ako-workload-set01</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>adminCredentialRef</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>name</span>: <span style=color:#ae81ff>avi-controller-credentials</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>namespace</span>: <span style=color:#ae81ff>tkg-system-networking</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>certificateAuthorityRef</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>name</span>: <span style=color:#ae81ff>avi-controller-ca</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>namespace</span>: <span style=color:#ae81ff>tkg-system-networking</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>cloudName</span>: <span style=color:#ae81ff>tanzu-vcenter-01</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>clusterSelector</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>matchLabels</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>type</span>: <span style=color:#ae81ff>workloadset01</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>controller</span>: <span style=color:#ae81ff>avi-ha.lab.vmw</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>dataNetwork</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>cidr</span>: <span style=color:#ae81ff>tkg-workload-vip-segment</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>name</span>: <span style=color:#ae81ff>172.16.70.0</span><span style=color:#ae81ff>/24</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>extraConfigs</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>image</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>pullPolicy</span>: <span style=color:#ae81ff>IfNotPresent</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>repository</span>: <span style=color:#ae81ff>projects.registry.vmware.com/tkg/ako</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>version</span>: <span style=color:#ae81ff>v1.3.2_vmware.1</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>ingress</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>defaultIngressController</span>: <span style=color:#66d9ef>false</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>disableIngressClass</span>: <span style=color:#66d9ef>true</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>serviceEngineGroup</span>: <span style=color:#ae81ff>tanzu-wkld-segroup-01</span>
</span></span></code></pre></div><p>Once you have the AKO configuration file ready, use the <code>kubectl</code> command to set the context to Tanzu Kubernetes Grid management cluster. Use the following command to list the available <code>AKODeploymentConfig</code>:</p><p><code>kubectl apply -f &lt;path_to_akodeploymentconfig.yaml></code></p><p><img src=.//img/deployment-guides/tko-on-vsphere-vds/image19.png alt></p><p>Use the following command to list all AKODeploymentConfig created under the management cluster:</p><p><code>kubectl get adc</code>
or
<code>kubectl get akodeploymentconfig</code></p><p><img src=.//img/deployment-guides/tko-on-vsphere-vds/image75.png alt></p><p>Now that you have successfully created the AKO deployment config, you need to apply the cluster labels defined in the AKODeploymentConfig to any of the Tanzu Kubernetes Grid workload clusters. Once the labels are applied, the Tanzu Kubernetes Grid management cluster will deploy the AKO pod on the target workload cluster.</p><p><code>kubectl label cluster &lt;cluster name>\&lt;label></code></p><p><img src=.//img/deployment-guides/tko-on-vsphere-vds/image11.png alt></p><hr><h3 id=connect-to-tanzu-kubernetes-grid-workload-cluster-and-validate-the-deployment>Connect to Tanzu Kubernetes Grid Workload Cluster and Validate the Deployment</h3><p>Now that the Tanzu Kubernetes Grid workload cluster is created and the required AKO configurations are applied, use the following command to get the admin context of the Tanzu Kubernetes Grid workload cluster.</p><p><code>tanzu cluster kubeconfig get &lt;cluster-name> --admin</code></p><p><img src=.//img/deployment-guides/tko-on-vsphere-vds/image51.png alt></p><p>Now connect to the Tanzu Kubernetes Grid workload cluster using the <code>kubectl</code> command and run the following commands to check the status of AKO and other components:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get nodes <span style=color:#75715e># List all nodes with status</span>
</span></span><span style=display:flex><span>kubectl get pods -n avi-system <span style=color:#75715e># To check the status of AKO pod</span>
</span></span><span style=display:flex><span>kubectl get pods -A <span style=color:#75715e># Lists all pods and its status</span>
</span></span></code></pre></div><p><img src=.//img/deployment-guides/tko-on-vsphere-vds/image32.png alt></p><p>You can see that the workload cluster is successfully deployed and AKO pod is deployed on the cluster. You can now <a href=#config-saas-services>configure SaaS services</a> for the cluster and/or <a href=#deploy-user-managed-packages-on-tkg-clusters>deploy user managed packages</a> on this cluster.</p><h2 id=a-iddeploy-user-managed-packages-on-tkg-clustersa-deploy-user-managed-packages-on-tanzu-kubernetes-grid-clusters>Deploy User-Managed Packages on Tanzu Kubernetes Grid Clusters</h2><p>Tanzu Kubernetes Grid includes the following user-managed packages. These packages provide in-cluster and shared services to the Kubernetes clusters that are running in your Tanzu Kubernetes Grid environment.</p><table><thead><tr><th>Function</th><th>Package</th><th>Location</th></tr></thead><tbody><tr><td>Certificate Management</td><td>cert-manager</td><td>Workload or shared services cluster</td></tr><tr><td>Container networking</td><td>multus-cni</td><td>Workload cluster</td></tr><tr><td>Container registry</td><td>harbor</td><td>Shared services cluster</td></tr><tr><td>Ingress control</td><td>contour</td><td>Workload or shared services cluster</td></tr><tr><td>Log forwarding</td><td>fluent-bit</td><td>Workload cluster</td></tr><tr><td>Monitoring</td><td>grafanaprometheus</td><td>Workload cluster</td></tr><tr><td>Service discovery</td><td>external-dns</td><td>Workload or shared services cluster</td></tr></tbody></table><h3 id=a-idinstall-cert-manager-user-package-a-install-cert-manager-user-package>Install Cert-Manager User Package</h3><p>Cert-Manager is required for the Contour, Harbor, Prometheus, and Grafana packages.</p><ol><li><p>Switch the context to the respective cluster and capture the available Cert-Manager version</p><p><code>tanzu package available list cert-manager.tanzu.vmware.com -A</code></p></li><li><p>Install the Cert-Manager package.</p><p><code>tanzu package install cert-manager --package-name cert-manager.tanzu.vmware.com --namespace cert-manager --version &lt;AVAILABLE-PACKAGE-VERSION> --create-namespace</code></p></li><li><p>Validate the Cert-Manager package installation. The status must change to “<strong>Reconcile succeeded</strong>”
<code>tanzu package installed list -A | grep cert-manager</code></p></li></ol><h3 id=a-idinstall-contour-user-package-a-install-the-contour-user-package>Install the Contour User Package</h3><p>Contour is required for the Harbor, Prometheus, and Grafana packages.</p><ol><li><p>Switch context to the respective cluster, and ensure that the AKO pod is in a running state.
<code>kubectl get pods -A | grep ako</code></p></li><li><p>Create the following configuration file named contour-data-values.yaml.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span> ---
</span></span><span style=display:flex><span>  <span style=color:#f92672>infrastructure_provider</span>: <span style=color:#ae81ff>vsphere</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>namespace</span>: <span style=color:#ae81ff>tanzu-system-ingress</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>contour</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>configFileContents</span>: {}
</span></span><span style=display:flex><span>    <span style=color:#f92672>useProxyProtocol</span>: <span style=color:#66d9ef>false</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>replicas</span>: <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>pspNames</span>: <span style=color:#e6db74>&#34;vmware-system-restricted&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>logLevel</span>: <span style=color:#ae81ff>info</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>envoy</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>service</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>type</span>: <span style=color:#ae81ff>LoadBalancer</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>annotations</span>: {}
</span></span><span style=display:flex><span>      <span style=color:#f92672>nodePorts</span>:
</span></span><span style=display:flex><span>        <span style=color:#f92672>http</span>: <span style=color:#66d9ef>null</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>https</span>: <span style=color:#66d9ef>null</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>externalTrafficPolicy</span>: <span style=color:#ae81ff>Cluster</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>disableWait</span>: <span style=color:#66d9ef>false</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>hostPorts</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>enable</span>: <span style=color:#66d9ef>true</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>http</span>: <span style=color:#ae81ff>80</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>https</span>: <span style=color:#ae81ff>443</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>hostNetwork</span>: <span style=color:#66d9ef>false</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>terminationGracePeriodSeconds</span>: <span style=color:#ae81ff>300</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>logLevel</span>: <span style=color:#ae81ff>info</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>pspNames</span>: <span style=color:#66d9ef>null</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>certificates</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>duration</span>: <span style=color:#ae81ff>8760h</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>renewBefore</span>: <span style=color:#ae81ff>360h</span>
</span></span></code></pre></div></li><li><p>Use the following command to capture the available Contour version:</p><p><code>tanzu package available list contour.tanzu.vmware.com -A</code></p></li><li><p>Install the Contour Package</p><p><code>tanzu package install contour --package-name contour.tanzu.vmware.com --version &lt;AVAILABLE-PACKAGE-VERSION> --values-file &lt;path_to_contour-data-values.yaml_file> --namespace tanzu-system-contour --create-namespace</code></p></li><li><p>Validate the Contour package installation. The status must change to “Reconcile succeeded”.
<code>tanzu package installed list -A | grep contour</code></p></li></ol><h3 id=a-idinstall-harbor-user-package-a-install-harbor-user-package>Install Harbor User Package</h3><p>In order to install Harbor, ensure that Cert-Manager and Contour user packages are installed on the cluster.</p><ol><li><p>Check if the prerequisite packages are installed on the cluster</p><p><code>tanzu package installed list -A</code></p><p>Ensure that the status of <code>cert-manager</code> and <code>contour</code> is &ldquo;Reconcile succeeded&rdquo;.</p></li><li><p>Capture the available Harbor version</p><p><code>tanzu package available list harbor.tanzu.vmware.com -A</code></p></li><li><p>Obtain the &ldquo;harbor-data-values.yaml&rdquo; file</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>image_url<span style=color:#f92672>=</span><span style=color:#66d9ef>$(</span>kubectl -n tanzu-package-repo-global get packages harbor.tanzu.vmware.com.&lt;package version&gt; -o jsonpath<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;{.spec.template.spec.fetch[0].imgpkgBundle.image}&#39;</span><span style=color:#66d9ef>)</span>
</span></span><span style=display:flex><span>imgpkg pull -b $image_url -o /tmp/harbor-package
</span></span><span style=display:flex><span>cp /tmp/harbor-package/config/values.yaml &lt;path to save harbor-data-values.yaml&gt;
</span></span></code></pre></div></li><li><p>Set the mandatory passwords and secrets in the <code>harbor-data-values.yaml</code> file.</p><p><code>bash /tmp/harbor-package/config/scripts/generate-passwords.sh ./harbor-data-values.yaml</code></p></li><li><p><strong>Update</strong> the following sections and remove comments in the <strong>harbor-data-values.yaml</strong> file.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e>##Update required fields</span>
</span></span><span style=display:flex><span>hostname: &lt;Harbor Registry FQDN&gt;
</span></span><span style=display:flex><span>tls.crt: &lt;Full Chain cert&gt; <span style=color:#f92672>(</span>Optional, only <span style=color:#66d9ef>if</span> provided<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>tls.key: &lt;Cert Key&gt; <span style=color:#f92672>(</span>Optional, only <span style=color:#66d9ef>if</span> provided<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span><span style=color:#75715e>##Delete the auto generated password and replace it with the user provided value</span>
</span></span><span style=display:flex><span>harborAdminPassword: &lt;Set admin password&gt;
</span></span><span style=display:flex><span><span style=color:#75715e>## Remove all comments in the harbor-data-values.yaml file:</span>
</span></span><span style=display:flex><span>yq -i eval <span style=color:#e6db74>&#39;... comments=&#34;&#34;&#39;</span> ./harbor-data-values.yaml
</span></span></code></pre></div></li><li><p>Install the Harbor Package using the following command:</p><p><code>tanzu package install harbor --package-name harbor.tanzu.vmware.com --version &lt;available package version> --values-file &lt;path to harbor-data-values.yaml> --namespace tanzu-system-registry --create-namespace</code></p></li><li><p>To address a known issue, patch the Harbor package by following the steps in the Knowledge Base article: The <a href=https://kb.vmware.com/s/article/85725>harbor-notary-signer pod fails to start</a></p></li><li><p>Confirm that the harbor package has been installed. The status must change to “Reconcile succeeded”.
<code>tanzu package installed list -A | grep harbor</code></p></li></ol><h2 id=a-idconfig-saas-servicesa-configure-saas-services>Configure SaaS Services</h2><p>The following VMware SaaS services provide additional Kubernetes lifecycle management, observability, and service mesh features.</p><ul><li>Tanzu Mission Control (TMC)</li><li>Tanzu Observability (TO)</li><li>Tanzu Service Mesh (TSM)</li></ul><p>For configuration information, see <a href=tko-saas-services.md>Configure SaaS Services</a>.</p></div></div><div class="loader-container-toc col-lg-2 w-100" style=display:none><span class=spinner></span></div><div class="rhs-right-container d-none d-lg-block col-lg-2"><div class=rhs-right-panel><div class="rhs-right-panel-header d-flex justify-content-between"><div class=rhs-right-panel-title>In this article</div></div><div id=right-toc-div-id class="onpage-toc-container d-flex flex-column"><nav id=TableOfContents><ul><li><a href=#deploying-with-vmware-service-installer-for-tanzu>Deploying with VMware Service Installer for Tanzu</a></li><li><a href=#prepare-your-environment-for-deploying-tanzu-for-kubernetes-operations>Prepare Your Environment for Deploying Tanzu for Kubernetes Operations</a></li><li><a href=#overview--of-the-deployment-steps>Overview of the Deployment Steps</a></li><li><a href=#a-iddeploy-and-configure-nsx-advanced-load-balancer-a-deploy-and-configure-nsx-advanced-load-balancer>Deploy and Configure NSX Advanced Load Balancer</a><ul><li><a href=#a-iddeploy-nsx-advanced-load-balancer-a-deploy-nsx-advanced-load-balancer>Deploy NSX Advanced Load Balancer</a></li><li><a href=#nsx-advanced-load-balancer-initial-setup>NSX Advanced Load Balancer: Initial setup</a></li><li><a href=#nsx-advanced-load-balancer-licensing>NSX Advanced Load Balancer: Licensing</a></li><li><a href=#nsx-advanced-load-balancer-controller-high-availability>NSX Advanced Load Balancer: Controller High Availability</a></li><li><a href=#nsx-advanced-load-balancer-certificate-management>NSX Advanced Load Balancer: Certificate Management</a></li><li><a href=#nsx-advanced-load-balancer-create-vcenter-cloud-and-se-groups>NSX Advanced Load Balancer: Create vCenter Cloud and SE Groups</a></li><li><a href=#nsx-advanced-load-balancer-configure-network-and-ipam-profile>NSX Advanced Load Balancer: Configure Network and IPAM Profile</a></li></ul></li><li><a href=#a-iddeploy-and-configure-bootstrap-machineadeploy-and-configure-bootstrap-machine>Deploy and Configure Bootstrap Machine</a></li><li><a href=#a-iddeploy-tanzu-kubernetes-grid-tkg-management-cluster-a-deploy-tanzu-kubernetes-grid-management-cluster>Deploy Tanzu Kubernetes Grid Management Cluster</a><ul><li><a href=#a-idimport-base-image-template-for-tkg-cluster-deployment-a-import-base-image-template-for-tanzu-kubernetes-grid-cluster-deployment>Import Base Image template for Tanzu Kubernetes Grid Cluster Deployment</a></li><li><a href=#deploy-tanzu-kubernetes-grid-management-cluster-from-the-ui>Deploy Tanzu Kubernetes Grid Management Cluster from the UI</a></li></ul></li><li><a href=#a-iddeploy-tanzu-shared-service-cluster-a-deploy-tanzu-kubernetes-grid-shared-service-cluster>Deploy Tanzu Kubernetes Grid Shared Service Cluster</a></li><li><a href=#a-iddeploy-tanzu-workload-clusters-a-deploy-tanzu-kubernetes-grid-workload-clusters>Deploy Tanzu Kubernetes Grid Workload Clusters</a><ul><li><a href=#a-idconfigure-nsx-advanced-load-balancer-in-tkg-workload-cluster-a-configure-nsx-advanced-load-balancer-in-tanzu-kubernetes-grid-workload-cluster>Configure NSX Advanced Load Balancer in Tanzu Kubernetes Grid Workload Cluster</a></li><li><a href=#connect-to-tanzu-kubernetes-grid-workload-cluster-and-validate-the-deployment>Connect to Tanzu Kubernetes Grid Workload Cluster and Validate the Deployment</a></li></ul></li><li><a href=#a-iddeploy-user-managed-packages-on-tkg-clustersa-deploy-user-managed-packages-on-tanzu-kubernetes-grid-clusters>Deploy User-Managed Packages on Tanzu Kubernetes Grid Clusters</a><ul><li><a href=#a-idinstall-cert-manager-user-package-a-install-cert-manager-user-package>Install Cert-Manager User Package</a></li><li><a href=#a-idinstall-contour-user-package-a-install-the-contour-user-package>Install the Contour User Package</a></li><li><a href=#a-idinstall-harbor-user-package-a-install-harbor-user-package>Install Harbor User Package</a></li></ul></li><li><a href=#a-idconfig-saas-servicesa-configure-saas-services>Configure SaaS Services</a></li></ul></nav></div></div></div></section></div><footer class=tech-pub-footer><div id=page-footer><section class="footer-component footer-container"><div class=personalization_div_1 style=min-height:1px></div><div class=personalization_div_2 style=min-height:1px></div><div class=container><div class=content><div class=row><div class="col-lg-12 col-md-12"><footer class=footer><div class=row><div class="col-lg-2 col-md-12 mb-40 mt-3"><a class=footer-vmware-logo href=https://www-lt.vmware.com/ name="nav_footer : VMware Logo"><picture class=float-lg-left><source media=(max-width:800px) srcset=https://www-lt.vmware.com/content/dam/digitalmarketing/vmware/vm-logo-big.png.imgo.jpeg><img loading=lazy class=vmware-logo src=/img/vm-logo-big.png alt=VMware title=VMware></picture></a></div></div></footer></div></div></div></div></section></div></footer><script src=/js/pageStore.js></script>
<script src=/js/main.js></script></body></html>