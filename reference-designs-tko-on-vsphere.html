<!doctype html><html><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=0"><meta name="last modified" content="27/09/2021 12:47:24"><meta name=abstract content><meta name=author content="dpavel@vmware.com"><meta name=primary-product-name content="MD2Docs-TestBed"><meta name=primary-product-version content="1"><meta name=description content><meta name=guid content="GUID-1Intro"><meta name=language content="en"><meta name=title content="Basic Markdown"><meta name=publication-author content="dpavel@vmware.com"><meta property="og:title" content="Basic Markdown"><meta property="og:image" content="https://docs-uat.vmware.com/uicontent/images/vmware-docs-default.png"><meta property="og:description" content><meta property="og:type" content="article"><meta property="og:locale" content="en"><meta property="og:url" content="https://docs-uat-staging.vmware.com/en/MD2Docs-TestBed/1/md-2-docs-test-bed/GUID-1Intro.html"><meta name=cdf-utag content="https://tags.tiqcdn.com/utag/vmware/cdf-privacy/qa/utag.js"><link rel=stylesheet type=text/css href=/validated-solutions/css/commonltr.css><link rel=stylesheet type=text/css href=/validated-solutions/css/non-draft.vmware.productdocs.css><link rel=canonical href=https://docs-uat-staging.vmware.com/en/MD2Docs-TestBed/1/md-2-docs-test-bed/GUID-1Intro.html><link class=user href=/validated-solutions/css/responsive.css rel=stylesheet type=text/css><link rel=icon href=https://www.vmware.com/favicon.ico type=image/x-icon><link rel=stylesheet href=/validated-solutions/css/v2-global.20200911172508.css><title>Docs Preview</title></head><body><header class=tech-pub-header><div id=header class="global-header col-12"><div class="row desktop-header h-100"><div class="col col-md-3 align-self-center header-logo-wrapper"><div class="d-inline-flex align-items-center justify-content-start w-100"><div class="d-inline-flex align-items-center w-100"><span class="my-auto d-md-none header-menu-icon"><i class="fa fa-bars"></i></span><h1><a href=https://docs-uat-staging.vmware.com/ class="d-inline-flex align-items-center my-auto nav-link header-logo-url pl-md-1 pl-xl-3"><span class=mr-2><img src=/validated-solutions/img/vm-logo.png alt="VMware Logo"></span>
<span class=vm-logo-title>Docs Preview</span></a></h1></div><div class=align-items-center><span id=toggleTOC class="d-md-none header-toc-icon"><i class="fa fa-ellipsis-v px-2"></i></span></div></div></div></div></div><div class="col-12 vmware-gradient w-100 px-0 mx-0"></div></header><div class="tech-pub-container main-container d-flex flex-column pubView"><section class="tech-pub-section d-flex flex-md-row"><div class="lhs lhs-container col-md-4 col-lg-3" style=display:block><div class=backdrop></div><div class=left-panel><div class="panel-header position-relative hidden-xs"><div class="panel-header-left d-flex justify-content-between align-items-center"><span class=container-collapse-expand><span id=expand-all-id class=expand-tree onclick=expandAll()><span class="lhs-expand-shape align-middle"><i class="fa fa-chevron-down"></i></span>
<span class="expand-text align-middle" data-i18n data-i18n-expand-all>Expand All</span></span>
<span id=collapse-all-id class="collapse-tree hide" onclick=collapseAll()><span class="lhs-collapse-shape align-middle"><i class="fa fa-chevron-up"></i></span>
<span class="collapse-text align-middle" data-i18n data-i18n-collapse-all>Collapse All</span></span></span></div></div><div class="panel-content p-2" id=left_toc><div class="dropdown collection-dropdown-container w-100"><button class="btn w-100 text-left dropdown-toggle collection-name" type=button id=collectionDropdwnBtn data-toggle=dropdown aria-haspopup=true aria-label="Collection Dropdown" aria-expanded=false>
<span class=label></span>
<span class="float-right icon-down pl-2 fa fa-angle-down"></span></button><div class="dropdown-menu w-100" id=collectionMenu aria-labelledby=collectionDropdwnBtn></div></div><div id=tree class=mt-2></div><div class="w-100 px-2 toc-product-container"><a class="mr-3 my-2 position-relative toc-product-link"><span class=toc-product-name></span>
<span class=localized-page-name>Product Documentation</span></a></div><ul class=rm-default-ul-styles></ul></div></div></div><div class="rhs rhs-container col-md-8 col-lg-7" style=display:block><div class=rhs-top><div class="rhs-top-container-top d-flex flex-row justify-content-between"><h1 class=primary-header id=vmware-tanzu-for-kubernetes-operations-on-vsphere-reference-design>VMware Tanzu for Kubernetes Operations on vSphere Reference Design</h1></div><div class="rhs-top-container-middle d-flex flex-row justify-content-between"></div><div class=rhs-top-container-bottom><div class=last-updated-container><div class=calendar-icon><svg width="36" height="36" viewBox="0 0 36 36" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path class="clr-i-outline clr-i-outline-path-1" d="M32.25 6H29V8h3V30H4V8H7V6H3.75A1.78 1.78.0 002 7.81V30.19A1.78 1.78.0 003.75 32h28.5A1.78 1.78.0 0034 30.19V7.81A1.78 1.78.0 0032.25 6z"/><rect class="clr-i-outline clr-i-outline-path-2" x="8" y="14" width="2" height="2"/><rect class="clr-i-outline clr-i-outline-path-3" x="14" y="14" width="2" height="2"/><rect class="clr-i-outline clr-i-outline-path-4" x="20" y="14" width="2" height="2"/><rect class="clr-i-outline clr-i-outline-path-5" x="26" y="14" width="2" height="2"/><rect class="clr-i-outline clr-i-outline-path-6" x="8" y="19" width="2" height="2"/><rect class="clr-i-outline clr-i-outline-path-7" x="14" y="19" width="2" height="2"/><rect class="clr-i-outline clr-i-outline-path-8" x="20" y="19" width="2" height="2"/><rect class="clr-i-outline clr-i-outline-path-9" x="26" y="19" width="2" height="2"/><rect class="clr-i-outline clr-i-outline-path-10" x="8" y="24" width="2" height="2"/><rect class="clr-i-outline clr-i-outline-path-11" x="14" y="24" width="2" height="2"/><rect class="clr-i-outline clr-i-outline-path-12" x="20" y="24" width="2" height="2"/><rect class="clr-i-outline clr-i-outline-path-13" x="26" y="24" width="2" height="2"/><path class="clr-i-outline clr-i-outline-path-14" d="M10 10a1 1 0 001-1V3A1 1 0 009 3V9a1 1 0 001 1z"/><path class="clr-i-outline clr-i-outline-path-15" d="M26 10a1 1 0 001-1V3a1 1 0 00-2 0V9a1 1 0 001 1z"/><rect class="clr-i-outline clr-i-outline-path-16" x="13" y="6" width="10" height="2"/><rect x="0" y="0" width="36" height="36" fill-opacity="0"/></svg></div><span class=last-updated-label data-i18n-updated-on>Updated on</span>
&nbsp;
<span class=last-updated-date-ph>9/22/22</span></div></div></div><div class="rhs-center article-wrapper" id=content-div-id><p>Tanzu for Kubernetes Operations simplifies operating Kubernetes for multi-cloud deployment by centralizing management and governance for clusters and teams across on-premises, public clouds, and edge. It delivers an open source aligned Kubernetes distribution with consistent operations and management to support infrastructure and app modernization.</p><p>This document describes a reference design for deploying VMware Tanzu for Kubernetes Operations on vSphere backed by vSphere Networking (VDS).</p><p>The following reference design is based on the architecture and components described in <a href=index.md>VMware Tanzu for Kubernetes Operations Reference Architecture</a>.</p><p><img src=img/reference-designs/tko-on-vsphere/tkgm-diagram.png alt="Tanzu for Kubernetes Operations components"></p><h2 id=supported-component-matrix>Supported Component Matrix</h2><p>The validated Bill of Materials that can be used to install Tanzu Kubernetes Grid on your vSphere with NSX-T environment is as follows:</p><table><thead><tr><th><strong>Software Components</strong></th><th><strong>Version</strong></th></tr></thead><tbody><tr><td>Tanzu Kubernetes Grid</td><td>1.5.1</td></tr><tr><td>VMware vSphere ESXi</td><td>7.0 U2 and later</td></tr><tr><td>VMware vCenter (VCSA)</td><td>7.0 U2 and later</td></tr><tr><td>VMware vSAN</td><td>7.0 U2 and later</td></tr><tr><td>NSX Advanced Load Balancer</td><td>20.1.7</td></tr></tbody></table><p>For up-to-date information about which software versions can be used together, check the Interoperability Matrix <a href="https://interopmatrix.vmware.com/Interoperability?col=551,&row=2,%26789,">here</a>.</p><h2 id=tanzu-kubernetes-grid-components>Tanzu Kubernetes Grid Components</h2><p>VMware Tanzu Kubernetes Grid (TKG) provides organizations with a consistent, upstream-compatible, regional Kubernetes substrate that is ready for end-user workloads and ecosystem integrations. You can deploy Tanzu Kubernetes Grid across software-defined datacenters (SDDC) and public cloud environments, including vSphere, Microsoft Azure, and Amazon EC2.</p><p>Tanzu Kubernetes Grid comprises the following components:</p><p><strong>Management Cluster -</strong> A management cluster is the first element that you deploy when you create a Tanzu Kubernetes Grid instance. The management cluster is a Kubernetes cluster that performs the role of the primary management and operational center for the Tanzu Kubernetes Grid instance. The management cluster is purpose-built for operating the platform and managing the lifecycle of Tanzu Kubernetes clusters.</p><p><strong>Cluster API -</strong> Tanzu Kubernetes Grid functions through the creation of a management Kubernetes cluster which houses <a href=https://cluster-api.sigs.k8s.io/>Cluster API</a>. The Cluster API then interacts with the infrastructure provider to service workload Kubernetes cluster lifecycle requests.</p><p><strong>Tanzu Kubernetes Cluster -</strong> Tanzu Kubernetes clusters are the Kubernetes clusters in which your application workloads run. These clusters are also referred to as workload clusters. Tanzu Kubernetes clusters can run different versions of Kubernetes, depending on the needs of the applications they run.</p><p><strong>Shared Service Cluster -</strong> Each Tanzu Kubernetes Grid instance can only have one shared services cluster. You will deploy this cluster only if you intend to deploy shared services such as Contour and Harbor.</p><p><strong>Tanzu Kubernetes Cluster Plans -</strong> A cluster plan is a blueprint that describes the configuration with which to deploy a Tanzu Kubernetes cluster. It provides a set of configurable values that describe settings like the number of control plane machines, worker machines, VM types, and so on. This release of Tanzu Kubernetes Grid provides two default templates, dev and prod.</p><p><strong>Tanzu Kubernetes Grid Instance -</strong> A Tanzu Kubernetes Grid instance is the full deployment of Tanzu Kubernetes Grid, including the management cluster, the workload clusters, and the shared services cluster that you configure.</p><p><strong>Tanzu CLI -</strong> A command-line utility that provides the necessary commands to build and operate Tanzu management and Tanzu Kubernetes clusters.</p><p><strong>Bootstrap Machine -</strong> The bootstrap machine is the laptop, host, or server on which you download and run the Tanzu CLI. This is where the initial bootstrapping of a management cluster occurs before it is pushed to the platform where it will run.</p><p><strong>Tanzu Kubernetes Grid Installer -</strong> The Tanzu Kubernetes Grid installer is a graphical wizard that you launch by running the <code>tanzu management-cluster create --ui</code> command. The installer wizard runs locally on the bootstrap machine and provides a user interface to guide you through the process of deploying a management cluster.</p><h2 id=tanzu-kubernetes-grid-storage>Tanzu Kubernetes Grid Storage</h2><p>Tanzu Kubernetes Grid integrates with shared datastores available in the vSphere infrastructure. The following types of shared datastores are supported:</p><ul><li>vSAN</li><li>VMFS</li><li>NFS</li><li>vVols</li></ul><p>Tanzu Kubernetes Grid Cluster Plans can be defined by operators to use a certain vSphere datastore when creating new workload clusters. All developers then have the ability to provision container-backed persistent volumes from that underlying datastore.</p><p>Tanzu Kubernetes Grid is agnostic to which option you choose. For Kubernetes stateful workloads, Tanzu Kubernetes Grid installs the <a href=https://github.com/container-storage-interface/spec/blob/master/spec.md>vSphere Container Storage interface (vSphere CSI)</a> to automatically provision Kubernetes persistent volumes for pods.</p><p><a href=https://docs.vmware.com/en/VMware-vSAN/index.html>VMware vSAN</a> is a recommended storage solution for deploying Tanzu Kubernetes Grid clusters on vSphere.</p><table><thead><tr><th><strong>Decision ID</strong></th><th><strong>Design Decision</strong></th><th><strong>Design Justification</strong></th><th><strong>Design Implications</strong></th></tr></thead><tbody><tr><td>TKO-STG-001</td><td>Use vSAN storage for TKO</td><td>vSAN supports NFS volumes in ReadWriteMany access modes.</td><td>vSAN File Services need to be configured to leverage this. vSAN File Service is available only in vSAN Enterprise and Enterprise Plus editions</td></tr></tbody></table><p>While the default vSAN storage policy can be used, administrators should evaluate the needs of their applications and craft a specific <a href=https://docs.vmware.com/en/VMware-vSphere/6.7/com.vmware.vsphere.storage.doc/GUID-89091D59-D844-46B2-94C2-35A3961D23E7.html>vSphere Storage Policy</a>. vSAN storage policies describe classes of storage (For example, SSD or NVME) along with quotas for your clusters.</p><p><img src=img/reference-designs/tko-on-vsphere/tko-on-vsphere-vds-2.png alt="Tanzu for Kubernetes Grid storage integration with vSAN"></p><h2 id=tanzu-kubernetes-clusters-networking>Tanzu Kubernetes Clusters Networking</h2><p>A Tanzu Kubernetes cluster provisioned by Tanzu Kubernetes Grid supports two Container Network Interface (CNI) options:</p><ul><li><a href=https://antrea.io/>Antrea</a></li><li><a href=https://www.tigera.io/project-calico/>Calico</a></li></ul><p>Both are open-source software that provides networking for cluster pods, services, and ingress.</p><p>When you deploy a Tanzu Kubernetes cluster using Tanzu Mission Control or Tanzu CLI, Antrea CNI is automatically enabled in the cluster.</p><p>Tanzu Kubernetes Grid also supports <a href=https://github.com/k8snetworkplumbingwg/multus-cni>Multus</a> CNI which can be installed through Tanzu user-managed packages. Multus CNI lets you attach multiple network interfaces to a single pod and associate each with a different address range.</p><p>To provision a Tanzu Kubernetes cluster using a non-default CNI, see the following instructions:</p><ul><li><a href=https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid/1.5/vmware-tanzu-kubernetes-grid-15/GUID-tanzu-k8s-clusters-networking.html#calico>Deploy Tanzu Kubernetes clusters with Calico</a></li><li><a href=https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid/1.5/vmware-tanzu-kubernetes-grid-15/GUID-packages-cni-multus.html>Implement Multiple Pod Network Interfaces with Multus</a></li></ul><p>Each CNI is suitable for a different use case. The following table lists some common use cases for the three CNIs that Tanzu Kubernetes Grid supports. This table will help you with information on selecting the right CNI in your Tanzu Kubernetes Grid implementation.</p><table><thead><tr><th><strong>CNI</strong></th><th><strong>Use Case</strong></th><th><strong>Pros and Cons</strong></th></tr></thead><tbody><tr><td>Antrea</td><td>Enable Kubernetes pod networking with IP overlay networks using VXLAN or Geneve for encapsulation. Optionally encrypt node-to-node communication using IPSec packet encryption.Antrea supports advanced network use cases like kernel bypass and network service mesh.</td><td>Pros- Provides an option to configure egress IP pool or static egress IP for Kubernetes workloads.</td></tr><tr><td>Calico</td><td>Calico is used in environments where factors like network performance, flexibility, and power are essential.For routing packets between nodes, Calico leverages the BGP routing protocol instead of an overlay network. This eliminates the need to wrap packets with an encapsulation layer resulting in increased network performance for Kubernetes workloads.</td><td>Pros- Support for network policies- High network performance- SCTP supportCons- No multicast support</td></tr><tr><td>Multus</td><td>Multus CNI provides multiple interfaces per each Kubernetes pod. Using Multus CRDs, you can specify which pods get which interfaces and allow different interfaces depending on the use case.</td><td>Pros- Separation of data/control planes.- Separate security policies can be used for separate interfaces. - Supports SR-IOV, DPDK, OVS-DPDK, and VPP workloads in Kubernetes with both cloud native and NFV based applications in Kubernetes.</td></tr></tbody></table><h2 id=tanzu-kubernetes-grid-infrastructure-networking>Tanzu Kubernetes Grid Infrastructure Networking</h2><p>Tanzu Kubernetes Grid on vSphere can be deployed on various networking stacks including</p><ul><li>VMware NSX-T Data Center Networking.</li><li>vSphere Networking (VDS).</li></ul><p><strong>Note:</strong> The scope of this document is limited to vSphere Networking.</p><h2 id=tanzu-kubernetes-grid-on-vsphere-networking-with-nsx-advanced-load-balancer>Tanzu Kubernetes Grid on vSphere Networking with NSX Advanced Load Balancer</h2><p>Tanzu Kubernetes Grid when deployed on the vSphere networking uses the distributed port groups to provide connectivity to Kubernetes control plane VMs, worker nodes, services, and applications. All hosts from the cluster where Tanzu Kubernetes clusters are deployed are connected to the distributed switch that provides connectivity to the Kubernetes environment.</p><p>Tanzu Kubernetes Grid leverages NSX Advanced Load Balancer to provide L4 load balancing for the Tanzu Kubernetes Clusters Control-Plane HA and L7 ingress to the applications deployed in the Tanzu Kubernetes clusters. Users access the applications by connecting to the virtual IP address (VIP) of the applications provisioned by NSX Advanced Load Balancer.</p><h2 id=nsx-advanced-load-balancer-components>NSX Advanced Load Balancer Components</h2><p>NSX Advanced Load Balancer is deployed in Write Access Mode in the vSphere environment. This mode grants NSX Advanced Load Balancer Controller full write access to the vCenter which helps in automatically creating, modifying, and removing service engines (SEs) and other resources as needed to adapt to changing traffic needs. The core components of NSX Advanced Load Balancer are as follows:</p><ul><li><strong>NSX Advanced Load Balancer Controller</strong> - NSX Advanced Load Balancer Controller manages Virtual Service objects and interacts with the vCenter Server infrastructure to manage the lifecycle of the service engines (SEs). It is the central repository for the configurations and policies related to services and management, and it provides the portal for viewing the health of VirtualServices and SEs and the associated analytics that NSX Advanced Load Balancer provides.</li><li><strong>NSX Advanced Load Balancer Service Engine</strong> - The service engines (SEs) are lightweight VMs that handle all data plane operations by receiving and executing instructions from the controller. The SEs perform load balancing and all client- and server-facing network interactions.</li><li><strong>Avi Kubernetes Operator (AKO)</strong> - It is a Kubernetes operator that runs as a pod in the Supervisor Cluster and Tanzu Kubernetes clusters, and it provides ingress and load balancing functionality. AKO translates the required Kubernetes objects to NSX Advanced Load Balancer objects and automates the implementation of ingresses, routes, and services on the service engines (SE) through the NSX Advanced Load Balancer Controller.</li><li><strong>AKO Operator (AKOO)</strong> - This is an operator which is used to deploy, manage, and remove the AKO pod in Kubernetes clusters. This operator when deployed creates an instance of the AKO controller and installs all the relevant objects like:<ul><li>AKO <code>StatefulSet</code></li><li><code>ClusterRole</code> and <code>ClusterRoleBinding</code></li><li><code>ConfigMap</code> required for the AKO controller and other artifacts.</li></ul></li></ul><p>Tanzu Kubernetes Grid management clusters have an AKO operator installed out of the box during cluster deployment. By default, a Tanzu Kubernetes Grid management cluster has a couple of AkoDeploymentConfig created which dictates when and how AKO pods are created in the workload clusters. For more information, see <a href=https://github.com/vmware/load-balancer-and-ingress-services-for-kubernetes/tree/master/ako-operator>AKO Operator documentation</a>.</p><p>Each environment configured in NSX Advanced Load Balancer is referred to as a cloud. Each cloud in NSX Advanced Load Balancer maintains networking and NSX Advanced Load Balancer Service Engine settings. The cloud is configured with one or more VIP networks to provide IP addresses to load balancing (L4 or L7) virtual services created under that cloud.</p><p>The virtual services can span across multiple service engines if the associated Service Engine Group is configured in the Active/Active HA mode. A service engine can belong to only one Service Engine group at a time.</p><p>IP address allocation for virtual services can be over DHCP or using NSX Advanced Load Balancer in-built IPAM functionality. The VIP networks created or configured in NSX Advanced Load Balancer are associated with the IPAM profile.</p><h2 id=network-architecture>Network Architecture</h2><p>For the deployment of Tanzu Kubernetes Grid in the vSphere environment, it is required to build separate networks for the Tanzu Kubernetes Grid management cluster and workload clusters, NSX Advanced Load Balancer management, cluster-VIP network for control plane HA, Tanzu Kubernetes Grid management VIP or data network, and Tanzu Kubernetes Grid workload data or VIP network.</p><p>The network reference design can be mapped into this general framework.</p><p><img src=img/reference-designs/tko-on-vsphere/tko-on-vsphere-vds-3.png alt="Tanzu for Kubernetes Grid network layout"></p><p>This topology enables the following benefits:</p><ul><li>Isolate and separate SDDC management components (vCenter, ESX) from the Tanzu Kubernetes Grid components. This reference design allows only the minimum connectivity between the Tanzu Kubernetes Grid clusters and NSX Advanced Load Balancer to the vCenter Server.</li><li>Isolate and separate NSX Advanced Load Balancer management network from the Tanzu Kubernetes Grid management segment and the Tanzu Kubernetes Grid workload segments.</li><li>Depending on the workload cluster type and use case, multiple workload clusters may leverage the same workload network or new networks can be used for each workload cluster. To isolate and separate Tanzu Kubernetes Grid workload cluster networking from each other it’s recommended to make use of separate networks for each workload cluster and configure the required firewall between these networks. For more information, see <a href=#firewall-requirements>Firewall Requirements</a>.</li><li>Separate provider and tenant access to the Tanzu Kubernetes Grid environment.<ul><li>Only provider administrators need access to the Tanzu Kubernetes Grid management cluster. This prevents tenants from attempting to connect to the TKG management cluster.</li></ul></li><li>Only allow tenants to access their Tanzu Kubernetes Grid workload clusters and restrict access to this cluster from other tenants.</li></ul><h3 id=network-requirements>Network Requirements</h3><p>As per the defined architecture, the list of required networks follows:</p><table><thead><tr><th><strong>Network Type</strong></th><th><strong>DHCP Service</strong></th><th><strong>Description & Recommendations</strong></th></tr></thead><tbody><tr><td>NSX ALB Management Network</td><td>Optional</td><td>NSX ALB controllers and SEs will be attached to this network. DHCP is not a mandatory requirement on this network as NSX ALB can take care of IPAM.</td></tr><tr><td>TKG Management Network</td><td>Yes</td><td>Control plane and worker nodes of TKG management cluster and shared service clusters will be attached to this network.Creating shared service cluster on a separate network is also supported.</td></tr><tr><td>TKG Workload Network</td><td>Yes</td><td>Control plane and worker nodes of TKG workload clusters will be attached to this network.</td></tr><tr><td>TKG Cluster VIP/Data Network</td><td>No</td><td>Virtual services for control plane HA of all TKG clusters (management, shared service, and workload).Reserve sufficient IP addresses depending on the number of TKG clusters planned to be deployed in the environment. NSX Advanced Load Balancer takes care of IPAM on this network.</td></tr><tr><td>TKG Management VIP/Data Network</td><td>No</td><td>Virtual services for all user-managed packages (such as Contour, Harbor, Contour, Prometheus, Grafana) hosted on the Shared service cluster. For more information, see <a href=https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid/1.4/vmware-tanzu-kubernetes-grid-14/GUID-packages-user-managed-index.html>User-Managed Packages</a>.</td></tr><tr><td>TKG Workload VIP/Data Network</td><td>No</td><td>Virtual services for all applications are hosted on the workload clusters. Reserve sufficient IP addresses depending on the number of applications that are planned to be hosted on the workload clusters and scalability considerations.</td></tr></tbody></table><h3 id=subnet-and-cidr-examples>Subnet and CIDR Examples</h3><p>The deployment described in this document makes use of the following CIDR.</p><table><thead><tr><th><strong>Network Type</strong></th><th><strong>Port Group Name</strong></th><th><strong>Gateway CIDR</strong></th><th><strong>DHCP Pool</strong></th><th><strong>NSX ALB IP Pool</strong></th></tr></thead><tbody><tr><td>NSX ALB Mgmt Network</td><td><code>nsx_alb_management_pg</code></td><td>172.16.10.1/24</td><td>N/A</td><td>172.16.10.100- 172.16.10.200</td></tr><tr><td>TKG Management Network</td><td><code>tkg_mgmt_pg</code></td><td>172.16.40.1/24</td><td>172.16.40.100- 172.16.40.200</td><td>N/A</td></tr><tr><td>TKG Mgmt VIP Network</td><td><code>tkg_mgmt_vip_pg</code></td><td>172.16.50.1/24</td><td>N/A</td><td>172.16.50.100- 172.16.50.200</td></tr><tr><td>TKG Cluster VIP Network</td><td><code>tkg_cluster_vip_pg</code></td><td>172.16.80.1/24</td><td>N/A</td><td>172.16.80.100- 172.16.80.200</td></tr><tr><td>TKG Workload VIP Network</td><td><code>tkg_workload_vip_pg</code></td><td>172.16.70.1/24</td><td>N/A</td><td>172.16.70.100 - 172.16.70.200</td></tr><tr><td>TKG Workload Segment</td><td><code>tkg_workload_pg</code></td><td>172.16.60.1/24</td><td>172.16.60.100- 172.16.60.200</td><td>N/A</td></tr></tbody></table><h2 id=firewall-requirements>Firewall Requirements</h2><p>To prepare the firewall, you need to gather the following information:</p><ul><li>NSX Advanced Load Balancer management network CIDR</li><li>TKG Management cluster network CIDR</li><li>TKG Cluster VIP network CIDR</li><li>TKG Management VIP network CIDR</li><li>TKG Workload cluster CIDR</li><li>VMware Harbor registry IP address</li><li>vCenter server IP address</li><li>DNS server IP addresses</li><li>NTP servers</li></ul><table><thead><tr><th><strong>Source</strong></th><th><strong>Destination</strong></th><th><strong>Protocol:Port</strong></th><th><strong>Description</strong></th></tr></thead><tbody><tr><td>TKG management and workload networks</td><td>DNS ServerNTP Server</td><td>UDP:53UDP:123</td><td>DNS Service Time Synchronization</td></tr><tr><td>TKG management and workload Networks</td><td>DHCP Server</td><td>UDP: 67, 68</td><td>Allows hosts to get DHCP addresses</td></tr><tr><td>TKG management and workload Networks</td><td>vCenter IP</td><td>TCP:443</td><td>Allows components to access vCenter to create VMs and storage volumes</td></tr><tr><td>TKG management, shared service, and workload cluster CIDR</td><td>Harbor Registry</td><td>TCP:443</td><td>Allows components to retrieve container images. This registry can be a local or a public image registry (projects.registry.vmware.com)</td></tr><tr><td>TKG management cluster network</td><td>TKG cluster VIP network</td><td>TCP:6443</td><td>For management cluster to configure shared service and workload cluster.</td></tr><tr><td>TKG shared service cluster network(Required only if using a separate network for shared service cluster)</td><td>TKG cluster VIP network</td><td>TCP:6443</td><td>Allow shared cluster to register with management cluster</td></tr><tr><td>TKG workload cluster network</td><td>TKG cluster VIP network</td><td>TCP:6443</td><td>Allow workload cluster to register with management cluster</td></tr><tr><td>TKG management, shared service, and workload Networks</td><td>Avi Controllers (NSX ALB Management Network)</td><td>TCP:443</td><td>Allow Avi Kubernetes Operator (AKO) and AKO Operator (AKOO) access to Avi Controller</td></tr><tr><td>Avi Controllers (NSX ALB Management Network)</td><td>vCenter and ESXi Hosts</td><td>TCP:443</td><td>Allow Avi to discover vCenter objects and deploy SEs as required</td></tr><tr><td>Admin network</td><td>Bootstrap VM</td><td>SSH:22</td><td>To deploy, manage and configure TKG clusters</td></tr><tr><td>deny-all</td><td>any</td><td>any</td><td>deny</td></tr></tbody></table><h2 id=installation-experience>Installation Experience</h2><p>Tanzu Kubernetes Grid management cluster is the first component that you deploy to get started with Tanzu Kubernetes Grid.</p><p>You can deploy the management cluster in two ways:</p><ul><li>Run the Tanzu Kubernetes Grid installer, a wizard interface that guides you through the process of deploying a management cluster. This is the recommended method if you are installing a Tanzu Kubernetes Grid management cluster for the first time.</li><li>Create and edit YAML configuration files, and use them to deploy a management cluster with the CLI commands.</li></ul><p>The Tanzu Kubernetes Grid Installation user interface shows that, in the current version, it is possible to install Tanzu Kubernetes Grid on vSphere (including VMware Cloud on AWS), AWS EC2, and Microsoft Azure. The UI provides a guided experience tailored to the IaaS, in this case, VMware vSphere.</p><p><img src=img/reference-designs/tko-on-vsphere/tko-on-vsphere-vds-4.png alt="Tanzu for Kubernetes Grid installer welcome screen"></p><p>The installation of Tanzu Kubernetes Grid on vSphere is done through the same installer UI but tailored to a vSphere environment.</p><p><img src=img/reference-designs/tko-on-vsphere/tko-on-vsphere-vds-5.png alt="Tanzu for Kubernetes Grid installer UI for vSphere"></p><p>This installation process will take you through the setup of a management cluster on your vSphere environment. Once the management cluster is deployed, you can make use of <a href=https://tanzu.vmware.com/mission-control>Tanzu Mission Control</a> or Tanzu CLI to deploy Tanzu Kubernetes shared service and workload clusters.</p><h2 id=design-recommendations>Design Recommendations</h2><h3 id=nsx-advanced-load-balancer-recommendations>NSX Advanced Load Balancer Recommendations</h3><p>The following table provides the recommendations for configuring NSX Advanced Load Balancer in a vSphere with Tanzu environment.</p><table><thead><tr><th><strong>Decision ID</strong></th><th><strong>Design Decision</strong></th><th><strong>Design Justification</strong></th><th><strong>Design Implications</strong></th></tr></thead><tbody><tr><td>TKO-ALB-001</td><td>Deploy NSX ALB controller cluster nodes on a network dedicated to NSX-ALB.</td><td>Isolate NSX ALB traffic from infrastructure management traffic and Kubernetes workloads.</td><td>Using the same network for NSX ALB Controller Cluster nodes allows for configuring a floating cluster IP address that will be assigned to the cluster leader.</td></tr><tr><td>TKO-ALB-002</td><td>Deploy 3 NSX ALB controllers nodes.</td><td>To achieve high availability for the NSX ALB platform.</td><td>In clustered mode, NSX ALB availability is not impacted by an individual controller node failure. The failed node can be removed from the cluster and redeployed if recovery is not possible.</td></tr><tr><td>TKO-ALB-003</td><td>Use static IP addresses for the NSX ALB controllers if DHCP cannot guarantee a permanent lease.</td><td>NSX ALB Controller cluster uses management IP addresses to form and maintain quorum for the control plane cluster. Any changes to management IP addresses will be disruptive.</td><td>NSX ALB Controller control plane might go down if the management IP addresses of the controller node change.</td></tr><tr><td>TKO-ALB-004</td><td>Use NSX ALB IPAM for service engine data network and virtual services.</td><td>Guarantees IP address assignment for service engine data NICs and virtual services.</td><td>Removes the corner case scenario when the DHCP server runs out of the lease or is down.</td></tr><tr><td>TKO-ALB-005</td><td>Reserve an IP address in the NSX ALB management subnet to be used as the cluster IP address for the controller cluster.</td><td>NSX ALB portal is always accessible over cluster IP address regardless of a specific individual controller node failure.</td><td>NSX ALB administration is not affected by an individual controller node failure.</td></tr><tr><td>TKO-ALB-006</td><td>Use separate VIP networks for application load balancing and L7 services in TKG clusters</td><td>Separate dev/test and prod workloads L7 load balancer traffic from each other.</td><td>Install AKO in TKG clusters manually using helm charts. Reference the VIP network to use in the AKO configuration.</td></tr><tr><td>TKO-ALB-007</td><td>Create separate service engine groups for TKG management and workload clusters.</td><td>This allows isolating load balancing traffic of the management and shared services cluster from workload clusters.</td><td>Create dedicated service engine groups under the vCenter cloud configured manually.</td></tr><tr><td>TKO-ALB-008</td><td>Shared service engines for the same type of workload (dev/test/prod) clusters.</td><td>Minimize the licensing cost.</td><td>Each service engine contributes to the CPU core capacity associated with a license.Sharing service engines can help reduce the licensing cost.</td></tr></tbody></table><h3 id=network-recommendations>Network Recommendations</h3><p>The key network recommendations for a production-grade Tanzu Kubernetes Grid deployment with NSX-T Data Center Networking are as follows:</p><table><thead><tr><th><strong>Decision ID</strong></th><th><strong>Design Decision</strong></th><th><strong>Design Justification</strong></th><th><strong>Design Implications</strong></th></tr></thead><tbody><tr><td>TKO-NET-001</td><td>Use separate networks for management cluster and workload clusters.</td><td>To have a flexible firewall and security policies.</td><td>Sharing the same network for multiple clusters can complicate firewall rules creation.</td></tr><tr><td>TKO-NET-002</td><td>Use separate networks for workload clusters based on their usage.</td><td>Isolate production Kubernetes clusters from dev/test clusters.</td><td>A separate set of service engines can be used for separating dev/test workload clusters from prod clusters.</td></tr><tr><td>TKO-NET-003</td><td>Configure DHCP for each TKG Cluster Network.</td><td>Tanzu Kubernetes Grid does not support static IP address assignments for Kubernetes VM components.</td><td>IP address pool can be used for the TKG clusters in absence of DHCP.</td></tr></tbody></table><h3 id=tanzu-kubernetes-grid-clusters-recommendations>Tanzu Kubernetes Grid Clusters Recommendations</h3><table><thead><tr><th><strong>Decision ID</strong></th><th><strong>Design Decision</strong></th><th><strong>Design Justification</strong></th><th><strong>Design Implications</strong></th></tr></thead><tbody><tr><td>TKO-TKG-001</td><td>Deploy TKG management cluster from TKG Installer UI.</td><td>Simplified method of installation.</td><td>When you deploy a management cluster by using the installer interface, it populates a cluster configuration file for the management cluster with the required parameters. You can use the configuration file as a model for future deployments from the CLI.</td></tr><tr><td>TKO-TKG-002</td><td>Register the management cluster with Tanzu Mission Control.</td><td>Tanzu Mission Control automates the creation of the Tanzu Kubernetes clusters and manages the life cycle of all clusters centrally.</td><td>Tanzu Mission Control also automates the deployment of Tanzu Packages in all Tanzu Kubernetes clusters associated with TMC.</td></tr><tr><td>TKO-TKG-003</td><td>Use NSX Advanced Load Balancer as your control plane endpoint provider and for application load balancing.</td><td>Eliminates the requirement for an external load balancer and additional configuration changes on your Tanzu Kubernetes Grid clusters.</td><td>NSX ALB is a true SDN solution and it offers a flexible deployment model and an automated way of scaling load balancer objects when needed.</td></tr><tr><td>TKO-TKG-004</td><td>Deploy Tanzu Kubernetes clusters in large form factor.</td><td>Allow TKG clusters integration with Tanzu SaaS components (Tanzu Mission Control, Tanzu Observability, and Tanzu Service Mesh).</td><td>When TKG is integrated with SaaS endpoints, new pods or services are created in the target cluster and the pods have specific CPU requirements which can’t be fulfilled with medium and small-sized control plane or worker nodes.</td></tr><tr><td>TKO-TKG-005</td><td>Deploy Tanzu Kubernetes clusters with prod plan.</td><td>This deploys multiple control plane nodes and provides high availability for the control plane.</td><td>TKG infrastructure is not impacted by single node failure.</td></tr><tr><td>TKO-TKG-006</td><td>Enable identity management for Tanzu Kubernetes Grid clusters.</td><td>To avoid usage of administrator credentials and ensure that required users with right roles have access to Tanzu Kubernetes Grid clusters.</td><td>Pinniped package helps with integrating TKG Management cluster with LDAPS/OIDC authentication.Workload cluster inherits the authentication configuration from the management cluster.</td></tr><tr><td>TKO-TKG-007</td><td>Enable Machine Health Checks for TKG clusters.</td><td>vSphere HA and Machine Health Checks interoperably work together to enhance workload resiliency.</td><td>A MachineHealthCheck is a resource within the Cluster API which allows users to define conditions under which machines within a cluster are considered unhealthy. Remediation actions can be taken when MachineHealthCheck has identified a node as unhealthy.</td></tr><tr><td>TKO-TKG-008</td><td>Use Photon based image for TKG clusters.</td><td>TMC supports only Photon based images for deploying TKG clusters.</td><td>Provisioning clusters from TMC with Ubuntu or any custom images is still in development.</td></tr></tbody></table><h3 id=kubernetes-ingress-routing>Kubernetes Ingress Routing</h3><p>The default installation of Tanzu Kubernetes Grid does not have any ingress controller installed. Users can use Contour (available for installation through Tanzu Packages) or any third-party ingress controller of their choice.</p><p>Contour is an open-source controller for Kubernetes ingress routing. Contour can be installed in the shared services cluster on any Tanzu Kubernetes Cluster. Deploying Contour is a prerequisite if you want to deploy the Prometheus, Grafana, and Harbor Packages on a workload cluster.</p><p>For more information about Contour, see the <a href=https://projectcontour.io/>Contour</a> site and <a href=https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid/1.5/vmware-tanzu-kubernetes-grid-15/GUID-packages-ingress-contour.html>Implementing Ingress Control with Contour</a>.</p><p>Another option is to use the NSX Advanced Load Balancer Kubernetes ingress controller which offers an advanced L7 ingress for containerized applications that are deployed in the Tanzu Kubernetes workload cluster.</p><p><img src=img/reference-designs/tko-on-vsphere/tko-on-vsphere-vds-6.png alt="NSX Advanced Load Balancing capabilities for VMware Tanzu"></p><p>For more information about the NSX Advanced Load Balancer ingress controller, see <a href=https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid/1.5/vmware-tanzu-kubernetes-grid-15/GUID-networking-configure-l7.html>Configuring L7 Ingress with NSX Advanced Load Balancer</a>.</p><p><a href=https://tanzu.vmware.com/service-mesh>Tanzu Service Mesh</a>, which is a SaaS offering for modern applications running across multi-cluster, multi-clouds, also offers an ingress controller based on <a href=https://istio.io/>Istio</a>.</p><p>The following table provides general recommendations on when you should use a specific ingress controller for your Kubernetes environment.</p><table><thead><tr><th><strong>Ingress Controller</strong></th><th><strong>Use Cases</strong></th></tr></thead><tbody><tr><td>Contour</td><td>Use Contour when only north-south traffic is needed in a Kubernetes cluster. You can apply security policies for the north-south traffic by defining the policies in the application&rsquo;s manifest file.It&rsquo;s a reliable solution for simple Kubernetes workloads.</td></tr><tr><td>Istio</td><td>Use Istio ingress controller when you intend to provide security, traffic direction, and insights within the cluster (east-west traffic) and between the cluster and the outside world (north-south traffic).</td></tr><tr><td>NSX ALB ingress controller</td><td>Use NSX ALB ingress controller when a containerized application requires features like local and global server load balancing (GSLB), web application firewall (WAF), performance monitoring, direct routing from LB to pod, etc.</td></tr></tbody></table><h2 id=nsx-advanced-load-balancer-sizing-guidelines>NSX Advanced Load Balancer Sizing Guidelines</h2><h3 id=nsx-advanced-load-balancer-controller-sizing-guidelines>NSX Advanced Load Balancer Controller Sizing Guidelines</h3><p>Regardless of NSX Advanced Load Balancer Controller configuration, each controller cluster can achieve up to 5000 virtual services, which is a hard limit. For further details, refer to <a href=https://docs.vmware.com/en/VMware-Cloud-Foundation/services/vcf-nsx-advanced-load-balancer-v1/GUID-0B159D7A-E9ED-4C3C-B959-AC09877D26CE.html>Sizing Compute and Storage Resources for NSX Advanced Load Balancer Controller(s)</a>.</p><table><thead><tr><th><strong>Controller Size</strong></th><th><strong>VM Configuration</strong></th><th><strong>Virtual Services</strong></th><th><strong>Avi SE Scale</strong></th></tr></thead><tbody><tr><td>Small</td><td>4 vCPUS, 12 GB RAM</td><td>0-50</td><td>0-10</td></tr><tr><td>Medium</td><td>8 vCPUS, 24 GB RAM</td><td>0-200</td><td>0-100</td></tr><tr><td>Large</td><td>16 vCPUS, 32 GB RAM</td><td>200-1000</td><td>100-200</td></tr><tr><td>Extra Large</td><td>24 vCPUS, 48 GB RAM</td><td>1000-5000</td><td>200-400</td></tr></tbody></table><h3 id=service-engine-sizing-guidelines>Service Engine Sizing Guidelines</h3><p>For guidance on sizing your service engines (SEs), see <a href=https://docs.vmware.com/en/VMware-Cloud-Foundation/services/vcf-nsx-advanced-load-balancer-v1/GUID-149D3FFA-BF77-4B6F-B73D-A42D5375E9CF.html>Sizing Compute and Storage Resources for NSX Advanced Load Balancer Service Engine(s)</a>.</p><table><thead><tr><th><strong>Performance metric</strong></th><th><strong>1 vCPU core</strong></th></tr></thead><tbody><tr><td>Throughput</td><td>4 Gb/s</td></tr><tr><td>Connections/s</td><td>40k</td></tr><tr><td>SSL Throughput</td><td>1 Gb/s</td></tr><tr><td>SSL TPS (RSA2K)</td><td>~600</td></tr><tr><td>SSL TPS (ECC)</td><td>2500</td></tr></tbody></table><p>Multiple performance vectors or features may have an impact on performance. For instance, to achieve 1 Gb/s of SSL throughput and 2000 TPS of SSL with EC certificates, NSX Advanced Load Balancer recommends two cores.</p><p>NSX Advanced Load Balancer SEs may be configured with as little as 1 vCPU core and 1 GB RAM, or up to 36 vCPU cores and 128 GB RAM. SEs can be deployed in Active/Active or Active/Standby mode depending on the license tier used. NSX Advanced Load Balancer Essentials license doesn’t support Active/Active HA mode for SE.</p><table><thead><tr><th><strong>Decision ID</strong></th><th><strong>Design Decision</strong></th><th><strong>Design Justification</strong></th><th><strong>Design Implications</strong></th></tr></thead><tbody><tr><td>TKO-ALB-SE-001</td><td>Configure the high availability mode for SEs.</td><td>To mitigate a single point of failure for the NSX ALB data plane.</td><td>High availability for SEs is configured by setting the Elastic HA mode to Active/Active or N+M in the Service Engine Group.</td></tr></tbody></table><h2 id=container-registry>Container Registry</h2><p>VMware Tanzu for Kubernetes Operations using Tanzu Kubernetes Grid includes Harbor as a container registry. Harbor provides a location for pushing, pulling, storing, and scanning container images used in your Kubernetes clusters.</p><p>Harbor registry is used for day-2 operations of the Tanzu Kubernetes workload clusters. Typical day-2 operations include tasks such as pulling images from Harbor for application deployment, pushing custom images to Harbor, etc.</p><p>You may use one of the following methods to install Harbor:</p><ul><li><a href=https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid/1.5/vmware-tanzu-kubernetes-grid-15/GUID-packages-harbor-registry.html><strong>Tanzu Kubernetes Grid Package deployment</strong></a> to a Tanzu Kubernetes Grid cluster - VMware recommends this installation method for general use cases. The Tanzu packages, including Harbor, must either be pulled directly from VMware or be hosted in an internal registry.</li><li><a href=https://goharbor.io/docs/latest/install-config/installation-prereqs/><strong>VM-based deployment</strong></a> using <code>docker-compose</code> - VMware recommends using this installation method in cases where Tanzu Kubernetes Grid is being installed in an air-gapped or Internet-less environment and no pre-existing image registry exists to host the Tanzu Kubernetes Grid system images. VM-based deployments are only supported by VMware Global Support Services to host the system images for air-gapped or Internet-less deployments. Do not use this method for hosting application images.</li><li><a href=https://goharbor.io/docs/latest/install-config/harbor-ha-helm/><strong>Helm-based deployment</strong></a> to a Kubernetes cluster - This installation method may be preferred for customers already invested in Helm. Helm deployments of Harbor are only supported by the Open Source community and not by VMware Global Support Services.</li></ul><p>If you are deploying Harbor without a publicly signed certificate, you must include the Harbor root CA in your Tanzu Kubernetes Grid clusters. To do so, follow the procedure in <a href=https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid/1.5/vmware-tanzu-kubernetes-grid-15/GUID-cluster-lifecycle-secrets.html#trust-custom-ca-certificates-in-new-clusters-6>Trust Custom CA Certificates on Cluster Nodes</a>.</p><p><img src=img/reference-designs/tko-on-vsphere/tko-on-vsphere-vds-7.png alt="Harbor Container Registry"></p><h2 id=monitoring>Monitoring</h2><p>Tanzu Kubernetes Grid provides cluster monitoring services by implementing the open source Prometheus and Grafana projects.</p><p>Tanzu Kubernetes Grid includes signed binaries for Prometheus and Grafana that you can deploy on Tanzu Kubernetes clusters to monitor cluster health and services.</p><ul><li>Prometheus is an open source systems monitoring and alerting toolkit. It can collect metrics from target clusters at specified intervals, evaluate rule expressions, display the results, and trigger alerts if certain conditions arise. The Tanzu Kubernetes Grid implementation of Prometheus includes Alert Manager, which you can configure to notify you when certain events occur.</li><li>Grafana is an open source visualization and analytics software. It allows you to query, visualize, alert on, and explore your metrics no matter where they are stored. Grafana is used for visualizing Prometheus metrics without the need to manually write the PromQL queries. You can create custom charts and graphs in addition to the pre-packaged options.</li></ul><p>You deploy Prometheus and Grafana on Tanzu Kubernetes clusters. The following diagram shows how the monitoring components on a cluster interact.</p><p><img src=img/reference-designs/tko-on-vsphere/images-monitoring-stack.png alt="Monitoring components interaction in a cluster"></p><h2 id=logging>Logging</h2><p>Fluent Bit is a lightweight log processor and forwarder that allows you to collect data and logs from different sources, unify them, and send them to multiple destinations. Tanzu Kubernetes Grid includes signed binaries for Fluent Bit that you can deploy on management clusters and on Tanzu Kubernetes clusters to provide a log-forwarding service.</p><p>Tanzu for Kubernetes Operations includes <a href=https://fluentbit.io/>Fluent Bit</a> as a user managed package for integration with logging platforms such as vRealize LogInsight, Elasticsearch, Splunk, or other logging solutions. For information about configuring Fluent Bit to your logging provider, see <a href=https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid/1.5/vmware-tanzu-kubernetes-grid-15/GUID-packages-logging-fluentbit.html>Implement Log Forwarding with Fluent Bit</a>.</p><p>You can deploy Fluent Bit on any management cluster or Tanzu Kubernetes clusters from which you want to collect logs. First, you configure an output plugin on the cluster from which you want to gather logs, depending on the endpoint that you use. Then, you deploy Fluent Bit on the cluster as a package.</p><p>vRealize Log Insight (vRLI) provides real-time log management and log analysis with machine learning based intelligent grouping, high-performance searching, and troubleshooting across physical, virtual, and cloud environments. vRLI already has a deep integration with the vSphere platform where you can get key actionable insights, and it can be extended to include the cloud native stack as well.</p><p>vRealize Log Insight appliance is available as a separate on-prem deployable product. You can also choose to go with the SaaS version vRealize Log Insight Cloud.</p><h2 id=appendix>Appendix</h2><h3 id=configure-node-sizes>Configure Node Sizes</h3><p>The Tanzu CLI creates the individual nodes of management clusters and Tanzu Kubernetes clusters according to the settings that you provide in the configuration file.</p><p>On vSphere, you can configure all node VMs to have the same predefined configurations, set different predefined configurations for control plane and worker nodes, or customize the configurations of the nodes. By using these settings, you can create clusters that have nodes with different configurations to the management cluster nodes. You can also create clusters in which the control plane nodes and worker nodes have different configurations.</p><p><strong>Use Predefined Node Configurations</strong></p><p>The Tanzu CLI provides the following predefined configurations for cluster nodes:</p><table><thead><tr><th><strong>Size</strong></th><th><strong>CPU</strong></th><th><strong>Memory (in GB)</strong></th><th><strong>Disk (in GB)</strong></th></tr></thead><tbody><tr><td>Small</td><td>2</td><td>4</td><td>20</td></tr><tr><td>Medium</td><td>2</td><td>8</td><td>40</td></tr><tr><td>Large</td><td>4</td><td>16</td><td>40</td></tr><tr><td>Extra-large</td><td>8</td><td>32</td><td>80</td></tr></tbody></table><p>To create a cluster in which all of the control plane and worker node VMs are the same size, specify the <code>SIZE</code> variable. If you set the <code>SIZE</code> variable, all nodes will be created with the configuration that you set.</p><ul><li><code>SIZE: "large"</code></li></ul><p>To create a cluster in which the control plane and worker node VMs are different sizes, specify the <code>CONTROLPLANE_SIZE</code> and <code>WORKER_SIZE</code> options.</p><ul><li><code>CONTROLPLANE_SIZE: "medium"</code></li><li><code>WORKER_SIZE: "large"</code></li></ul><p>You can combine the <code>CONTROLPLANE_SIZE</code> and <code>WORKER_SIZE</code> options with the <code>SIZE</code> option. For example, if you specify <code>SIZE: "large"</code> with <code>WORKER_SIZE: "extra-large"</code>, the control plane nodes will be set to large and worker nodes will be set to extra-large.</p><ul><li><code>SIZE: "large"</code></li><li><code>WORKER_SIZE: "extra-large"</code></li></ul><p><strong>Define Custom Node Configurations</strong></p><p>You can customize the configuration of the nodes rather than using the predefined configurations.</p><p>To use the same custom configuration for all nodes, specify the <code>VSPHERE_NUM_CPUS</code>, <code>VSPHERE_DISK_GIB</code>, and <code>VSPHERE_MEM_MIB</code> options.</p><ul><li><code>VSPHERE_NUM_CPUS: 2</code></li><li><code>VSPHERE_DISK_GIB: 40</code></li><li><code>VSPHERE_MEM_MIB: 4096</code></li></ul><p>To define different custom configurations for control plane nodes and worker nodes, specify the <code>VSPHERE_CONTROL_PLANE_*</code> and <code>VSPHERE_WORKER_*</code></p><ul><li><code>VSPHERE_CONTROL_PLANE_NUM_CPUS: 2</code></li><li><code>VSPHERE_CONTROL_PLANE_DISK_GIB: 20</code></li><li><code>VSPHERE_CONTROL_PLANE_MEM_MIB: 8192</code></li><li><code>VSPHERE_WORKER_NUM_CPUS: 4</code></li><li><code>VSPHERE_WORKER_DISK_GIB: 40</code></li><li><code>VSPHERE_WORKER_MEM_MIB: 4096</code></li></ul><h2 id=summary>Summary</h2><p>Tanzu Kubernetes Grid on vSphere on hyper-converged hardware offers high-performance potential, convenience, and addresses the challenges of creating, testing, and updating on-premises Kubernetes platforms in a consolidated production environment. This validated approach will result in a near-production quality installation with all the application services needed to serve combined or uniquely separated workload types through a combined infrastructure solution.</p><p>This plan meets many Day 0 needs for quickly aligning product capabilities to full stack infrastructure, including networking, firewalling, load balancing, workload compute alignment, and other capabilities.</p><h2 id=deployment-instructions>Deployment Instructions</h2><p>For instructions on how to deploy this reference design, see <a href=../deployment-guides/tko-on-vsphere-vds.md>Deploy Tanzu for Kubernetes Operations on vSphere with VMware VDS</a>.</p></div></div><div class="loader-container-toc col-lg-2 w-100" style=display:none><span class=spinner></span></div><div class="rhs-right-container d-none d-lg-block col-lg-2"><div class=rhs-right-panel><div class="rhs-right-panel-header d-flex justify-content-between"><div class=rhs-right-panel-title>In this article</div></div><div id=right-toc-div-id class="onpage-toc-container d-flex flex-column"><nav id=TableOfContents><ul><li><a href=#supported-component-matrix>Supported Component Matrix</a></li><li><a href=#tanzu-kubernetes-grid-components>Tanzu Kubernetes Grid Components</a></li><li><a href=#tanzu-kubernetes-grid-storage>Tanzu Kubernetes Grid Storage</a></li><li><a href=#tanzu-kubernetes-clusters-networking>Tanzu Kubernetes Clusters Networking</a></li><li><a href=#tanzu-kubernetes-grid-infrastructure-networking>Tanzu Kubernetes Grid Infrastructure Networking</a></li><li><a href=#tanzu-kubernetes-grid-on-vsphere-networking-with-nsx-advanced-load-balancer>Tanzu Kubernetes Grid on vSphere Networking with NSX Advanced Load Balancer</a></li><li><a href=#nsx-advanced-load-balancer-components>NSX Advanced Load Balancer Components</a></li><li><a href=#network-architecture>Network Architecture</a><ul><li><a href=#network-requirements>Network Requirements</a></li><li><a href=#subnet-and-cidr-examples>Subnet and CIDR Examples</a></li></ul></li><li><a href=#firewall-requirements>Firewall Requirements</a></li><li><a href=#installation-experience>Installation Experience</a></li><li><a href=#design-recommendations>Design Recommendations</a><ul><li><a href=#nsx-advanced-load-balancer-recommendations>NSX Advanced Load Balancer Recommendations</a></li><li><a href=#network-recommendations>Network Recommendations</a></li><li><a href=#tanzu-kubernetes-grid-clusters-recommendations>Tanzu Kubernetes Grid Clusters Recommendations</a></li><li><a href=#kubernetes-ingress-routing>Kubernetes Ingress Routing</a></li></ul></li><li><a href=#nsx-advanced-load-balancer-sizing-guidelines>NSX Advanced Load Balancer Sizing Guidelines</a><ul><li><a href=#nsx-advanced-load-balancer-controller-sizing-guidelines>NSX Advanced Load Balancer Controller Sizing Guidelines</a></li><li><a href=#service-engine-sizing-guidelines>Service Engine Sizing Guidelines</a></li></ul></li><li><a href=#container-registry>Container Registry</a></li><li><a href=#monitoring>Monitoring</a></li><li><a href=#logging>Logging</a></li><li><a href=#appendix>Appendix</a><ul><li><a href=#configure-node-sizes>Configure Node Sizes</a></li></ul></li><li><a href=#summary>Summary</a></li><li><a href=#deployment-instructions>Deployment Instructions</a></li></ul></nav></div></div></div></section></div><footer class=tech-pub-footer><div id=page-footer><section class="footer-component footer-container"><div class=personalization_div_1 style=min-height:1px></div><div class=personalization_div_2 style=min-height:1px></div><div class=container><div class=content><div class=row><div class="col-lg-12 col-md-12"><footer class=footer><div class=row><div class="col-lg-2 col-md-12 mb-40 mt-3"><a class=footer-vmware-logo href=https://www-lt.vmware.com/ name="nav_footer : VMware Logo"><picture class=float-lg-left><source media=(max-width:800px) srcset=https://www-lt.vmware.com/content/dam/digitalmarketing/vmware/vm-logo-big.png.imgo.jpeg><img loading=lazy class=vmware-logo src=/validated-solutions/img/vm-logo-big.png alt=VMware title=VMware></picture></a></div></div></footer></div></div></div></div></section></div></footer><script src=/validated-solutions/js/pageStore.js></script>
<script src=/validated-solutions/js/main.js></script></body></html>