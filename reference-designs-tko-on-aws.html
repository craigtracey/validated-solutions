<!doctype html><html><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=0"><meta name="last modified" content="27/09/2021 12:47:24"><meta name=abstract content><meta name=author content="dpavel@vmware.com"><meta name=primary-product-name content="MD2Docs-TestBed"><meta name=primary-product-version content="1"><meta name=description content><meta name=guid content="GUID-1Intro"><meta name=language content="en"><meta name=title content="Basic Markdown"><meta name=publication-author content="dpavel@vmware.com"><meta property="og:title" content="Basic Markdown"><meta property="og:image" content="https://docs-uat.vmware.com/uicontent/images/vmware-docs-default.png"><meta property="og:description" content><meta property="og:type" content="article"><meta property="og:locale" content="en"><meta property="og:url" content="https://docs-uat-staging.vmware.com/en/MD2Docs-TestBed/1/md-2-docs-test-bed/GUID-1Intro.html"><meta name=cdf-utag content="https://tags.tiqcdn.com/utag/vmware/cdf-privacy/qa/utag.js"><link rel=stylesheet type=text/css href=/css/commonltr.css><link rel=stylesheet type=text/css href=/css/non-draft.vmware.productdocs.css><link rel=canonical href=https://docs-uat-staging.vmware.com/en/MD2Docs-TestBed/1/md-2-docs-test-bed/GUID-1Intro.html><link class=user href=/css/responsive.css rel=stylesheet type=text/css><link rel=icon href=https://www.vmware.com/favicon.ico type=image/x-icon><link rel=stylesheet href=/css/v2-global.20200911172508.css><title>Docs Preview</title></head><body><header class=tech-pub-header><div id=header class="global-header col-12"><div class="row desktop-header h-100"><div class="col col-md-3 align-self-center header-logo-wrapper"><div class="d-inline-flex align-items-center justify-content-start w-100"><div class="d-inline-flex align-items-center w-100"><span class="my-auto d-md-none header-menu-icon"><i class="fa fa-bars"></i></span><h1><a href=https://docs-uat-staging.vmware.com/ class="d-inline-flex align-items-center my-auto nav-link header-logo-url pl-md-1 pl-xl-3"><span class=mr-2><img src=/img/vm-logo.png alt="VMware Logo"></span>
<span class=vm-logo-title>Docs Preview</span></a></h1></div><div class=align-items-center><span id=toggleTOC class="d-md-none header-toc-icon"><i class="fa fa-ellipsis-v px-2"></i></span></div></div></div></div></div><div class="col-12 vmware-gradient w-100 px-0 mx-0"></div></header><div class="tech-pub-container main-container d-flex flex-column pubView"><section class="tech-pub-section d-flex flex-md-row"><div class="lhs lhs-container col-md-4 col-lg-3" style=display:block><div class=backdrop></div><div class=left-panel><div class="panel-header position-relative hidden-xs"><div class="panel-header-left d-flex justify-content-between align-items-center"><span class=container-collapse-expand><span id=expand-all-id class=expand-tree onclick=expandAll()><span class="lhs-expand-shape align-middle"><i class="fa fa-chevron-down"></i></span>
<span class="expand-text align-middle" data-i18n data-i18n-expand-all>Expand All</span></span>
<span id=collapse-all-id class="collapse-tree hide" onclick=collapseAll()><span class="lhs-collapse-shape align-middle"><i class="fa fa-chevron-up"></i></span>
<span class="collapse-text align-middle" data-i18n data-i18n-collapse-all>Collapse All</span></span></span></div></div><div class="panel-content p-2" id=left_toc><div class="dropdown collection-dropdown-container w-100"><button class="btn w-100 text-left dropdown-toggle collection-name" type=button id=collectionDropdwnBtn data-toggle=dropdown aria-haspopup=true aria-label="Collection Dropdown" aria-expanded=false>
<span class=label></span>
<span class="float-right icon-down pl-2 fa fa-angle-down"></span></button><div class="dropdown-menu w-100" id=collectionMenu aria-labelledby=collectionDropdwnBtn></div></div><div id=tree class=mt-2></div><div class="w-100 px-2 toc-product-container"><a class="mr-3 my-2 position-relative toc-product-link"><span class=toc-product-name></span>
<span class=localized-page-name>Product Documentation</span></a></div><ul class=rm-default-ul-styles></ul></div></div></div><div class="rhs rhs-container col-md-8 col-lg-7" style=display:block><div class=rhs-top><div class="rhs-top-container-top d-flex flex-row justify-content-between"><div class=primary-header id=page-heading-id></div></div><div class="rhs-top-container-middle d-flex flex-row justify-content-between"></div><div class=rhs-top-container-bottom><div class=last-updated-container><div class=calendar-icon><svg width="36" height="36" viewBox="0 0 36 36" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path class="clr-i-outline clr-i-outline-path-1" d="M32.25 6H29V8h3V30H4V8H7V6H3.75A1.78 1.78.0 002 7.81V30.19A1.78 1.78.0 003.75 32h28.5A1.78 1.78.0 0034 30.19V7.81A1.78 1.78.0 0032.25 6z"/><rect class="clr-i-outline clr-i-outline-path-2" x="8" y="14" width="2" height="2"/><rect class="clr-i-outline clr-i-outline-path-3" x="14" y="14" width="2" height="2"/><rect class="clr-i-outline clr-i-outline-path-4" x="20" y="14" width="2" height="2"/><rect class="clr-i-outline clr-i-outline-path-5" x="26" y="14" width="2" height="2"/><rect class="clr-i-outline clr-i-outline-path-6" x="8" y="19" width="2" height="2"/><rect class="clr-i-outline clr-i-outline-path-7" x="14" y="19" width="2" height="2"/><rect class="clr-i-outline clr-i-outline-path-8" x="20" y="19" width="2" height="2"/><rect class="clr-i-outline clr-i-outline-path-9" x="26" y="19" width="2" height="2"/><rect class="clr-i-outline clr-i-outline-path-10" x="8" y="24" width="2" height="2"/><rect class="clr-i-outline clr-i-outline-path-11" x="14" y="24" width="2" height="2"/><rect class="clr-i-outline clr-i-outline-path-12" x="20" y="24" width="2" height="2"/><rect class="clr-i-outline clr-i-outline-path-13" x="26" y="24" width="2" height="2"/><path class="clr-i-outline clr-i-outline-path-14" d="M10 10a1 1 0 001-1V3A1 1 0 009 3V9a1 1 0 001 1z"/><path class="clr-i-outline clr-i-outline-path-15" d="M26 10a1 1 0 001-1V3a1 1 0 00-2 0V9a1 1 0 001 1z"/><rect class="clr-i-outline clr-i-outline-path-16" x="13" y="6" width="10" height="2"/><rect x="0" y="0" width="36" height="36" fill-opacity="0"/></svg></div><span class=last-updated-label data-i18n-updated-on>Updated on</span>
&nbsp;
<span class=last-updated-date-ph>9/22/22</span></div></div></div><div class="rhs-center article-wrapper" id=content-div-id><h1 id=vmware-tanzu-for-kubernetes-operations-on-aws-reference-design>VMware Tanzu for Kubernetes Operations on AWS Reference Design</h1><p>VMware Tanzu for Kubernetes Operations simplifies operation of Kubernetes for multi-cloud deployment by centralizing management and governance for clusters and teams across on-premises, public clouds, and edge. Tanzu for Kubernetes Operations delivers an open source aligned Kubernetes distribution with consistent operations and management to support infrastructure and application modernization.</p><p>This document lays out a reference design for deploying VMware Tanzu for Kubernetes Operations with Tanzu components on AWS.</p><p>The following reference design is based on the architecture and components described in <a href=index.md>VMware Tanzu for Kubernetes Operations Reference Architecture</a>.</p><blockquote><p><strong>Note:</strong> This reference design is supported and validated for customers deploying Tanzu Kubernetes Grid 1.5.x on AWS.</p></blockquote><p><img src=.//img/reference-designs/tko-on-aws/tkg-aws-overview.png alt="Tanzu Edition reference design diagram"></p><h2 id=network-overview>Network Overview</h2><p>The following network diagram shows the network layout used with this reference design. It shows the layout for a single virtual private cloud (VPC). The network layout uses the following types of subnets:</p><ol><li>One private subnet for each AWS availability zone (AZ). These subnets are not automatically allocated a public IP address. The default gateway is a NAT gateway.</li><li>One public subnet for each AWS availability zone (AZ). These subnets are automatically allocated a public IP address. The default gateway is an Internet gateway if subnet is connected to the Internet. A public subnet is optional if you do not need Internet ingress or egress.</li></ol><p><img src=.//img/reference-designs/tko-on-aws/tkg-aws-network-overview.png alt="TKG AWS network overview diagram"></p><h3 id=network-recommendations>Network Recommendations</h3><p>This reference design uses Tanzu Kubernetes Grid to manage the lifecycle of multiple Kubernetes workload clusters by bootstrapping a Kubernetes management cluster with the Tanzu command line tool. Consider the following when configuring the network for Tanzu Kubernetes Grid:</p><ul><li><p>Use an internal load balancer scheme. A best practice is to create an internal load balancer as a best practice to avoid exposing the Kubernetes API to the public Internet. To avoid creating a public-facing load balancer, you can set AWS_LOAD_BALANCER_SCHEME_INTERNAL to true in the cluster configuration file <code>AWS_LOAD_BALANCER_SCHEME_INTERNAL: true</code>
This setting customizes the management cluster’s load balancer to use an internal scheme, which means that its Kubernetes API server will not be accessible and routed over the Internet. If you use an internal load balancer, run Tanzu Kubernetes Grid from a machine with access to the target VPC private IP space.</p></li><li><p>If you don&rsquo;t want an outbound Internet or inbound connection from AWS, you can eliminate the public subnet.</p></li><li><p>Beware that 172.17.0.0/16 is the default docker subnet. If you are going to use that for a VPC deployment, you must change your docker container subnet.</p></li></ul><h2 id=storage>Storage</h2><p>Tanzu Kubernetes Grid ships with the AWS cloud storage driver, which allows you to provision stateful storage volumes in your Tanzu Kubernetes Grid cluster. The following storage classes are available:</p><ul><li>gp2 - General Purpose SSD (default storage class)</li><li>io1 - IOPS provisioned SSD</li><li>st1 - Throughput Optimized HHD</li><li>sc1 - Cold HDD</li></ul><p>For more information on the available storage options see <a href=https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html>Amazon EBS volume types</a>.</p><h2 id=vpc-architectures>VPC Architectures</h2><p>In a production deployment, Tanzu Kubernetes Grid creates a multi-AZ deployment.</p><p>We recommend that you create the VPCs before you deploy Tanzu Kubernetes Grid. Also, make sure that you tag a public and private subnet in each AZ, including the control plane cluster, with a key of <code>kubernetes.io/cluster/&lt;cluster_name></code>. As a best practice, ensure that the value you use for the public and private subnets for an AZ can easily identify the subnets as belonging to the same AZ. For example,</p><pre tabindex=0><code>aws ec2 create-subnet --vpc-id $vpcId --cidr-block &lt;ip_address&gt;  --availability-zone ${AWS_REGION}b  --tag-specifications ‘ResourceType=subnet, Tags=[{Key=Name,Value=priv-b}]’ --output json &gt; $WORKING_DIR/subnet-priv-b
aws ec2 create-subnet --vpc-id $vpcId --cidr-block &lt;ip_address&gt;  --availability-zone ${AWS_REGION}b  --tag-specifications ‘ResourceType=subnet, Tags=[{Key=Name,Value=pub-b}]’  --output json &gt; $WORKING_DIR/subnet-pub-b
</code></pre><p>Based on your application needs and desired outcomes, you can organize your workloads using one of the following VPC architectures.</p><h3 id=single-vpc-with-multiple-availability-zones>Single VPC with Multiple Availability Zones</h3><p>Most use cases require only a single VPC spread across multiple AZs as shown in the reference diagram. If more separation is needed within one VPC, more subnets can be used to provide better IP based visibility to corporate firewalls.</p><p><img src=.//img/reference-designs/tko-on-aws/tkg-aws-single-vpc-multi-az.jpg alt="TKG on AWS with Single VPC and Multiple Availability Zones diagram"></p><h3 id=multiple-vpc-with-multiple-availability-zones>Multiple VPC with Multiple Availability Zones</h3><p>For more separation of application workloads on AWS, you can deploy separate Kubernetes clusters to independent VPCs. This separation might be desirable for workloads with different compliance requirements, across different business units, or with different levels of Internet ingress and egress. By default, Tanzu Kubernetes Grid creates a VPC per cluster.</p><p>The following diagram shows an example architecture with multiple VPCs. The control plane load balancers in the example architecture are configured as internal load balancers.</p><p><img src=.//img/reference-designs/tko-on-aws/tkg-aws-multi-vpc-multi-az.jpg alt="TKG on AWS with Multiple VPCs and Multiple Availability Zones diagram"></p><p>Another variant of multiple VPC and multiple AZ design is to have one VPC for the control plane and another for just workload clusters. The following diagram shows such a design.</p><p><img src=.//img/reference-designs/tko-on-aws/tkg-aws-multi-vpc-multi-az-separated-control-plane-and-workloads.jpg alt="TKG on AWS with Segregated VPCs for control plane and workloads diagram"></p><h2 id=availability>Availability</h2><p>We recommend deploying your Tanzu Kubernetes Grid cluster in an odd number of AZs to ensure high availability of components that require consensus to operate in failure modes.</p><p>The Tanzu Kubernetes Grid management cluster performs <a href=https://cluster-api.sigs.k8s.io/tasks/automated-machine-management/healthchecking.html>Machine Health Checks</a> on all Kubernetes worker VMs. This ensures workload remain in a functional state, and can remediate issues such as:</p><ul><li>Worker VM is accidentally deleted or corrupted.</li><li>Kubelet process on worker VM is accidentally stopped or corrupted.</li></ul><p>This health check ensures that your worker capacity remains stable and can be scheduled for workloads. This health check, however, does not apply to the control plane or the load balancer VMs. The health check does not recreate VMs due to physical host failure.</p><h3 id=quotas>Quotas</h3><p>Provide sufficient quotas to support both the management cluster and the workload clusters in your deployment. Otherwise, the cluster deployments will fail. Depending on the number of workload clusters you will deploy, you may need to increase the AWS services quotas from their default values. You will need to increase the quota in every region in which you plan to deploy Tanzu Kubernetes Grid.</p><p>See <a href=https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid/1.5/vmware-tanzu-kubernetes-grid-15/GUID-mgmt-clusters-aws.html#aws-resources>Tanzu Kubernetes Grid resources in AWS account</a> for more details.</p><p><strong>Note</strong> : The number of VPCs will depend on the VPC architecture you have selected.</p><p>See <a href=https://docs.aws.amazon.com/general/latest/gr/aws_service_limits.html>AWS service quotas</a> for more information on AWS services default quotas.</p><h2 id=cluster-creation-and-management>Cluster Creation and Management</h2><p>This reference design uses Tanzu Kubernetes Grid to create and manage ubiquitous Kubernetes clusters on AWS using Kubernetes <a href=https://cluster-api.sigs.k8s.io/>Cluster API</a>. Tanzu Kubernetes Grid functions through the creation of a management cluster which houses the Cluster API. The Cluster API then interacts with the infrastructure provider to service workload Kubernetes cluster lifecycle requests.</p><p><img src=.//img/reference-designs/tko-on-aws/tkg-kickstart-install.png alt="Tanzu Kubernetes Grid Kickstart Install Screen"></p><p>Tanzu Editions include components for observability, as well as container registry. We recommended installing the necessary components into a centralized shared services cluster.</p><h2 id=global-cluster-lifecycle-management>Global Cluster Lifecycle Management</h2><p>Registering Management cluster and attaching workload clusters to Tanzu Mission Control allows you to manage your global portfolio of Kubernetes clusters. You can do the following with Tanzu Mission Control:</p><ul><li>Centralized lifecycle management: managing the creation and deletion of workload clusters using registered management or supervisor clusters</li><li>Centralized management: viewing the inventory of clusters and the health of clusters and their components</li><li>Authorization: centralized authentication and authorization with federated identity from multiple sources (e.g., AD, LDAP, and SAML), plus an easy-to-use policy engine for granting the right access to the right users across teams</li><li>Compliance: enforcing all clusters to apply the same set of policies</li><li>Data protection: managing Velero deployment, configuration, and schedule to ensure that cluster manifests and persistent volumes are backed up & restorable</li><li>Inspection: running a Sonobouy conformance check suite to ensure Kubernetes cluster functionality</li></ul><p><img src=.//img/reference-designs/tko-on-aws/tmc-global-policy-control-plane.png alt="VMware Tanzu Mission Control - global policy control plane diagram"></p><p>For a complete list of Tanzu Mission Control features, see <a href=https://content.cdntwrk.com/files/aT0xMjk5NjY3JnY9OSZpc3N1ZU5hbWU9dG1jLWNvbXBhcmlzb24tY2hhcnQmY21kPWQmc2lnPTc2YTA2N2E4MWRjMmVkNjE0ZDcwMTlmNjc4NjhmMjI4>VMware Tanzu Mission
Control Feature Comparison</a>.</p><p>To Register your management or supervisor cluster for management through Tanzu Mission Control, navigate to <strong>Administration > Management Cluster</strong> on the Tanzu Mission Control console and follow the prompts.</p><p><img src=.//img/reference-designs/tko-on-aws/tmc-register-management-cluster.jpg alt="Tanzu Mission Control Register Management cluster"></p><p>To attach your cluster for management through Tanzu Mission Control, navigate to <strong>Clusters > Attach Cluster</strong> on the Tanzu Mission Control console and follow the prompts.</p><blockquote><p><strong>Note:</strong> If a workload cluster under management requires a proxy to access the Internet, you can use the Tanzu Mission Control CLI to <a href=https://docs.vmware.com/en/VMware-Tanzu-Mission-Control/services/tanzumc-using/GUID-97672F56-2AD4-46E6-94E1-805ED38D06C7.html>generate the YAML</a> necessary to install Tanzu Mission Control components on it.</p></blockquote><p><img src=.//img/reference-designs/tko-on-aws/tmc-attach-cluster-screen.png alt="Tanzu Mission Control attach cluster"></p><h2 id=ingress-and-load-balancing>Ingress and Load Balancing</h2><p>Tanzu Kubernetes Grid requires load balancing for both the control plane and the workload clusters. Tanzu Kubernetes Grid for AWS uses elastic load balancers for the control plane and workload clusters.</p><p>For workload clusters, the Tanzu Kubernetes Grid <a href=https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid/1.5/vmware-tanzu-kubernetes-grid-15/GUID-packages-ingress-contour.html>Contour ingress controller package</a> can be used for layer 7 load balancing.</p><p>If you have deployed with both public and private subnets, by default you will get an Internet-facing load balancer. If you want a private load balancer, you can specifically request one by setting <code>service.beta.kubernetes.io/aws-load-balancer-internal: "true"</code> in the annotations of the service. This setting also applies to the Contour ingress and controls whether Contour is internal-facing or external-facing.</p><p><img src=.//img/reference-designs/tko-on-aws/tkg-aws-ingress-contour.jpg alt="TKG on AWS ingress with Contour diagram"></p><p>In Tanzu Kubernetes Grid, you can optionally deploy the <a href=https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid/1.5/vmware-tanzu-kubernetes-grid-15/GUID-packages-external-dns.html>external-dns package</a>, which automates the updates to DNS records in AWS (Route53) associated with ingress resources or LoadBalancer services. This can automate away toil associated with DNS record management for externally exposed services.</p><h2 id=authentication-with-pinniped>Authentication with Pinniped</h2><p>The Pinniped authentication and authorization service components are deployed into the management cluster. Pinniped uses the OIDC or LDAP identity provider (IDP) configurations specified during the management cluster deployment. The workload cluster inherits its authentication configurations from its management cluster. With authentication in place, a Kubernetes administrator can enforce role-based access control (RBAC) with Kubernetes RoleBinding resources. These resources associate an identity provider user with a given Kubernetes role on the workload cluster.</p><p>Pinniped consists of following components:</p><ul><li><strong>The Pinniped Supervisor</strong> is an OIDC server that authenticates users through an external identity provider (IDP)/LDAP, and then issues its own federation ID tokens to be passed on to clusters based on the user information from the IDP.</li><li><strong>The Pinniped Concierge</strong> is a credential exchange API which takes as input a credential from an identity source (e.g., Pinniped Supervisor, proprietary IDP), authenticates the user via that credential, and returns another credential which is understood by the host Kubernetes cluster or by an impersonation proxy which acts on behalf of the user.</li><li><strong>Dex</strong> Pinniped uses Dex as a broker for your upstream LDAP identity provider. Dex is only deployed when LDAP is selected as the OIDC backend during Tanzu Kubernetes Grid management cluster creation.</li></ul><p>The following diagram shows the Pinniped authentication flow with an external IDP. In the diagram, the blue arrows represent the authentication flow between the workload cluster, the management cluster and the external IDP. The green arrows represent Tanzu CLI and <code>kubectl</code> traffic between the workload cluster, the management cluster and the external IDP.</p><p><img src=.//img/reference-designs/tko-on-aws/authwith-Pinniped.png alt="Authentication with pinniped"></p><p>See the <a href=https://pinniped.dev/docs/>Pinniped docs</a> for more information on how to integrate Pinniped into Tanzu Kubernetes Grid with OIDC providers and LDAP.</p><p>We recommend the following best practices for managing identities in Tanzu Kubernetes Grid provisioned clusters:</p><ul><li>Configure Pinniped services during management cluster creation.</li><li>Limit access to cluster resources following the <a href=https://csrc.nist.gov/glossary/term/least_privilege>least privilege</a> principle.</li><li>Limit access to management clusters to the appropriate set of users. For example, provide access only to users who are responsible for managing infrastructure and cloud resources but not to application developers. This is especially important because access to the management cluster inherently provides access to all workload clusters.</li><li>Limit cluster administrator access for workload clusters to the appropriate set of users. For example, provide access to users who are responsible for managing infrastructure and platform resources in your organization, but not to application developers.</li><li>Connect to an <a href=https://csrc.nist.gov/glossary/term/identity_provider>identity provider</a> to manage the user identities allowed to access cluster resources instead of relying on administrator-generated <code>kubeconfig</code> files.</li></ul><h2 id=observability>Observability</h2><h3 id=metrics-monitoring-with-tanzu-observability-by-wavefront-recommended-solution>Metrics Monitoring with Tanzu Observability by Wavefront (Recommended Solution)</h3><p>Using <a href=https://tanzu.vmware.com/observability>VMware Tanzu Observability by Wavefront</a> significantly enhances observability. Tanzu Observability is a VMware SaaS application that collects and displays metrics and trace data from the full stack platform, as well as from applications. The service provides the ability to create alerts tuned with advanced analytics, assist in the troubleshooting of systems, and to understand the impact of running production code.</p><p>Tanzu Observability collects data from Kubernetes and from applications running within Kubernetes.</p><p>You can configure Tanzu Observability with an array of capabilities. There are over 200 <a href=https://vmware.wavefront.com/integrations>integrations</a> with prebuilt dashboards available in Wavefront.</p><p>The following table describes the plugins we recommend for this design:</p><table><thead><tr><th>Plugin</th><th>Purpose</th><th>Key Metrics</th><th>Example Metrics</th></tr></thead><tbody><tr><td>Wavefront Kubernetes Integration</td><td>Collect metrics from Kubernetes clusters and pods</td><td>Kubernetes container and POD statistics</td><td>POD CPU usage rate</td></tr><tr><td>Wavefront by VMware for Istio</td><td>Adapts Istio collected metrics and forwards to Wavefront</td><td>Istio metrics including request rates, trace rates, throughput, etc.</td><td>Request rate (Transactions per Second)</td></tr></tbody></table><p><img src=.//img/reference-designs/tko-on-aws/image11.png alt=kubernetes-metrics-1>
<img src=.//img/reference-designs/tko-on-aws/image6.png alt=kubernetes-metrics-2></p><h4 id=custom-tanzu-observability-dashboards>Custom Tanzu Observability Dashboards</h4><p>Tanzu Observability provides various out-of-the-box dashboards. You can customize the dashboards for your particular deployment. For information on how to customize Tanzu Observability dashboards for Tanzu for Kubernetes Operations, see <a href=../deployment-guides/tko-to-customized-dashboard.md>Customize Tanzu Observability Dashboard for Tanzu for Kubernetes Operations</a>.</p><h3 id=metrics-monitoring-with-prometheus-and-grafana-alternative-solution>Metrics Monitoring with Prometheus and Grafana (Alternative Solution)</h3><p>Tanzu Kubernetes Grid also supports <a href=https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid/1.5/vmware-tanzu-kubernetes-grid-15/GUID-packages-prometheus.html>Prometheus</a> and <a href=https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid/1.5/vmware-tanzu-kubernetes-grid-15/GUID-packages-grafana.html>Grafana</a> as an alternative on-premises solution for monitoring Kubernetes clusters.</p><p>Prometheus exposes scrapable metrics endpoints for various monitoring targets throughout your cluster. Metrics are ingested by polling the endpoints at a set interval. The metrics are then stored in a time-series database. You use the <a href=https://prometheus.io/docs/prometheus/latest/querying/basics/>Prometheus Query Language interface</a> to explore the metrics.</p><p>Grafana is responsible for visualizing Prometheus metrics without the need to manually write the <code>PromQL</code> queries. You can create custom charts and graphs in addition to the pre-packaged options.</p><p><img src=.//img/reference-designs/tko-on-aws/tanzu-observability-cpu-dashboard.png alt="Tanzu Observability CPU utilization dashboard"></p><p><img src=.//img/reference-designs/tko-on-aws/tanzu-observability-availability-dashboard.png alt="Tanzu Observability availability dashboard"></p><p>Prometheus and Grafana are user-managed packages available with Tanzu Kubernetes Grid. For more information about packages bundled with Tanzu Kubernetes Grid, see <a href=https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid/1.5/vmware-tanzu-kubernetes-grid-15/GUID-packages-index.html>Install and Configure Packages</a>. For more information about user-managed packages, see <a href=https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid/1.5/vmware-tanzu-kubernetes-grid-15/GUID-packages-user-managed-index.html>User-Managed Packages</a></p><h3 id=log-forwarding>Log Forwarding</h3><p>Tanzu also includes Fluent Bit for integration with logging platforms such as vRealize, Log Insight Cloud, and Elasticsearch. See <a href=https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid/1.5/vmware-tanzu-kubernetes-grid-15/GUID-packages-logging-fluentbit.html>Fluent Bit Documentation</a> for various logging providers.</p><h2 id=summary>Summary</h2><p>Tanzu Kubernetes Grid on AWS offers high-performance potential, convenience, and addresses the challenges of creating, testing, and updating cloud based Kubernetes platforms in a consolidated production environment. This validated approach will result in a production quality installation with all the application services needed to serve combined or uniquely separated workload types via a combined infrastructure solution.</p><p>This plan meets many Day 0 needs for aligning product capabilities, such as configuring firewall rules, networking, load balancing, and workload compute, to the full stack infrastructure.</p><h2 id=deployment-instructions>Deployment Instructions</h2><p>For instructions on how to deploy this reference design, see <a href=../deployment-guides/tko-aws.md>Deploy Tanzu for Kubernetes Operations on AWS</a>.</p></div></div><div class="loader-container-toc col-lg-2 w-100" style=display:none><span class=spinner></span></div><div class="rhs-right-container d-none d-lg-block col-lg-2"><div class=rhs-right-panel><div class="rhs-right-panel-header d-flex justify-content-between"><div class=rhs-right-panel-title>In this article</div></div><div id=right-toc-div-id class="onpage-toc-container d-flex flex-column"><nav id=TableOfContents><ul><li><a href=#network-overview>Network Overview</a><ul><li><a href=#network-recommendations>Network Recommendations</a></li></ul></li><li><a href=#storage>Storage</a></li><li><a href=#vpc-architectures>VPC Architectures</a><ul><li><a href=#single-vpc-with-multiple-availability-zones>Single VPC with Multiple Availability Zones</a></li><li><a href=#multiple-vpc-with-multiple-availability-zones>Multiple VPC with Multiple Availability Zones</a></li></ul></li><li><a href=#availability>Availability</a><ul><li><a href=#quotas>Quotas</a></li></ul></li><li><a href=#cluster-creation-and-management>Cluster Creation and Management</a></li><li><a href=#global-cluster-lifecycle-management>Global Cluster Lifecycle Management</a></li><li><a href=#ingress-and-load-balancing>Ingress and Load Balancing</a></li><li><a href=#authentication-with-pinniped>Authentication with Pinniped</a></li><li><a href=#observability>Observability</a><ul><li><a href=#metrics-monitoring-with-tanzu-observability-by-wavefront-recommended-solution>Metrics Monitoring with Tanzu Observability by Wavefront (Recommended Solution)</a></li><li><a href=#metrics-monitoring-with-prometheus-and-grafana-alternative-solution>Metrics Monitoring with Prometheus and Grafana (Alternative Solution)</a></li><li><a href=#log-forwarding>Log Forwarding</a></li></ul></li><li><a href=#summary>Summary</a></li><li><a href=#deployment-instructions>Deployment Instructions</a></li></ul></nav></div></div></div></section></div><footer class=tech-pub-footer><div id=page-footer><section class="footer-component footer-container"><div class=personalization_div_1 style=min-height:1px></div><div class=personalization_div_2 style=min-height:1px></div><div class=container><div class=content><div class=row><div class="col-lg-12 col-md-12"><footer class=footer><div class=row><div class="col-lg-2 col-md-12 mb-40 mt-3"><a class=footer-vmware-logo href=https://www-lt.vmware.com/ name="nav_footer : VMware Logo"><picture class=float-lg-left><source media=(max-width:800px) srcset=https://www-lt.vmware.com/content/dam/digitalmarketing/vmware/vm-logo-big.png.imgo.jpeg><img loading=lazy class=vmware-logo src=/img/vm-logo-big.png alt=VMware title=VMware></picture></a></div></div></footer></div></div></div></div></section></div></footer><script src=/js/pageStore.js></script>
<script src=/js/main.js></script></body></html>