<!doctype html><html><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=0"><meta name="last modified" content="27/09/2021 12:47:24"><meta name=abstract content><meta name=author content="dpavel@vmware.com"><meta name=primary-product-name content="MD2Docs-TestBed"><meta name=primary-product-version content="1"><meta name=description content><meta name=guid content="GUID-1Intro"><meta name=language content="en"><meta name=title content="Basic Markdown"><meta name=publication-author content="dpavel@vmware.com"><meta property="og:title" content="Basic Markdown"><meta property="og:image" content="https://docs-uat.vmware.com/uicontent/images/vmware-docs-default.png"><meta property="og:description" content><meta property="og:type" content="article"><meta property="og:locale" content="en"><meta property="og:url" content="https://docs-uat-staging.vmware.com/en/MD2Docs-TestBed/1/md-2-docs-test-bed/GUID-1Intro.html"><meta name=cdf-utag content="https://tags.tiqcdn.com/utag/vmware/cdf-privacy/qa/utag.js"><link rel=stylesheet type=text/css href=/css/commonltr.css><link rel=stylesheet type=text/css href=/css/non-draft.vmware.productdocs.css><link rel=canonical href=https://docs-uat-staging.vmware.com/en/MD2Docs-TestBed/1/md-2-docs-test-bed/GUID-1Intro.html><link class=user href=/css/responsive.css rel=stylesheet type=text/css><link rel=icon href=https://www.vmware.com/favicon.ico type=image/x-icon><link rel=stylesheet href=/css/v2-global.20200911172508.css><title>Docs Preview</title></head><body><header class=tech-pub-header><div id=header class="global-header col-12"><div class="row desktop-header h-100"><div class="col col-md-3 align-self-center header-logo-wrapper"><div class="d-inline-flex align-items-center justify-content-start w-100"><div class="d-inline-flex align-items-center w-100"><span class="my-auto d-md-none header-menu-icon"><i class="fa fa-bars"></i></span><h1><a href=https://docs-uat-staging.vmware.com/ class="d-inline-flex align-items-center my-auto nav-link header-logo-url pl-md-1 pl-xl-3"><span class=mr-2><img src=/img/vm-logo.png alt="VMware Logo"></span>
<span class=vm-logo-title>Docs Preview</span></a></h1></div><div class=align-items-center><span id=toggleTOC class="d-md-none header-toc-icon"><i class="fa fa-ellipsis-v px-2"></i></span></div></div></div></div></div><div class="col-12 vmware-gradient w-100 px-0 mx-0"></div></header><div class="tech-pub-container main-container d-flex flex-column pubView"><section class="tech-pub-section d-flex flex-md-row"><div class="lhs lhs-container col-md-4 col-lg-3" style=display:block><div class=backdrop></div><div class=left-panel><div class="panel-header position-relative hidden-xs"><div class="panel-header-left d-flex justify-content-between align-items-center"><span class=container-collapse-expand><span id=expand-all-id class=expand-tree onclick=expandAll()><span class="lhs-expand-shape align-middle"><i class="fa fa-chevron-down"></i></span>
<span class="expand-text align-middle" data-i18n data-i18n-expand-all>Expand All</span></span>
<span id=collapse-all-id class="collapse-tree hide" onclick=collapseAll()><span class="lhs-collapse-shape align-middle"><i class="fa fa-chevron-up"></i></span>
<span class="collapse-text align-middle" data-i18n data-i18n-collapse-all>Collapse All</span></span></span></div></div><div class="panel-content p-2" id=left_toc><div class="dropdown collection-dropdown-container w-100"><button class="btn w-100 text-left dropdown-toggle collection-name" type=button id=collectionDropdwnBtn data-toggle=dropdown aria-haspopup=true aria-label="Collection Dropdown" aria-expanded=false>
<span class=label></span>
<span class="float-right icon-down pl-2 fa fa-angle-down"></span></button><div class="dropdown-menu w-100" id=collectionMenu aria-labelledby=collectionDropdwnBtn></div></div><div id=tree class=mt-2></div><div class="w-100 px-2 toc-product-container"><a class="mr-3 my-2 position-relative toc-product-link"><span class=toc-product-name></span>
<span class=localized-page-name>Product Documentation</span></a></div><ul class=rm-default-ul-styles></ul></div></div></div><div class="rhs rhs-container col-md-8 col-lg-7" style=display:block><div class=rhs-top><div class="rhs-top-container-top d-flex flex-row justify-content-between"><div class=primary-header id=page-heading-id></div></div><div class="rhs-top-container-middle d-flex flex-row justify-content-between"></div><div class=rhs-top-container-bottom><div class=last-updated-container><div class=calendar-icon><svg width="36" height="36" viewBox="0 0 36 36" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path class="clr-i-outline clr-i-outline-path-1" d="M32.25 6H29V8h3V30H4V8H7V6H3.75A1.78 1.78.0 002 7.81V30.19A1.78 1.78.0 003.75 32h28.5A1.78 1.78.0 0034 30.19V7.81A1.78 1.78.0 0032.25 6z"/><rect class="clr-i-outline clr-i-outline-path-2" x="8" y="14" width="2" height="2"/><rect class="clr-i-outline clr-i-outline-path-3" x="14" y="14" width="2" height="2"/><rect class="clr-i-outline clr-i-outline-path-4" x="20" y="14" width="2" height="2"/><rect class="clr-i-outline clr-i-outline-path-5" x="26" y="14" width="2" height="2"/><rect class="clr-i-outline clr-i-outline-path-6" x="8" y="19" width="2" height="2"/><rect class="clr-i-outline clr-i-outline-path-7" x="14" y="19" width="2" height="2"/><rect class="clr-i-outline clr-i-outline-path-8" x="20" y="19" width="2" height="2"/><rect class="clr-i-outline clr-i-outline-path-9" x="26" y="19" width="2" height="2"/><rect class="clr-i-outline clr-i-outline-path-10" x="8" y="24" width="2" height="2"/><rect class="clr-i-outline clr-i-outline-path-11" x="14" y="24" width="2" height="2"/><rect class="clr-i-outline clr-i-outline-path-12" x="20" y="24" width="2" height="2"/><rect class="clr-i-outline clr-i-outline-path-13" x="26" y="24" width="2" height="2"/><path class="clr-i-outline clr-i-outline-path-14" d="M10 10a1 1 0 001-1V3A1 1 0 009 3V9a1 1 0 001 1z"/><path class="clr-i-outline clr-i-outline-path-15" d="M26 10a1 1 0 001-1V3a1 1 0 00-2 0V9a1 1 0 001 1z"/><rect class="clr-i-outline clr-i-outline-path-16" x="13" y="6" width="10" height="2"/><rect x="0" y="0" width="36" height="36" fill-opacity="0"/></svg></div><span class=last-updated-label data-i18n-updated-on>Updated on</span>
&nbsp;
<span class=last-updated-date-ph>9/22/22</span></div></div></div><div class="rhs-center article-wrapper" id=content-div-id><hr><h2 id=url-reference-designs-tkg-nsxt-airgap-ramdoldhtml>url: reference-designs-tkg-nsxt-airgap-ra.md.old.html</h2><p>﻿# Tanzu Kubernetes Grid on NSX-T Networking in Airgap Environment - Reference Design</p><p>VMware Tanzu Kubernetes Grid (multi-cloud) provides organizations with a consistent, upstream-compatible, regional Kubernetes substrate that is ready for end-user workloads and ecosystem integrations.</p><p>An air-gapped environment is a network security measure employed to ensure a computer or computer network is secure by physically isolating it from unsecured networks, such as the public Internet or an unsecured local area network. This means a computer or network is disconnected from all other systems.</p><p>This document lays out a reference design for deploying Tanzu Kubernetes Grid in a vSphere environment backed by NSX-T Networking in an <strong>internet-restricted/airgap</strong> environment and offers a high-level overview of the different components required for setting up a Tanzu Kubernetes Grid environment. The following components are used in this design:</p><ul><li><p><strong>Tanzu Kubernetes Grid (TKG)</strong> - Enables creation and lifecycle management of Kubernetes clusters.</p></li><li><p><strong>NSX Advanced Load Balancer (Enterprise)</strong> - An enterprise-grade integrated Load balancer, that provides Layer 4 Load Balancer and layer 7 Ingress support, recommended for vSphere deployments without NSX-T, or when there are unique scaling requirements.</p></li><li><p><strong>Tanzu User-Managed Packages:</strong> User-managed packages are distributed via package repositories. The tanzu-standard package repository includes the following user-managed packages:</p><ul><li><p><a href=https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid/1.4/vmware-tanzu-kubernetes-grid-14/GUID-packages-cert-manager.html><strong>Cert Manager</strong></a> - Provides automated certificate management. It runs by default in management clusters.</p></li><li><p><a href=https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid/1.4/vmware-tanzu-kubernetes-grid-14/GUID-packages-ingress-contour.html><strong>Contour</strong></a> - Provides layer 7 ingress control to deployed HTTP(S) applications. Tanzu Kubernetes Grid includes signed binaries for Contour. Deploying Contour is a prerequisite for deploying the Prometheus, Grafana, and Harbor extensions.</p></li><li><p><a href=https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid/1.4/vmware-tanzu-kubernetes-grid-14/GUID-packages-logging-fluentbit.html><strong>Fluent Bit</strong></a> - Collects data and logs from different sources, unify them, and sends them to multiple destinations. Tanzu Kubernetes Grid includes signed binaries for Fluent Bit.</p></li><li><p><a href=https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid/1.4/vmware-tanzu-kubernetes-grid-14/GUID-packages-monitoring.html><strong>Prometheus</strong></a> - Provides out-of-the-box health monitoring of Kubernetes clusters. The Tanzu Kubernetes Grid implementation of Prometheus includes an Alert Manager. You can configure Alert Manager to notify you when certain events occur.</p></li><li><p><a href=https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid/1.4/vmware-tanzu-kubernetes-grid-14/GUID-packages-monitoring.html><strong>Grafana</strong></a> - Provides monitoring dashboards for displaying key health metrics of Kubernetes clusters. Tanzu Kubernetes Grid includes an implementation of Grafana.</p></li><li><p><a href=https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid/1.4/vmware-tanzu-kubernetes-grid-14/GUID-packages-harbor-registry.html><strong>Harbor Image Registry</strong></a> - Provides a centralized location to push, pull, store, and scan container images used in Kubernetes workloads. It supports storing artifacts and includes enterprise-grade features such as RBAC, retention policies, automated garbage clean up, and docker hub proxying.</p></li><li><p><strong>Multus CNI</strong> - Enables attaching multiple network interfaces to pods. Multus CNI is a container network interface (CNI) plugin for Kubernetes that lets you attach multiple network interfaces to a single pod and associate each with a different address range.</p></li></ul></li><li><p><strong>Jumpbox/Bootstrap Machine -</strong> The bootstrap machine is the machine on which you run the Tanzu CLI and other utilities such as Kubectl, Kind, etc. This is where the initial bootstrapping of a management cluster occurs before it is pushed to the platform where it will run. The installation binaries (including tanzu packages) for TKG installation are made available in iso/tarball format on this machine. This machine should have access to the infrastructure components such as the vCenter server and the components that will be deployed during the installation of Tanzu Kubernetes Grid. This machine should have a browser installed to access the UI of the components described above.</p></li><li><p><strong>Local Image Registry -</strong> An image registry provides a location for pushing, pulling, storing, and scanning container images used in the Tanzu Kubernetes Grid environment. The image registry is also used for day 2 operations of the Tanzu Kubernetes clusters. Typical day-2 operations include tasks such as storing application images, upgrading Tanzu Kubernetes clusters, etc.</p></li></ul><p>In an airgap environment, there are a couple of possible solutions for using an image registry as explained below:</p><ul><li><p><strong>Existing Image Registry -</strong> An image registry pre-existing in the environment with a project created for storing TKG binaries and the bootstrap machine has access to this registry. The operator will untar the tarball present at the bootstrap machine and push the TKG binaries to the TKG project using the script present in the tarball. This registry can be a <a href=https://goharbor.io/>Harbor</a> registry or any other container registry solution.</p></li><li><p><strong>New Image Registry -</strong> If an image registry solution doesn’t exist in the environment, a new registry instance can be deployed. The easiest way of creating a new image registry instance is installing Harbor using the docker-compose method and then pushing the TKG binaries to the appropriate project.</p></li></ul><h2 id=tkg-bill-of-materials><strong>TKG Bill Of Materials</strong></h2><p>Below is the validated Bill of Materials that can be used to install TKG on your vSphere environment today:</p><table><thead><tr><th style=text-align:left><strong>Software Components</strong></th><th style=text-align:left><strong>Version</strong></th></tr></thead><tbody><tr><td style=text-align:left>Tanzu Kubernetes Grid</td><td style=text-align:left>1.5.4</td></tr><tr><td style=text-align:left>VMware vSphere ESXi</td><td style=text-align:left>7.0 U2 and later</td></tr><tr><td style=text-align:left>VMware vCenter (VCSA)</td><td style=text-align:left>7.0 U2 and later</td></tr><tr><td style=text-align:left>NSX Advanced Load Balancer</td><td style=text-align:left>20.1.7</td></tr></tbody></table><p>The Interoperability Matrix can be verified at all times <a href="https://interopmatrix.vmware.com/Interoperability?col=551,&row=1,%262,%26789,">here</a>.</p><h2 id=tanzu-kubernetes-grid-components><strong>Tanzu Kubernetes Grid Components</strong></h2><p>VMware Tanzu Kubernetes Grid (TKG) provides organizations with a consistent, upstream-compatible, regional Kubernetes substrate that is ready for end-user workloads and ecosystem integrations. You can deploy Tanzu Kubernetes Grid across software-defined datacenters (SDDC) and public cloud environments, including vSphere, Microsoft Azure, and Amazon EC2.</p><p>Tanzu Kubernetes Grid comprises the following components:</p><p><strong>Management Cluster -</strong> A management cluster is the first element you deploy when creating a Tanzu Kubernetes Grid instance. The management cluster is a Kubernetes cluster that performs the primary management and operational center role for the Tanzu Kubernetes Grid instance. The management cluster is purpose-built for operating the platform and managing the lifecycle of Tanzu Kubernetes clusters.</p><p><strong>Cluster API -</strong> TKG functions through the creation of a Management Kubernetes cluster that houses <a href=https://cluster-api.sigs.k8s.io/>Cluster API</a>. The Cluster API then interacts with the infrastructure provider to service workload Kubernetes cluster lifecycle requests.</p><p><strong>Tanzu Kubernetes Cluster -</strong> Tanzu Kubernetes clusters are the Kubernetes clusters in which your application workloads run. These clusters are also referred to as workload clusters. Tanzu Kubernetes clusters can run different versions of Kubernetes, depending on the needs of the applications they run.</p><p><strong>Shared Service Cluster -</strong> Each Tanzu Kubernetes Grid instance can only have one shared services cluster. You will deploy this cluster only if you intend to deploy shared services such as Contour and Harbor.</p><p><strong>Tanzu Kubernetes Cluster Plans -</strong> A cluster plan is a blueprint that describes the configuration with which to deploy a Tanzu Kubernetes cluster. It provides a set of configurable values that describe settings like the number of control plane machines, worker machines, VM types, and so on.</p><p>This current release of Tanzu Kubernetes Grid provides two default templates, dev, and prod. You can create and use custom plans to meet your requirements.</p><p><strong>Tanzu Kubernetes Grid Instance -</strong> A Tanzu Kubernetes Grid instance is the full deployment of Tanzu Kubernetes Grid, including the management cluster, the workload clusters, and the shared services cluster that you configure.</p><p><strong>Tanzu CLI -</strong> A command-line utility that provides the necessary commands to build and operate Tanzu management and tanzu Kubernetes clusters.</p><p><strong>Bootstrap Machine -</strong> The bootstrap machine is the laptop, host, or server on which you download and run the Tanzu CLI. This is where the initial bootstrapping of a management cluster occurs before it is pushed to the platform where it will run. This machine also houses an instance of the harbor where all required TKG installation binaries are pushed.</p><p><strong>Carvel Tools</strong> <strong>-</strong> Carvel is an open-source suite of tools. Carvel provides a set of reliable, single-purpose, composable tools that aid in your application building, configuration, and deployment to Kubernetes. Tanzu Kubernetes Grid uses the following tools from the Carvel open-source project:</p><ul><li><strong>ytt -</strong> a command-line tool for templating and patching YAML files. You can also use ytt to collect fragments and piles of YAML into modular chunks for easy re-use.</li><li><strong>kapp -</strong> the application deployment CLI for Kubernetes. It allows you to install, upgrade, and delete multiple Kubernetes resources as one application.</li><li><strong>kbld -</strong> an image-building and resolution tool.</li><li><strong>imgpkg -</strong> a tool that enables Kubernetes to store configurations and the associated container images as OCI images, and to transfer these images.</li></ul><p><strong>Tanzu Kubernetes Grid Installer -</strong> The Tanzu Kubernetes Grid installer is a CLI/graphical wizard that you launch by running the <strong>tanzu management-cluster create</strong> command. The installer wizard runs locally on the bootstrap machine and provides the user an option of deploying a management cluster.</p><h2 id=tanzu-kubernetes-grid-storage><strong>Tanzu Kubernetes Grid Storage</strong></h2><p>Tanzu Kubernetes Grid integrates with shared datastores available in the vSphere infrastructure. The following types of shared datastores are supported:</p><ul><li>vSAN</li><li>VMFS</li><li>NFS</li><li>vVols</li></ul><p>TKG uses storage policies to integrate with shared datastores. The policies represent datastores and manage the storage placement of such objects as control plane VMs, container images, and persistent storage volumes.</p><p>TKG Cluster Plans can be defined by operators to use a certain vSphere Datastore when creating new workload clusters. All developers would then have the ability to provision container-backed persistent volumes from that underlying datastore.</p><p>Tanzu Kubernetes Grid is agnostic about which option you choose. For Kubernetes stateful workloads, TKG installs the <a href=https://github.com/container-storage-interface/spec/blob/master/spec.md>vSphere Container Storage interface (vSphere CSI)</a> to automatically provision Kubernetes persistent volumes for pods.</p><p><a href=https://docs.vmware.com/en/VMware-vSAN/index.html>VMware vSAN</a> is a recommended storage solution for deploying Tanzu Kubernetes Grid clusters on vSphere.</p><table><thead><tr><th style=text-align:left><strong>Decision ID</strong></th><th style=text-align:left><strong>Design Decision</strong></th><th style=text-align:left><strong>Design Justification</strong></th><th style=text-align:left><strong>Design Implications</strong></th></tr></thead><tbody><tr><td style=text-align:left>TKO-STG-001</td><td style=text-align:left>Use vSAN storage for TKG</td><td style=text-align:left>vSAN supports NFS volumes in ReadWriteMany access modes.</td><td style=text-align:left>vSAN File Services need to be configured to leverage this.</td></tr><tr><td style=text-align:left>TKO-STG-002</td><td style=text-align:left>Use vSAN storage for TKG</td><td style=text-align:left>By using vSAN as the shared storage solution, you can take advantage of more cost-effective local storage.</td><td style=text-align:left>Minimizes storage platform complexity by standardizing on a single type.</td></tr></tbody></table><p><strong>Note:</strong> vSAN File Service is available only in vSAN Enterprise and Enterprise Plus editions</p><p>While the default vSAN storage policy can be used, administrators should evaluate the needs of their applications and craft a specific <a href=https://docs.vmware.com/en/VMware-vSphere/6.7/com.vmware.vsphere.storage.doc/GUID-89091D59-D844-46B2-94C2-35A3961D23E7.html>vSphere Storage Policy</a>. vSAN storage policies describe classes of storage (e.g. SSD, NVME, etc.) along with quotas for your clusters.</p><p><img src=reference-designs/tkg-nsxt-airgap-ra.md.old/img/tkg-nsxt-airgap01.png alt="VMware vSAN Storage"></p><h2 id=tanzu-kubernetes-clusters-networking><strong>Tanzu Kubernetes Clusters Networking</strong></h2><p>A Tanzu Kubernetes cluster provisioned by the Tanzu Kubernetes Grid supports two Container Network Interface (CNI) options:</p><ul><li><a href=https://antrea.io/>Antrea</a></li><li><a href=https://www.tigera.io/project-calico/>Calico</a></li></ul><p>Both are open-source software that provides networking for cluster pods, services, and ingress.</p><p>When you deploy a Tanzu Kubernetes cluster using Tanzu CLI, Antrea CNI is automatically enabled in the cluster.</p><p>To provision a Tanzu Kubernetes cluster using a non-default CNI, please see <a href=https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid/1.5/vmware-tanzu-kubernetes-grid-15/GUID-tanzu-k8s-clusters-networking.html#calico>Deploy Tanzu Kubernetes clusters with calico</a></p><p>Each CNI is suitable for a different use case. The below table lists some common use cases for the three CNI’s that Tanzu Kubernetes Grid supports. This table will help you with information on selecting the right CNI in your Tanzu Kubernetes Grid implementation.</p><table><thead><tr><th style=text-align:left><strong>CNI</strong></th><th style=text-align:left><strong>Use Case</strong></th><th style=text-align:left><strong>Pros and Cons</strong></th></tr></thead><tbody><tr><td style=text-align:left>Antrea</td><td style=text-align:left>Enable Kubernetes pod networking with IP overlay networks using VXLAN or Geneve for encapsulation. Optionally encrypt node-to-node communication using IPSec packet encryption.Antrea supports advanced network use cases like kernel bypass and network service mesh.</td><td style=text-align:left>Pros- Provide an option to Configure Egress IP Pool or Static Egress IP for the Kubernetes Workloads.</td></tr><tr><td style=text-align:left>Calico</td><td style=text-align:left>Calico is used in environments where factors like network performance, flexibility, and power are essential.For routing packets between nodes, Calico leverages the BGP routing protocol instead of an overlay network. This eliminates the need to wrap packets with an encapsulation layer resulting in increased network performance for Kubernetes workloads.</td><td style=text-align:left>Pros:- Support for Network Policies.- High network performance.- SCTP Support.Cons:- No multicast support.</td></tr></tbody></table><h2 id=tanzu-kubernetes-grid-infrastructure-networking><strong>Tanzu Kubernetes Grid Infrastructure Networking</strong></h2><p>Tanzu Kubernetes Grid on vSphere can be deployed on the following networking stacks:</p><ul><li>VMware NSX-T Data Center Networking.</li><li>vSphere Networking (VDS).</li></ul><p><strong>Note:</strong> The scope of this document is limited to NSX-T Data Center Networking.</p><h2 id=tkg-on-nsx-t-networking-with-nsx-advanced-load-balancer><strong>TKG on NSX-T Networking with NSX Advanced Load Balancer</strong></h2><p>Tanzu Kubernetes Grid, when deployed on the VMware NSX-T Networking, uses the NSX-T logical segments & Tier-1 gateways to provide connectivity to Kubernetes control plane VMs, worker nodes, services, and applications. All hosts from the cluster where Tanzu Kubernetes clusters are deployed are configured as NSX-T Transport nodes that provide network connectivity to the Kubernetes environment.</p><p>Tanzu Kubernetes Grid leverages NSX Advanced Load Balancer to provide L4 load balancing for the Tanzu Kubernetes Clusters Control-Plane HA and L4/L7 for the applications deployed in the Tanzu Kubernetes clusters. Users access the applications by connecting to the Virtual IP address (VIP) of the applications provisioned by NSX Advanced Load Balancer.</p><h1 id=nsx-advanced-load-balancer-components><strong>NSX Advanced Load Balancer Components</strong></h1><p>NSX Advanced Load Balancer is deployed in Write Access Mode mode in the vSphere Environment. This mode grants NSX Advanced Load Balancer Controllers full write access to the vCenter which helps in automatically creating, modifying, and removing Service Engines and other resources as needed to adapt to changing traffic needs. The following are the core components of NSX Advanced Load Balancer:</p><ul><li><p><strong>NSX Advanced Load Balancer Controller</strong> - NSX Advanced Load Balancer Controller manages Virtual Service objects and interacts with the vCenter Server infrastructure to manage the lifecycle of the service engines (SEs). It is the central repository for the configurations and policies related to services and management and provides the portal for viewing the health of VirtualServices and SEs and the associated analytics that NSX Advanced Load Balancer provides.</p></li><li><p><strong>NSX Advanced Load Balancer Service Engine</strong> - The Service Engines (SEs) are lightweight VMs that handle all data plane operations by receiving and executing instructions from the controller. The SEs perform load balancing and all client and server-facing network interactions.</p></li><li><p><strong>Avi Kubernetes Operator (AKO)</strong> - It is a Kubernetes operator that runs as a pod in the Management and Tanzu Kubernetes clusters and provides ingress and load balancing functionality. AKO translates the required Kubernetes objects to NSX Advanced Load Balancer objects and automates the implementation of ingresses/routes/services on the Service Engines (SE) via the NSX Advanced Load Balancer Controller.</p></li><li><p><strong>AKO Operator (AKOO)</strong> - This is an operator which is used to deploy, manage and remove AKO Pod in Kubernetes clusters. This operator when deployed creates an instance of the AKO controller and installs all the relevant objects like</p><ul><li>AKO <code>StatefulSet</code></li><li><code>ClusterRole</code> and <code>ClusterRoleBinding</code></li><li><code>ConfigMap</code> required for the AKO controller and other artifacts.</li></ul></li></ul><p>TKG management clusters have an ako-operator installed out of the box during cluster deployment. By default, a TKG management cluster has a couple of <code>AkoDeploymentConfig</code> created which dictates when and how ako pods are created in the workload clusters. For more information on ako-operator, please see the official <a href=https://github.com/vmware/load-balancer-and-ingress-services-for-kubernetes/tree/master/ako-operator>documentation</a>.</p><p>Each environment configured in NSX Advanced Load Balancer is referred to as Cloud. Each cloud in NSX Advanced Load Balancer maintains networking and NSX Advanced Load Balancer Service Engine settings. The cloud is configured with one or more VIP networks to provide IP addresses to load balancing (L4/L7) virtual services created under that cloud.</p><p>The virtual services can be spanned across multiple Service Engines if the associated Service Engine Group is configured in Active/Active HA mode. A Service Engine can belong to only one Service Engine group at a time.</p><p>IP address allocation for virtual services can be over DHCP or via NSX Advanced Load Balancer in-built IPAM functionality. The VIP networks created/configured in NSX Advanced Load Balancer are associated with the IPAM profile.</p><h1 id=network-architecture><strong>Network Architecture</strong></h1><p>For deployment of TKG in the vSphere environment, we build separate networks (port groups) for the TKG management cluster, TKG workload clusters, NSX Advanced Load Balancer management, and Cluster VIP network for Control plane HA, TKG Mgmt VIP/Data network, and TKG workload Data/VIP Network.</p><p>The network reference design can be mapped into this general framework.</p><p><img src=reference-designs/tkg-nsxt-airgap-ra.md.old/img/tkg-nsxt-airgap02.png alt="TKG General Network Layout"></p><p>This topology enables the following benefits:</p><ul><li><p>Isolate and separate SDDC management components (vCenter, ESX) from the TKG components. This reference design only allows the minimum connectivity between the TKG clusters and NSX Advanced Load Balancer to the vCenter Server.</p></li><li><p>Isolate and separate the NSX Advanced Load Balancer management network from the TKG management segment and the TKG workload segments.</p></li><li><p>Depending on the workload cluster type and use case, multiple workload clusters may leverage the same workload network or new networks can be used for each workload cluster.
To isolate and separate TKG workload cluster networking from each other it’s recommended to make use of separate networks for each workload cluster and configure the required firewall between these networks. Refer to <a href=#firewall-requirements>Firewall Recommendations</a> for more details.</p></li><li><p>Separate provider and tenant access to the TKG environment.</p><ul><li><p>Only provider administrators need access to the TKG management cluster. This prevents tenants from attempting to connect to the TKG management cluster.</p></li><li><p>Only allow tenants to access their TKG workload cluster(s) and restrict access to this cluster from other tenants.</p></li></ul></li></ul><h2 id=network-requirements><strong>Network Requirements</strong></h2><p>As per the defined architecture, below is the list of needed minimum networks:</p><table><thead><tr><th style=text-align:left><strong>Network Type</strong></th><th style=text-align:left><strong>DHCP Service</strong></th><th style=text-align:left><strong>Description & Recommendations</strong></th></tr></thead><tbody><tr><td style=text-align:left>NSX Advanced Load Balancer Management Network</td><td style=text-align:left>Optional</td><td style=text-align:left>NSX Advanced Load Balancer controllers and SEs will be attached to this network. DHCP is not a mandatory requirement on this network as NSX Advanced Load Balancer can take care of IPAM.</td></tr><tr><td style=text-align:left>TKG Management Network</td><td style=text-align:left>Yes</td><td style=text-align:left>Control plane and worker nodes of the TKG Management Cluster will be attached to this network.</td></tr><tr><td style=text-align:left>TKG Shared Services Network</td><td style=text-align:left>Yes</td><td style=text-align:left>Control plane and worker nodes of the TKG Shared Service Cluster will be attached to this network.</td></tr><tr><td style=text-align:left>TKG Workload Network</td><td style=text-align:left>Yes</td><td style=text-align:left>Control plane and worker nodes of TKG Workload Clusters will be attached to this network.</td></tr><tr><td style=text-align:left>TKG Cluster VIP/Data Network</td><td style=text-align:left>No</td><td style=text-align:left>Virtual services for Control plane HA of all TKG clusters (Management, Shared service, and Workload)Reserve sufficient IPs depending on the number of TKG clusters planned to be deployed in the environment, NSX Advanced Load Balancer takes care of IPAM on this network.</td></tr><tr><td style=text-align:left>TKG Management VIP/Data Network</td><td style=text-align:left>No</td><td style=text-align:left>Virtual services for all user-managed packages (such as Contour and Harbor) hosted on the Shared service cluster.</td></tr><tr><td style=text-align:left>TKG Workload VIP/Data Network</td><td style=text-align:left>No</td><td style=text-align:left>Virtual services for all applications hosted on the Workload clusters.Reserve sufficient IPs depending on the number of applications that are planned to be hosted on the Workload clusters along with scalability considerations.</td></tr></tbody></table><h2 id=subnet-and-cidr-examples><strong>Subnet and CIDR Examples</strong></h2><p>For the purpose of this demonstration, this document makes use of the following CIDR for TKG deployment.</p><table><thead><tr><th style=text-align:left><strong>Network Type</strong></th><th style=text-align:left><strong>Segment Name</strong></th><th style=text-align:left><strong>Gateway CIDR</strong></th><th style=text-align:left><strong>DHCP Pool in NSXT</strong></th><th style=text-align:left><strong>NSX ALB IP Pool</strong></th></tr></thead><tbody><tr><td style=text-align:left>NSX ALB Mgmt Network</td><td style=text-align:left>nsx-alb-mgmt-segment</td><td style=text-align:left>172.19.10.1/27</td><td style=text-align:left>N/A</td><td style=text-align:left>172.19.10.6- 172.19.10.30</td></tr><tr><td style=text-align:left>TKG Management Network</td><td style=text-align:left>tkg-mgmt-segment</td><td style=text-align:left>172.19.11.1/27</td><td style=text-align:left>172.19.11.2- 172.19.11.30</td><td style=text-align:left>N/A</td></tr><tr><td style=text-align:left>TKG Shared Service Network</td><td style=text-align:left>tkg-ss-segment</td><td style=text-align:left>172.19.12.1/27</td><td style=text-align:left>172.19.12.2 - 172.19.12.30</td><td style=text-align:left>N/A</td></tr><tr><td style=text-align:left>TKG Mgmt VIP Network</td><td style=text-align:left>tkg-mgmt-vip-segment</td><td style=text-align:left>172.19.13.1/26</td><td style=text-align:left>N/A</td><td style=text-align:left>172.19.13.2- 172.19.13.60</td></tr><tr><td style=text-align:left>TKG Cluster VIP Network</td><td style=text-align:left>tkg-cluster-vip-segment</td><td style=text-align:left>172.19.14.1/26</td><td style=text-align:left>N/A</td><td style=text-align:left>172.19.14.2- 172.19.14.60</td></tr><tr><td style=text-align:left>TKG Workload VIP Network</td><td style=text-align:left>tkg-workload-vip-segment</td><td style=text-align:left>172.19.15.1/26</td><td style=text-align:left>N/A</td><td style=text-align:left>172.19.15.2- 172.19.15.60</td></tr><tr><td style=text-align:left>TKG Workload Network</td><td style=text-align:left>tkg-workload-segment</td><td style=text-align:left>172.19.16.1/24</td><td style=text-align:left>172.19.16.2- 172.19.16.251</td><td style=text-align:left>N/A</td></tr></tbody></table><h2 id=a-idfirewall-requirements-a-firewall-requirements><strong>Firewall Requirements</strong></h2><p>To prepare the firewall, you need to gather the following:</p><ol><li>NSX ALB Controller nodes and Cluster IP address.</li><li>NSX ALB Management Network CIDR</li><li>TKG Management Network CIDR.</li><li>TKG Workload Network CIDR.</li><li>TKG Cluster VIP Range.</li><li>TKG Management VIP Range.</li><li>TKG Workload VIP Range.</li><li>Bastion host IP address.</li><li>Bootstrap machine IP address.</li><li>VMware Harbor registry IP</li><li>vCenter Server IP</li><li>DNS server IP(s)</li><li>NTP Server IP(s)</li><li>DHCP Server IP(s)</li></ol><p>The below table provides a list of firewall rules based on the assumption that there is no firewall within a subnet/VLAN.</p><table><thead><tr><th style=text-align:left><strong>Source</strong></th><th style=text-align:left><strong>Destination</strong></th><th style=text-align:left><strong>Protocol:Port</strong></th><th style=text-align:left><strong>Description</strong></th></tr></thead><tbody><tr><td style=text-align:left>Bastion HostBootstrap machine</td><td style=text-align:left>NSX Advanced Load Balancer Controller nodes and Cluster IP Address.</td><td style=text-align:left>TCP:443</td><td style=text-align:left>To access NSX Advanced Load Balancer portal for configuration.</td></tr><tr><td style=text-align:left>Bastion Host</td><td style=text-align:left>vCenter Server</td><td style=text-align:left>TCP:443</td><td style=text-align:left>To create resource pools, VM folders, etc, in vCenter.</td></tr><tr><td style=text-align:left>Bastion Host</td><td style=text-align:left>Bootstrap VM</td><td style=text-align:left>TCP:22TCP:443</td><td style=text-align:left>Access bootstrap vm to run tkg installation and configuration commands.To access harbor UI.</td></tr><tr><td style=text-align:left>TKG Management Network CIDR.TKG Shared-Services Network CIDRTKG Workload Network CIDR.</td><td style=text-align:left>DNS ServerNTP ServerDHCP Server</td><td style=text-align:left>UDP:53UDP:123UDP: 67, 68</td><td style=text-align:left>DNS Service. Time Synchronization.Allows TKG nodes to get DHCP addresses.</td></tr><tr><td style=text-align:left>TKG Management Network CIDR.TKG Shared-Services Network CIDR.TKG Workload Network CIDR.</td><td style=text-align:left>vCenter IPInternal Harbor Registry</td><td style=text-align:left>TCP:443TCP:443</td><td style=text-align:left>Allows components to access vCenter to create VMs and Storage Volumes.Allows components to retrieve container images.</td></tr><tr><td style=text-align:left>TKG Management Network CIDR.TKG Shared-Services Network CIDR.TKG Workload Network CIDR.</td><td style=text-align:left>Internal Harbor Registry</td><td style=text-align:left>TCP:443</td><td style=text-align:left>Allows components to retrieve container images.</td></tr><tr><td style=text-align:left>TKG Management Network CIDR.TKG Shared-Services Network CIDR.TKG Workload Network CIDR.</td><td style=text-align:left>TKG Cluster VIP Range.</td><td style=text-align:left>TCP:6443</td><td style=text-align:left>For Management cluster to configure Shared service and Workload Cluster.Allow shared services and workload clusters to register with the management cluster</td></tr><tr><td style=text-align:left>TKG Management Network CIDR.TKG Shared-Services Network CIDR.TKG Workload Network CIDR.</td><td style=text-align:left>NSX Advanced Load Balancer Controllers and Cluster IP Address.</td><td style=text-align:left>TCP:443</td><td style=text-align:left>Allow Avi Kubernetes Operator (AKO) and AKO Operator (AKOO) access to Avi Controller</td></tr><tr><td style=text-align:left>NSX Advanced Load Balancer Controllers.</td><td style=text-align:left>vCenter and ESXi Host</td><td style=text-align:left>TCP:443</td><td style=text-align:left>Allow NSX Advanced Load Balancer to discover vCenter objects and deploy SEs as required</td></tr><tr><td style=text-align:left>NSX Advanced Load Balancer Management Network CIDR.</td><td style=text-align:left>DNS ServerNTP Server</td><td style=text-align:left>UDP:53UDP:123</td><td style=text-align:left>DNS ServiceTime Synchronization</td></tr></tbody></table><p><strong>Installation Experience</strong></p><p>TKG management cluster is the first component that you deploy to get started with Tanzu Kubernetes Grid.</p><p>You can deploy the Management cluster in two ways:</p><ul><li><p>Run the Tanzu Kubernetes Grid installer, a wizard interface that guides you through the process of deploying a management cluster.</p></li><li><p>Create and edit YAML configuration files, and use them to deploy a management cluster with the CLI commands.</p></li></ul><p>The TKG Installation user interface shows that, in the current version, it is possible to install TKG on VMware vSphere, AWS, and Microsoft Azure. The UI provides a guided experience tailored to the IaaS, in this case on VMware vSphere backed by NSX-T Data Center networking.</p><p><img src=reference-designs/tkg-nsxt-airgap-ra.md.old/img/tkg-nsxt-airgap03.png alt="TKG Supported IaaS Platforms"></p><p>The installation process will take you through the setup of a <strong>Management Cluster</strong> on your vSphere environment. Once the management cluster is deployed you can make use of Tanzu CLI to deploy Tanzu Kubernetes Shared Service and workload clusters.</p><p>To deploy TKG Management Cluster directly from CLI, please see Supplemental information <a href=#deployment-parameters>Cluster Deployment Parameters</a> for a sample yaml file used for deployment.</p><h1 id=kubernetes-ingress-routing>Kubernetes Ingress Routing</h1><p>Default installation of Tanzu Kubernetes Grid does not have any default ingress controller deployed. Users can use Contour (available for installation through Tanzu Packages) or any Third-party ingress controller of their choice.</p><p>Contour is an open-source controller for Kubernetes Ingress routing. Contour can be installed in the Shared Services cluster on any Tanzu Kubernetes Cluster. Deploying Contour is a prerequisite if you want to deploy the Prometheus, Grafana, and Harbor Packages on a workload cluster.</p><p>For more information about Contour, see the <a href=https://projectcontour.io/>Contour</a> site and <a href=https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid/1.5/vmware-tanzu-kubernetes-grid-15/GUID-packages-ingress-contour.html>Implementing Ingress Control with Contour</a>.</p><p>Another option is to use the NSX Advanced Load Balancer Kubernetes ingress controller which offers an advanced L7 ingress for containerized applications that are deployed in the Tanzu Kubernetes workload cluster.</p><p><img src=reference-designs/tkg-nsxt-airgap-ra.md.old/img/tkg-nsxt-airgap04.png alt="NSX ALB Ingress Capabilities"></p><p>For more information about the NSX Advanced Load Balancer ingress controller, please see the <a href=https://avinetworks.com/docs/ako/1.5/avi-kubernetes-operator/>official documentation</a>.</p><p>Each ingress controller has pros and cons of its own. The below table provides general recommendations on when you should use a specific ingress controller for your Kubernetes environment.</p><table><thead><tr><th style=text-align:left><strong>Ingress Controller</strong></th><th style=text-align:center><strong>Use Cases</strong></th></tr></thead><tbody><tr><td style=text-align:left>Contour</td><td style=text-align:center>Use contour when only north-south traffic is needed in a Kubernetes cluster. You can apply security policies for north-south traffic by defining the policies in the applications manifest file.It&rsquo;s a reliable solution for simple Kubernetes workloads.</td></tr><tr><td style=text-align:left>NSX Advanced Load Balancer Ingress controller</td><td style=text-align:center>Use NSX Advanced Load Balancer ingress controller when a containerized application requires features like local and global server load balancing (GSLB), web application firewall (WAF), performance monitoring, direct routing from LB to pod, etc.</td></tr></tbody></table><h1 id=tanzu-kubernetes-grid-logging>Tanzu Kubernetes Grid Logging</h1><p>Fluent Bit is a lightweight log processor and forwarder that allows you to collect data and logs from different sources, unify them, and send them to multiple destinations. Tanzu Kubernetes Grid includes signed binaries for Fluent Bit, that you can deploy on management clusters and on Tanzu Kubernetes clusters to provide a log-forwarding service.</p><p><a href=https://fluentbit.io/>Fluent Bit</a> integrates with logging platforms such as <strong>vRealize LogInsight, Elasticsearch, Kafka, Splunk, or an HTTP endpoint</strong>. Details on configuring Fluent Bit to your logging provider can be found in the documentation <a href=https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid/1.5/vmware-tanzu-kubernetes-grid-15/GUID-packages-logging-fluentbit.html>here</a>.</p><p>You can deploy Fluent Bit on any management cluster or Tanzu Kubernetes clusters from which you want to collect logs. First, you configure an output plugin on the cluster from which you want to gather logs, depending on the endpoint that you use. Then, you deploy Fluent Bit on the cluster as a package.</p><p>vRealize Log Insight (vRLI) provides real-time log management and log analysis with machine learning-based intelligent grouping, high-performance searching, and troubleshooting across physical, virtual, and cloud environments. vRLI already has a deep integration with the vSphere platform where you can get key actionable insights.</p><h1 id=tanzu-kubernetes-grid-monitoring>Tanzu Kubernetes Grid Monitoring</h1><p>In an airgap environment, monitoring for the Tanzu Kubernetes clusters is provided via <a href=https://prometheus.io/>Prometheus</a> and <a href=https://grafana.com/>Grafana</a>.</p><ul><li>Prometheus is an open-source system monitoring and alerting toolkit. It can collect metrics from target clusters at specified intervals, evaluate rule expressions, display the results, and trigger alerts if certain conditions arise. The Tanzu Kubernetes Grid implementation of Prometheus includes <strong>Alert Manager</strong>, which you can configure to notify you when certain events occur.</li><li>Grafana is open-source visualization and analytics software. It allows you to query, visualize, alert on, and explore your metrics no matter where they are stored.</li></ul><p>Both Prometheus and Grafana are installed via user-managed Tanzu packages by creating the deployment manifests and invoking the kubectl command to deploy the packages in the Tanzu Kubernetes clusters.</p><p>The following diagram shows how the monitoring components on a cluster interact.</p><p><img src=reference-designs/tkg-nsxt-airgap-ra.md.old/img/tkg-nsxt-airgap05.png alt="TKG Monitoring via Prometheus and Grafana"></p><p>You can use out-of-the-box Kubernetes dashboards or can create new dashboards to monitor compute/network/storage utilization of Kubernetes objects such as Clusters, Namespaces, Pods, etc.</p><p>Please see the sample dashboards shown below:</p><p><strong>Namespace (Pods) Compute Resources Utilization Dashboard</strong></p><p><img src=reference-designs/tkg-nsxt-airgap-ra.md.old/img/tkg-nsxt-airgap06.png alt="Namespace Resource Utilization Dashboard"></p><p><strong>Namespace (Pods) Networking Utilization Dashboard</strong></p><p><img src=reference-designs/tkg-nsxt-airgap-ra.md.old/img/tkg-nsxt-airgap07.png alt="Namespace Network Utilization Dashboard"></p><p><strong>API Server Availability Dashboard</strong></p><p><img src=reference-designs/tkg-nsxt-airgap-ra.md.old/img/tkg-nsxt-airgap08.png alt="API Server Availability Dashboard"></p><p><strong>Cluster Compute Resources Utilization Dashboard</strong></p><p><img src=reference-designs/tkg-nsxt-airgap-ra.md.old/img/tkg-nsxt-airgap09.png alt="Cluster Compute Resources Utilization Dashboard"></p><h1 id=design-recommendations><strong>Design Recommendations</strong></h1><h2 id=nsx-advanced-load-balancer-recommendations>NSX Advanced Load Balancer Recommendations</h2><p>The below table provides the recommendations for configuring NSX Advanced Load Balancer in a Tanzu Kubernetes Grid environment.</p><table><thead><tr><th style=text-align:left><strong>Decision ID</strong></th><th style=text-align:left><strong>Design Decision</strong></th><th style=text-align:left><strong>Design Justification</strong></th><th style=text-align:left><strong>Design Implications</strong></th></tr></thead><tbody><tr><td style=text-align:left>TKO-ALB-001</td><td style=text-align:left>Deploy NSX Advanced Load Balancer controller cluster nodes on a network dedicated to NSX-Advanced Load Balancer</td><td style=text-align:left>Isolate NSX Advanced Load Balancer traffic from infrastructure management traffic and Kubernetes workloads.</td><td style=text-align:left>Using the same network for NSX Advanced Load Balancer Controller Cluster nodes allows for configuring a floating cluster IP address that will be assigned to the cluster leader.</td></tr><tr><td style=text-align:left>TKO-ALB-002</td><td style=text-align:left>Deploy 3 NSX Advanced Load Balancer controllers nodes.</td><td style=text-align:left>To achieve high availability for the NSX Advanced Load Balancer platform.</td><td style=text-align:left>In clustered mode, NSX Advanced Load Balancer availability is not impacted by an individual controller node failure. The failed node can be removed from the cluster and redeployed if recovery is not possible.</td></tr><tr><td style=text-align:left>TKO-ALB-003</td><td style=text-align:left>Use static IPs for the NSX Advanced Load Balancer controllers if DHCP cannot guarantee a permanent lease.</td><td style=text-align:left>NSX Advanced Load Balancer Controller cluster uses management IPs to form and maintain quorum for the control plane cluster. Any changes would be disruptive.</td><td style=text-align:left>NSX Advanced Load Balancer Controller control plane might go down if the management IPs of the controller node changes.</td></tr><tr><td style=text-align:left>TKO-ALB-004</td><td style=text-align:left>Use NSX Advanced Load Balancer IPAM for Service Engine data network and virtual services.</td><td style=text-align:left>Guarantees IP address assignment for Service Engine Data NICs and Virtual Services.</td><td style=text-align:left>Remove the corner case scenario when the DHCP server runs out of the lease or is down.</td></tr><tr><td style=text-align:left>TKO-ALB-005</td><td style=text-align:left>Reserve an IP in the NSX Advanced Load Balancer management subnet to be used as the Cluster IP for the Controller Cluster.</td><td style=text-align:left>NSX Advanced Load Balancer portal is always accessible over Cluster IP regardless of a specific individual controller node failure.</td><td style=text-align:left>NSX Advanced Load Balancer administration is not affected by an individual controller node failure.</td></tr><tr><td style=text-align:left>TKO-ALB-006</td><td style=text-align:left>Use separate VIP networks for application load balancing per TKC.</td><td style=text-align:left>Separate dev/test and prod workloads load balancer traffic from each other.</td><td style=text-align:left>This is achieved by creating AkoDeploymentConfig per TKC.</td></tr><tr><td style=text-align:left>TKO-ALB-007</td><td style=text-align:left>Create separate Service Engine Groups for TKG management and workload clusters.</td><td style=text-align:left>This allows isolating load balancing traffic of the management and shared services cluster from workload clusters.</td><td style=text-align:left>Create dedicated Service Engine Groups under the vCenter cloud configured manually.</td></tr><tr><td style=text-align:left>TKO-ALB-008</td><td style=text-align:left>Share Service Engines for the same type of workload (dev/test/prod)clusters.</td><td style=text-align:left>Minimize the licensing cost</td><td style=text-align:left>Each Service Engine contributes to the CPU core capacity associated with a license.Sharing Service Engines can help reduce the licensing cost.</td></tr></tbody></table><h2 id=network-recommendations>Network Recommendations</h2><p>The following are the key network recommendations for a production-grade Tanzu Kubernetes Grid deployment with NSX-T Data Center Networking:</p><table><thead><tr><th style=text-align:left><strong>Decision ID</strong></th><th style=text-align:left><strong>Design Decision</strong></th><th style=text-align:left><strong>Design Justification</strong></th><th style=text-align:left><strong>Design Implications</strong></th></tr></thead><tbody><tr><td style=text-align:left>TKO-NET-001</td><td style=text-align:left>Use separate networks for Management cluster and workload clusters</td><td style=text-align:left>To have a flexible firewall and security policies</td><td style=text-align:left>Sharing the same network for multiple clusters can complicate firewall rules creation.</td></tr><tr><td style=text-align:left>TKO-NET-002</td><td style=text-align:left>Use separate networks for workload clusters based on their usage.</td><td style=text-align:left>Isolate production Kubernetes clusters from dev/test clusters.</td><td style=text-align:left>A separate set of Service Engines can be used for separating dev/test workload clusters from prod clusters.</td></tr><tr><td style=text-align:left>TKO-NET-003</td><td style=text-align:left>Configure DHCP for each TKG Cluster Network</td><td style=text-align:left>Tanzu Kubernetes Grid does not support static IP assignments for Kubernetes VM components</td><td style=text-align:left>IP Pool can be used for the TKG clusters in absence of the DHCP.</td></tr></tbody></table><h2 id=tkg-clusters-recommendations>TKG Clusters Recommendations</h2><table><thead><tr><th style=text-align:left><strong>Decision ID</strong></th><th style=text-align:left><strong>Design Decision</strong></th><th style=text-align:left><strong>Design Justification</strong></th><th style=text-align:left><strong>Design Implications</strong></th></tr></thead><tbody><tr><td style=text-align:left>TKO-TKG-001</td><td style=text-align:left>Deploy TKG Management cluster from CLI</td><td style=text-align:left>UI doesn’t provide an option of specifying an internal registry to use for TKG installation.</td><td style=text-align:left>Additional parameters are required to be passed in the cluster deployment file. Using UI, you can’t pass these additional parameters.</td></tr><tr><td style=text-align:left>TKO-TKG-002</td><td style=text-align:left>Use NSX Advanced Load Balancer as your Control Plane Endpoint Provider and for application load balancing</td><td style=text-align:left>Eliminates the requirement for an external load balancer and additional configuration changes on your Tanzu Kubernetes Grid clusters</td><td style=text-align:left>NSX Advanced Load Balancer is a true SDN solution and offers a flexible deployment model and automated way of scaling load balancer objects when needed.</td></tr><tr><td style=text-align:left>TKO-TKG-003</td><td style=text-align:left>Deploy Tanzu Kubernetes clusters with Prod plan.</td><td style=text-align:left>This deploys multiple control plane nodes and provides High Availability for the control plane.</td><td style=text-align:left>TKG infrastructure is not impacted by single node failure.</td></tr><tr><td style=text-align:left>TKO-TKG-004</td><td style=text-align:left>Enable identity management for Tanzu Kubernetes Grid clusters.</td><td style=text-align:left>To avoid usage of administrator credentials and ensure that required users with the right roles have access to the Tanzu Kubernetes Grid clusters</td><td style=text-align:left>Pinniped package helps with integrating the TKG Management cluster with LDAPS/OIDC Authentication.Workload cluster inherits the authentication configuration from the management cluster</td></tr><tr><td style=text-align:left>TKO-TKG-005</td><td style=text-align:left>Enable Machine Health Checks for TKG clusters</td><td style=text-align:left>vSphere HA and Machine Health Checks interoperably work together to enhance workload resiliency</td><td style=text-align:left>A MachineHealthCheck allows users to define conditions under which Machines within a Cluster should be considered unhealthy. Remediation actions can be taken when MachineHealthCheck has identified a node as unhealthy.</td></tr></tbody></table><h1 id=bring-your-own-images-for-tanzu-kubernetes-grid-deployment>Bring Your Own Images for Tanzu Kubernetes Grid Deployment</h1><p>You can build custom machine images for Tanzu Kubernetes Grid to use as a VM template for the management and Tanzu Kubernetes (workload) cluster nodes that it creates. Each custom machine image packages a base operating system (OS) version and a Kubernetes version, along with any additional customizations, into an image that runs on vSphere, Microsoft Azure infrastructure, and AWS (EC2) environments.</p><p>A custom image must be based on the OS versions that are supported by Tanzu Kubernetes Grid. The table below provides a list of OS’es that are supported for building custom images for TKG.</p><table><thead><tr><th style=text-align:center><strong>vSphere</strong></th><th style=text-align:center><strong>AWS</strong></th><th style=text-align:center><strong>Azure</strong></th></tr></thead><tbody><tr><td style=text-align:center>Ubuntu 20.04Ubuntu 18.04RHEL 7Photon OS 3</td><td style=text-align:center>Ubuntu 20.04Ubuntu 18.04Amazon Linux 2</td><td style=text-align:center>Ubuntu 20.04Ubuntu 18.04</td></tr></tbody></table><p>For additional information on building custom images for TKG, please see the TKG product <a href=https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid/1.5/vmware-tanzu-kubernetes-grid-15/GUID-build-images-index.html>documentation</a></p><ul><li><p><a href=https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid/1.5/vmware-tanzu-kubernetes-grid-15/GUID-build-images-linux.html>Linux Custom Machine Images</a></p></li><li><p><a href=https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid/1.5/vmware-tanzu-kubernetes-grid-15/GUID-build-images-windows.html>Windows Custom Machine Images</a></p></li></ul><h1 id=compliance-and-security>Compliance and Security</h1><p>VMware published Tanzu Kubernetes releases (TKrs), along with compatible versions of Kubernetes and supporting components, uses the latest stable and generally-available update of the OS version that it packages, containing all current CVE and USN fixes, as of the day that the image is built. The image files are signed by VMware and have filenames that contain a unique hash identifier.</p><p>VMware provides FIPS-Capable Kubernetes ova which can be used to deploy FIPS compliant TKG management and workload clusters. TKG core components, such as the Kubelet, Kube-apiserver, Kube-controller manager, Kube-proxy, Kube-scheduler, Kubectl, Etcd, Coredns, Containerd, and Cri-tool are made FIPS compliant by compiling them with the <a href=https://csrc.nist.gov/projects/cryptographic-module-validation-program/certificate/2964>BoringCrypto</a> FIPS modules, an open-source cryptographic library that provides <a href=https://www.nist.gov/standardsgov/compliance-faqs-federal-information-processing-standards-fips>FIPS 140-2</a> approved algorithms.</p><h1 id=supplemental-information>Supplemental Information</h1><h2 id=a-iddeployment-parameters-a-cluster-deployment-parameters>Cluster Deployment Parameters</h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#75715e># NSX Advanced Load Balancer details</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>AVI_CA_DATA_B64</span>: <span style=color:#75715e># NSX Advanced Load Balancer Controller Certificate in base64 encoded format.</span>
</span></span><span style=display:flex><span><span style=color:#f92672>AVI_CLOUD_NAME</span>: <span style=color:#75715e># Name of the cloud that you created in your NSX Advanced Load Balancer deployment.</span>
</span></span><span style=display:flex><span><span style=color:#f92672>AVI_CONTROL_PLANE_HA_PROVIDER</span>: <span style=color:#e6db74>&#34;true/false&#34;</span> <span style=color:#75715e># Set to true to enable NSX Advanced Load Balancer as the control plane API server endpoint</span>
</span></span><span style=display:flex><span><span style=color:#f92672>AVI_CONTROLLER</span>: <span style=color:#75715e># The IP or hostname of the NSX Advanced Load Balancer controller.</span>
</span></span><span style=display:flex><span><span style=color:#f92672>AVI_DATA_NETWORK</span>:  <span style=color:#75715e># The network’s name on which the floating IP subnet or IP Pool is assigned to a load balancer for traffic to applications hosted on workload clusters. This network must be present in the same vCenter Server instance as the Kubernetes network that Tanzu Kubernetes Grid uses</span>
</span></span><span style=display:flex><span><span style=color:#f92672>AVI_DATA_NETWORK_CIDR</span>: <span style=color:#75715e># The CIDR of the subnet to use for the load balancer VIP. This comes from one of the VIP network’s configured subnets.</span>
</span></span><span style=display:flex><span><span style=color:#f92672>AVI_ENABLE</span>: <span style=color:#e6db74>&#34;true/false&#34;</span> <span style=color:#75715e># Set to true or false. Enables NSX Advanced Load Balancer as a load balancer for workloads.</span>
</span></span><span style=display:flex><span><span style=color:#f92672>AVI_LABELS: # Optional labels in the format key</span>: <span style=color:#ae81ff>value. When set, NSX Advanced Load Balancer is enabled only on workload clusters that have this label.</span>
</span></span><span style=display:flex><span><span style=color:#f92672>AVI_MANAGEMENT_CLUSTER_VIP_NETWORK_CIDR</span>: <span style=color:#75715e># The CIDR of the subnet to use for the management cluster and workload cluster’s control plane (if using NSX ALB to provide control plane HA) load balancer VIP.</span>
</span></span><span style=display:flex><span><span style=color:#f92672>AVI_MANAGEMENT_CLUSTER_VIP_NETWORK_NAME</span>: <span style=color:#75715e># The network’s name where you assign a floating IP subnet or IP pool to a load balancer for management cluster and workload cluster control plane (if using NSX ALB to provide control plane HA).</span>
</span></span><span style=display:flex><span><span style=color:#f92672>AVI_PASSWORD</span>: <span style=color:#75715e># Password of the NSX ALB Controller admin user in th base 64 encoded format</span>
</span></span><span style=display:flex><span><span style=color:#f92672>AVI_SERVICE_ENGINE_GROUP</span>: <span style=color:#75715e># Name of the Service Engine Group configured in NSX ALB</span>
</span></span><span style=display:flex><span><span style=color:#f92672>AVI_USERNAME</span>: <span style=color:#ae81ff>admin</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Common Variables</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>CLUSTER_CIDR</span>: <span style=color:#75715e># The CIDR range to use for pods.</span>
</span></span><span style=display:flex><span><span style=color:#f92672>SERVICE_CIDR</span>: <span style=color:#75715e># The CIDR range to use for the Kubernetes services.</span>
</span></span><span style=display:flex><span><span style=color:#f92672>CLUSTER_NAME</span>: <span style=color:#75715e># The name of the TKG Management Cluster that must comply with DNS hostname requirements as outlined in https://datatracker.ietf.org/doc/html/rfc952</span>
</span></span><span style=display:flex><span><span style=color:#f92672>CLUSTER_PLAN</span>: <span style=color:#75715e># Can be set to dev, prod or custom. The dev plan deploys a cluster with a single control plane node. The prod plan deploys a highly available cluster with three control plane nodes.</span>
</span></span><span style=display:flex><span><span style=color:#f92672>ENABLE_AUDIT_LOGGING</span>: <span style=color:#75715e># Audit logging for the Kubernetes API server. The default value is false. To enable audit logging, set the variable to true.</span>
</span></span><span style=display:flex><span><span style=color:#f92672>ENABLE_CEIP_PARTICIPATION</span>: <span style=color:#75715e>#The default value is true. false opts out of the VMware Customer Experience Improvement Program.</span>
</span></span><span style=display:flex><span><span style=color:#f92672>ENABLE_MHC</span>: <span style=color:#e6db74>&#34;true/false&#34;</span> <span style=color:#75715e># When set to true, machine health checks are enabled for management cluster control plane and worker nodes. For more information on machine health checks, please refer https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid/1.5/vmware-tanzu-kubernetes-grid-15/GUID-tanzu-config-reference.html#machine-health-checks-7</span>
</span></span><span style=display:flex><span><span style=color:#f92672>IDENTITY_MANAGEMENT_TYPE</span>: <span style=color:#ae81ff>&lt;none/oidc/ldap&gt;</span> <span style=color:#75715e># Set oidc or ldap when enabling centralized authentication for management cluster access.</span>
</span></span><span style=display:flex><span><span style=color:#f92672>INFRASTRUCTURE_PROVIDER</span>: <span style=color:#75715e># For vSphere platform set this value to vsphere</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Node Configuration</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>OS_ARCH</span>: <span style=color:#ae81ff>amd64</span>
</span></span><span style=display:flex><span><span style=color:#f92672>OS_NAME</span>: <span style=color:#75715e># Defaults to ubuntu for Ubuntu LTS. Can also be photon for Photon OS on vSphere</span>
</span></span><span style=display:flex><span><span style=color:#f92672>OS_VERSION</span>: <span style=color:#e6db74>&#34;3&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Proxy Configuration</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>TKG_HTTP_PROXY_ENABLED</span>: <span style=color:#e6db74>&#34;true/false&#34;</span> <span style=color:#75715e># To send outgoing HTTP(S) traffic from the management cluster to a proxy, for example in an internet-restricted environment, set this to true.</span>
</span></span><span style=display:flex><span><span style=color:#f92672>TKG_IP_FAMILY</span>: <span style=color:#ae81ff>ipv4</span>
</span></span><span style=display:flex><span><span style=color:#f92672>VSPHERE_CONTROL_PLANE_ENDPOINT</span>: <span style=color:#e6db74>&#34;&#34;</span> <span style=color:#75715e># If you use NSX Advanced Load Balancer, leave this field blank.</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Control Plane and Worker VM sizing</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>VSPHERE_CONTROL_PLANE_DISK_GIB</span>: <span style=color:#e6db74>&#34;40&#34;</span> <span style=color:#75715e># The size in gigabytes of the disk for the control plane node VMs. Include the quotes (&#34;&#34;)</span>
</span></span><span style=display:flex><span><span style=color:#f92672>VSPHERE_CONTROL_PLANE_MEM_MIB</span>: <span style=color:#e6db74>&#34;16384&#34;</span> <span style=color:#75715e># The amount of memory in megabytes for the control plane node VMs</span>
</span></span><span style=display:flex><span><span style=color:#f92672>VSPHERE_CONTROL_PLANE_NUM_CPUS</span>: <span style=color:#e6db74>&#34;4&#34;</span> <span style=color:#75715e># The number of CPUs for the control plane node VMs. Include the quotes (&#34;&#34;). Must be at least 2.</span>
</span></span><span style=display:flex><span><span style=color:#f92672>VSPHERE_WORKER_DISK_GIB</span>: <span style=color:#e6db74>&#34;40&#34;</span> <span style=color:#75715e># The size in gigabytes of the disk for the worker node VMs. Include the quotes (&#34;&#34;)</span>
</span></span><span style=display:flex><span><span style=color:#f92672>VSPHERE_WORKER_MEM_MIB</span>: <span style=color:#e6db74>&#34;16384&#34;</span> <span style=color:#75715e># The amount of memory in megabytes for the worker node VMs. Include the quotes (&#34;&#34;)</span>
</span></span><span style=display:flex><span><span style=color:#f92672>VSPHERE_WORKER_NUM_CPUS</span>: <span style=color:#e6db74>&#34;4&#34;</span> <span style=color:#75715e># The number of CPUs for the worker node VMs. Include the quotes (””). Must be at least 2.</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># vSphere Infrastructure details</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>VSPHERE_DATACENTER</span>: <span style=color:#75715e># The name of the datacenter in which to deploy the TKG management cluster.</span>
</span></span><span style=display:flex><span><span style=color:#f92672>VSPHERE_DATASTORE</span>: <span style=color:#75715e># The name of the vSphere datastore where TKG cluster VMs will be stored.</span>
</span></span><span style=display:flex><span><span style=color:#f92672>VSPHERE_FOLDER</span>: <span style=color:#75715e># The name of an existing VM folder in which to place TKG VMs.</span>
</span></span><span style=display:flex><span><span style=color:#f92672>VSPHERE_INSECURE</span>: <span style=color:#75715e># Optional. Set to true or false to bypass thumbprint verification. If false, set VSPHERE_TLS_THUMBPRINT</span>
</span></span><span style=display:flex><span><span style=color:#f92672>VSPHERE_NETWORK</span>: <span style=color:#75715e># The name of an existing vSphere network where TKG management cluster control plane and worker VMs will be connected.</span>
</span></span><span style=display:flex><span><span style=color:#f92672>VSPHERE_PASSWORD</span>: <span style=color:#75715e># The password for the vSphere user account in base64 encoded format.</span>
</span></span><span style=display:flex><span><span style=color:#f92672>VSPHERE_RESOURCE_POOL</span>: <span style=color:#75715e># The name of an existing resource pool in which to place TKG cluster.</span>
</span></span><span style=display:flex><span><span style=color:#f92672>VSPHERE_SERVER</span>: <span style=color:#75715e># The IP address or FQDN of the vCenter Server instance on which to deploy the Tanzu Kubernetes cluster.</span>
</span></span><span style=display:flex><span><span style=color:#f92672>VSPHERE_SSH_AUTHORIZED_KEY</span>: <span style=color:#75715e># Paste in the contents of the SSH public key that you created in on the bootstrap machine.</span>
</span></span><span style=display:flex><span><span style=color:#f92672>VSPHERE_TLS_THUMBPRINT</span>: <span style=color:#75715e># if VSPHERE_INSECURE is false. The thumbprint of the vCenter Server certificate.</span>
</span></span><span style=display:flex><span><span style=color:#f92672>VSPHERE_USERNAME</span>: <span style=color:#75715e># A vSphere user account, including the domain name, with the required privileges for Tanzu Kubernetes Grid operation</span>
</span></span><span style=display:flex><span><span style=color:#f92672>TKG_CUSTOM_IMAGE_REPOSITORY</span>: <span style=color:#75715e># IP address or FQDN of your private registry</span>
</span></span><span style=display:flex><span><span style=color:#f92672>TKG_CUSTOM_IMAGE_REPOSITORY_CA_CERTIFICATE</span>: <span style=color:#75715e>#Set if your private image registry uses a self-signed certificate. Provide the CA certificate in base64 encoded format</span>
</span></span></code></pre></div><p>For a full list of configurable values, please see TKG product <a href=https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid/1.5/vmware-tanzu-kubernetes-grid-15/GUID-tanzu-config-reference.html>documentation</a>.</p><h2 id=configure-node-sizes>Configure Node Sizes</h2><p>The Tanzu CLI creates the individual nodes of management clusters and Tanzu Kubernetes clusters according to the settings that you provide in the configuration file.</p><p>On vSphere, you can configure all node VMs to have the same predefined configurations, set different predefined configurations for control plane and worker nodes, or customize the configurations of the nodes. By using these settings, you can create clusters that have nodes with different configurations from the management cluster nodes. You can also create clusters in which the control plane nodes and worker nodes have different configurations.</p><p><strong>Use Predefined Node Configurations</strong></p><p>The Tanzu CLI provides the following predefined configurations for cluster nodes:</p><table><thead><tr><th style=text-align:left><strong>Size</strong></th><th style=text-align:left><strong>CPU</strong></th><th style=text-align:left><strong>Memory (in GB)</strong></th><th style=text-align:left><strong>Disk (in GB)</strong></th></tr></thead><tbody><tr><td style=text-align:left>Small</td><td style=text-align:left>2</td><td style=text-align:left>4</td><td style=text-align:left>20</td></tr><tr><td style=text-align:left>Medium</td><td style=text-align:left>2</td><td style=text-align:left>8</td><td style=text-align:left>40</td></tr><tr><td style=text-align:left>Large</td><td style=text-align:left>4</td><td style=text-align:left>16</td><td style=text-align:left>40</td></tr><tr><td style=text-align:left>Extra-large</td><td style=text-align:left>8</td><td style=text-align:left>32</td><td style=text-align:left>80</td></tr></tbody></table><p>To create a cluster in which all of the control plane and worker node VMs are the same size, specify the <code>SIZE</code> variable. If you set the SIZE variable, all nodes will be created with the configuration that you set.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>SIZE: <span style=color:#e6db74>&#34;large&#34;</span>
</span></span></code></pre></div><p>To create a cluster in which the control plane and worker node VMs are different sizes, specify the <code>CONTROLPLANE_SIZE</code> and <code>WORKER_SIZE</code> options.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>CONTROLPLANE_SIZE: <span style=color:#e6db74>&#34;medium&#34;</span>
</span></span><span style=display:flex><span>WORKER_SIZE: <span style=color:#e6db74>&#34;large&#34;</span>
</span></span></code></pre></div><p>You can combine the <code>CONTROLPLANE_SIZE</code> and <code>WORKER_SIZE</code> options with the <code>SIZE</code> option. For example, if you specify <code>SIZE</code>: &ldquo;large&rdquo; with WORKER_SIZE: &ldquo;extra-large&rdquo;, the control plane nodes will be set to large and worker nodes will be set to extra-large.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>SIZE: <span style=color:#e6db74>&#34;large&#34;</span>
</span></span><span style=display:flex><span>WORKER_SIZE: <span style=color:#e6db74>&#34;extra-large&#34;</span>
</span></span></code></pre></div><p><strong>Define Custom Node Configurations</strong></p><p>You can customize the configuration of the nodes rather than using the predefined configurations.</p><p>To use the same custom configuration for all nodes, specify the <code>VSPHERE_NUM_CPUS</code>, <code>VSPHERE_DISK_GIB</code>, and <code>VSPHERE_MEM_MIB</code> options.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>VSPHERE_NUM_CPUS: <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>VSPHERE_DISK_GIB: <span style=color:#ae81ff>40</span>
</span></span><span style=display:flex><span>VSPHERE_MEM_MIB: <span style=color:#ae81ff>4096</span>
</span></span></code></pre></div><p>To define different custom configurations for control plane nodes and worker nodes, specify the VSPHERE_CONTROL_PLANE_* and VSPHERE_WORKER_*</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>VSPHERE_CONTROL_PLANE_NUM_CPUS: <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>VSPHERE_CONTROL_PLANE_DISK_GIB: <span style=color:#ae81ff>20</span>
</span></span><span style=display:flex><span>VSPHERE_CONTROL_PLANE_MEM_MIB: <span style=color:#ae81ff>8192</span>
</span></span><span style=display:flex><span>VSPHERE_WORKER_NUM_CPUS: <span style=color:#ae81ff>4</span>
</span></span><span style=display:flex><span>VSPHERE_WORKER_DISK_GIB: <span style=color:#ae81ff>40</span>
</span></span><span style=display:flex><span>VSPHERE_WORKER_MEM_MIB: <span style=color:#ae81ff>4096</span>
</span></span></code></pre></div><h2 id=nsx-advanced-load-balancer-sizing-guidelines><strong>NSX Advanced Load Balancer Sizing Guidelines</strong></h2><h3 id=nsx-alb-controller-sizing-guidelines>NSX ALB Controller Sizing Guidelines</h3><p>Regardless of NSX Advanced Load Balancer Controller configuration, each Controller cluster can achieve up to 5,000 virtual services, this is a hard limit. For further details, please refer to this <a href=https://avinetworks.com/docs/20.1/avi-controller-sizing/#cpuandmemalloc>guide</a>.</p><table><thead><tr><th style=text-align:left><strong>Controller Size</strong></th><th style=text-align:left><strong>VM Configuration</strong></th><th style=text-align:left><strong>Virtual Services</strong></th><th style=text-align:left><strong>NSX Advanced Load Balancer SE Scale</strong></th></tr></thead><tbody><tr><td style=text-align:left>Small</td><td style=text-align:left>4 vCPUS, 12 GB RAM</td><td style=text-align:left>0-50</td><td style=text-align:left>0-10</td></tr><tr><td style=text-align:left>Medium</td><td style=text-align:left>8 vCPUS, 24 GB RAM</td><td style=text-align:left>0-200</td><td style=text-align:left>0-100</td></tr><tr><td style=text-align:left>Large</td><td style=text-align:left>16 vCPUS, 32 GB RAM</td><td style=text-align:left>200-1000</td><td style=text-align:left>100-200</td></tr><tr><td style=text-align:left>Extra Large</td><td style=text-align:left>24 vCPUS, 48 GB RAM</td><td style=text-align:left>1000-5000</td><td style=text-align:left>200-400</td></tr></tbody></table><h3 id=service-engine-sizing-guidelines>Service Engine Sizing Guidelines</h3><p>See <a href=https://avinetworks.com/docs/20.1/sizing-service-engines/>Sizing Service Engines</a> for guidance on sizing your SEs.</p><table><thead><tr><th style=text-align:left><strong>Performance metric</strong></th><th style=text-align:left><strong>1 vCPU core</strong></th></tr></thead><tbody><tr><td style=text-align:left>Throughput</td><td style=text-align:left>4 Gb/s</td></tr><tr><td style=text-align:left>Connections/s</td><td style=text-align:left>40k</td></tr><tr><td style=text-align:left>SSL Throughput</td><td style=text-align:left>1 Gb/s</td></tr><tr><td style=text-align:left>SSL TPS (RSA2K)</td><td style=text-align:left>~600</td></tr><tr><td style=text-align:left>SSL TPS (ECC)</td><td style=text-align:left>2500</td></tr></tbody></table><p>Multiple performance vectors or features may have an impact on performance. For instance, to achieve 1 Gb/s of SSL throughput and 2000 TPS of SSL with EC certificates, NSX Advanced Load Balancer recommends two cores.</p><p>NSX Advanced Load Balancer Service Engines may be configured with as little as 1 vCPU core and 1 GB RAM, or up to 36 vCPU cores and 128 GB RAM. Service Engines can be deployed in Active/Active or Active/Standby mode depending on the license tier used. NSX Advanced Load Balancer Essentials license doesn’t support Active/Active HA mode for SE.</p><p>Please see the table below for Service Engine recommendations.</p><table><thead><tr><th style=text-align:left><strong>Decision ID</strong></th><th style=text-align:left><strong>Design Decision</strong></th><th style=text-align:left><strong>Design Justification</strong></th><th style=text-align:left><strong>Design Implications</strong></th></tr></thead><tbody><tr><td style=text-align:left>TKO-ALB-SE-001</td><td style=text-align:left>Configure the High Availability mode for SEs</td><td style=text-align:left>To mitigate a single point of failure for the NSX ALB data plane.</td><td style=text-align:left>High Availability** for Service Engines is configured via setting the Elastic HA mode to Active/Active or N+M in the Service Engine Group.</td></tr><tr><td style=text-align:left>TKO-ALB-SE-002</td><td style=text-align:left>Configure compute reservations in the Service Engine group as follows:vCPU: 2Memory: 4 GB</td><td style=text-align:left>Service Engine with just one vCPU and 1 GB (default settings in Service Engine Group) might not be enough for production workloads.</td><td style=text-align:left>Changing the default vCPU count in the Service Engine Group will result in extra license core consumption.</td></tr></tbody></table></div></div><div class="loader-container-toc col-lg-2 w-100" style=display:none><span class=spinner></span></div><div class="rhs-right-container d-none d-lg-block col-lg-2"><div class=rhs-right-panel><div class="rhs-right-panel-header d-flex justify-content-between"><div class=rhs-right-panel-title>In this article</div></div><div id=right-toc-div-id class="onpage-toc-container d-flex flex-column"><nav id=TableOfContents><ul><li><a href=#url-reference-designs-tkg-nsxt-airgap-ramdoldhtml>url: reference-designs-tkg-nsxt-airgap-ra.md.old.html</a></li><li><a href=#tkg-bill-of-materials><strong>TKG Bill Of Materials</strong></a></li><li><a href=#tanzu-kubernetes-grid-components><strong>Tanzu Kubernetes Grid Components</strong></a></li><li><a href=#tanzu-kubernetes-grid-storage><strong>Tanzu Kubernetes Grid Storage</strong></a></li><li><a href=#tanzu-kubernetes-clusters-networking><strong>Tanzu Kubernetes Clusters Networking</strong></a></li><li><a href=#tanzu-kubernetes-grid-infrastructure-networking><strong>Tanzu Kubernetes Grid Infrastructure Networking</strong></a></li><li><a href=#tkg-on-nsx-t-networking-with-nsx-advanced-load-balancer><strong>TKG on NSX-T Networking with NSX Advanced Load Balancer</strong></a></li></ul><ul><li><a href=#network-requirements><strong>Network Requirements</strong></a></li><li><a href=#subnet-and-cidr-examples><strong>Subnet and CIDR Examples</strong></a></li><li><a href=#a-idfirewall-requirements-a-firewall-requirements><strong>Firewall Requirements</strong></a></li></ul><ul><li><a href=#nsx-advanced-load-balancer-recommendations>NSX Advanced Load Balancer Recommendations</a></li><li><a href=#network-recommendations>Network Recommendations</a></li><li><a href=#tkg-clusters-recommendations>TKG Clusters Recommendations</a></li></ul><ul><li><a href=#a-iddeployment-parameters-a-cluster-deployment-parameters>Cluster Deployment Parameters</a></li><li><a href=#configure-node-sizes>Configure Node Sizes</a></li><li><a href=#nsx-advanced-load-balancer-sizing-guidelines><strong>NSX Advanced Load Balancer Sizing Guidelines</strong></a><ul><li><a href=#nsx-alb-controller-sizing-guidelines>NSX ALB Controller Sizing Guidelines</a></li><li><a href=#service-engine-sizing-guidelines>Service Engine Sizing Guidelines</a></li></ul></li></ul></nav></div></div></div></section></div><footer class=tech-pub-footer><div id=page-footer><section class="footer-component footer-container"><div class=personalization_div_1 style=min-height:1px></div><div class=personalization_div_2 style=min-height:1px></div><div class=container><div class=content><div class=row><div class="col-lg-12 col-md-12"><footer class=footer><div class=row><div class="col-lg-2 col-md-12 mb-40 mt-3"><a class=footer-vmware-logo href=https://www-lt.vmware.com/ name="nav_footer : VMware Logo"><picture class=float-lg-left><source media=(max-width:800px) srcset=https://www-lt.vmware.com/content/dam/digitalmarketing/vmware/vm-logo-big.png.imgo.jpeg><img loading=lazy class=vmware-logo src=/img/vm-logo-big.png alt=VMware title=VMware></picture></a></div></div></footer></div></div></div></div></section></div></footer><script src=/js/pageStore.js></script>
<script src=/js/main.js></script></body></html>